import os
import sys
import logging
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from datetime import datetime
from tqdm import tqdm
import math
import argparse
import random
import traceback
import time
import json
import warnings
import pytz
warnings.filterwarnings("ignore")

# ========== è®¾ç½®éšæœºç§å­ ==========
def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# ========== æ—¥å¿—æ–‡ä»¶å†™å…¥å·¥å…· ==========
class FileLogger:
    def __init__(self, log_path):
        log_dir = os.path.dirname(log_path)
        if log_dir and not os.path.exists(log_dir):
            os.makedirs(log_dir, exist_ok=True)
        self.log_file = open(log_path, 'w', encoding='utf-8')
    
    def info(self, msg):
        beijing_tz = pytz.timezone('Asia/Shanghai')
        timestamp = datetime.now(beijing_tz).strftime("%Y-%m-%d %H:%M:%S")
        self.log_file.write(f"[{timestamp}] {msg}\n")
        self.log_file.flush()
    
    def close(self):
        if hasattr(self, 'log_file') and self.log_file:
            self.log_file.close()

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
)
logger = logging.getLogger(__name__)

def create_dynamic_output_dir(base_dir):
    import datetime
    beijing_tz = pytz.timezone('Asia/Shanghai')
    timestamp = datetime.datetime.now(beijing_tz).strftime("%Y%m%d_%H%M%S")
    dynamic_dir = os.path.join(base_dir, f"senm_vae_run_{timestamp}")
    os.makedirs(dynamic_dir, exist_ok=True)
    return dynamic_dir

# ========== RDBæ®‹å·®å¯†é›†å— - SeNM-VAEæ ¸å¿ƒç»„ä»¶ ==========

class ResidualDenseBlock(nn.Module):
    """æ®‹å·®å¯†é›†å— (RDB) - è®ºæ–‡ä¸­çš„åŸºç¡€ç½‘ç»œå—
    
    RDBçš„è®¾è®¡ç‰¹ç‚¹ï¼š
    1. å¯†é›†è¿æ¥ï¼šæ¯ä¸€å±‚éƒ½ä¸å‰é¢æ‰€æœ‰å±‚è¿æ¥
    2. æ®‹å·®è¿æ¥ï¼šè¾“å…¥ç›´æ¥è¿æ¥åˆ°è¾“å‡º
    3. ç‰¹å¾é‡ç”¨ï¼šå……åˆ†åˆ©ç”¨å‰é¢å±‚çš„ç‰¹å¾ä¿¡æ¯
    4. æ¢¯åº¦æµç•…ï¼šæœ‰åŠ©äºæ·±åº¦ç½‘ç»œçš„è®­ç»ƒ
    """
    
    def __init__(self, input_dim, growth_rate=32, num_layers=4):
        super(ResidualDenseBlock, self).__init__()
        
        self.num_layers = num_layers
        self.growth_rate = growth_rate
        
        # æ„å»ºå¯†é›†è¿æ¥å±‚
        self.layers = nn.ModuleList()
        for i in range(num_layers):
            layer_input_dim = input_dim + i * growth_rate
            self.layers.append(
                nn.Sequential(
                    nn.Linear(layer_input_dim, growth_rate),
                    nn.LeakyReLU(0.2, inplace=True)
                )
            )
        
        # å±€éƒ¨ç‰¹å¾èåˆ
        final_input_dim = input_dim + num_layers * growth_rate
        self.local_feature_fusion = nn.Sequential(
            nn.Linear(final_input_dim, input_dim),
            nn.LeakyReLU(0.2, inplace=True)
        )
        
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        input_x = x
        features = [x]
        
        # å¯†é›†è¿æ¥çš„å‰å‘ä¼ æ’­
        for layer in self.layers:
            x = torch.cat(features, dim=-1)
            new_feature = layer(x)
            features.append(new_feature)
        
        # å±€éƒ¨ç‰¹å¾èåˆ
        x = torch.cat(features, dim=-1)
        local_fused = self.local_feature_fusion(x)
        
        # æ®‹å·®è¿æ¥
        output = input_x + local_fused * 0.2  # ç¼©æ”¾å› å­é˜²æ­¢ç‰¹å¾çˆ†ç‚¸
        
        return output

class RDBBlock(nn.Module):
    """RDBåŸºç¡€å—çš„å°è£…ï¼Œé€‚ç”¨äºæ—¶åºæ•°æ®"""
    
    def __init__(self, input_dim, hidden_dim=64, num_rdb=3, output_dim=None):
        super(RDBBlock, self).__init__()
        
        if output_dim is None:
            output_dim = input_dim
        
        self.output_dim = output_dim
        
        # è¾“å…¥æŠ•å½±
        self.input_projection = nn.Linear(input_dim, hidden_dim)
        
        # å¤šä¸ªRDBå †å 
        self.rdb_layers = nn.ModuleList([
            ResidualDenseBlock(hidden_dim, growth_rate=hidden_dim//4, num_layers=4)
            for _ in range(num_rdb)
        ])
        
        # è¾“å‡ºæŠ•å½± - æ”¯æŒä¸åŒçš„è¾“å‡ºç»´åº¦
        self.output_projection = nn.Linear(hidden_dim, output_dim)
        
    def forward(self, x):
        """å‰å‘ä¼ æ’­
        Args:
            x: [batch_size, seq_len, input_dim] è¾“å…¥æ—¶åºç‰¹å¾
        Returns:
            output: [batch_size, seq_len, output_dim] å¤„ç†åçš„ç‰¹å¾
        """
        # è¾“å…¥æŠ•å½±
        x = self.input_projection(x)
        
        # RDBå¤„ç†
        for rdb in self.rdb_layers:
            x = rdb(x)
        
        # è¾“å‡ºæŠ•å½±
        output = self.output_projection(x)
        
        return output

# ========== åˆ†å±‚VAEç¼–ç å™¨ç»„ä»¶ ==========

class HierarchicalEncoder(nn.Module):
    """åˆ†å±‚ç¼–ç å™¨ - SeNM-VAEçš„æ ¸å¿ƒç»„ä»¶
    
    å®ç°è®ºæ–‡ä¸­çš„å¤šå±‚ç¼–ç ç»“æ„ï¼š
    - q(z|features): ç‰¹å¾ç¼–ç å™¨
    - q(z|od_flows): ODæµé‡ç¼–ç å™¨  
    - q(zn|od_flows, z): åŠ¨æ€ä¿¡æ¯ç¼–ç å™¨
    """
    
    def __init__(self, input_dim, latent_dim=64, num_layers=3, hidden_dim=128):
        super(HierarchicalEncoder, self).__init__()
        
        self.input_dim = input_dim
        self.latent_dim = latent_dim
        self.num_layers = num_layers
        self.hidden_dim = hidden_dim
        
        # è¾“å…¥ç‰¹å¾å¤„ç†ï¼šå°†input_dimæ˜ å°„åˆ°hidden_dim
        self.input_rdb = RDBBlock(input_dim, hidden_dim, num_rdb=2, output_dim=hidden_dim)
        
        # åˆ†å±‚ç¼–ç ç½‘ç»œ
        self.encoding_layers = nn.ModuleList()
        self.mu_layers = nn.ModuleList()
        self.logvar_layers = nn.ModuleList()
        
        for l in range(num_layers):
            # ç¼–ç å±‚çš„è¾“å…¥ç»´åº¦
            if l == 0:
                layer_input_dim = hidden_dim
            else:
                layer_input_dim = hidden_dim + latent_dim  # åŒ…å«ä¸Šå±‚æ½œåœ¨å˜é‡
            
            # ç¼–ç ç½‘ç»œ
            self.encoding_layers.append(
                nn.Sequential(
                    nn.Linear(layer_input_dim, hidden_dim),
                    nn.LeakyReLU(0.2),
                    RDBBlock(hidden_dim, hidden_dim, num_rdb=1),
                    nn.Linear(hidden_dim, hidden_dim)
                )
            )
            
            # å‡å€¼å’Œæ–¹å·®ç½‘ç»œ
            self.mu_layers.append(nn.Linear(hidden_dim, latent_dim))
            self.logvar_layers.append(nn.Linear(hidden_dim, latent_dim))
    
    def forward(self, x):
        """åˆ†å±‚ç¼–ç å‰å‘ä¼ æ’­
        Args:
            x: [batch_size, seq_len, input_dim] è¾“å…¥åºåˆ—
        Returns:
            mu_list: å„å±‚æ½œåœ¨å˜é‡å‡å€¼åˆ—è¡¨
            logvar_list: å„å±‚æ½œåœ¨å˜é‡å¯¹æ•°æ–¹å·®åˆ—è¡¨
            z_list: å„å±‚é‡‡æ ·çš„æ½œåœ¨å˜é‡åˆ—è¡¨
        """
        batch_size, seq_len, _ = x.shape
        
        # è¾“å…¥ç‰¹å¾å¤„ç†ï¼šRDBBlockä¼šå¤„ç†ç»´åº¦æ˜ å°„
        x_processed = self.input_rdb(x)  # [batch_size, seq_len, input_dim] -> [batch_size, seq_len, hidden_dim]
        
        mu_list = []
        logvar_list = []
        z_list = []
        
        # è‡ªé¡¶å‘ä¸‹çš„åˆ†å±‚ç¼–ç 
        prev_z = None
        for l in range(self.num_layers):
            # å‡†å¤‡å½“å‰å±‚è¾“å…¥
            if l == 0:
                layer_input = x_processed
            else:
                # å°†ä¸Šå±‚æ½œåœ¨å˜é‡ä¸å½“å‰ç‰¹å¾æ‹¼æ¥
                prev_z_expanded = prev_z.unsqueeze(1).expand(-1, seq_len, -1)
                layer_input = torch.cat([x_processed, prev_z_expanded], dim=-1)
            
            # ç¼–ç 
            h = self.encoding_layers[l](layer_input)  # [batch_size, seq_len, hidden_dim]
            
            # è®¡ç®—å‡å€¼å’Œæ–¹å·® (å¯¹åºåˆ—ç»´åº¦å–å¹³å‡)
            h_pooled = h.mean(dim=1)  # [batch_size, hidden_dim]
            mu = self.mu_layers[l](h_pooled)  # [batch_size, latent_dim]
            logvar = self.logvar_layers[l](h_pooled)  # [batch_size, latent_dim]
            
            # é‡å‚æ•°åŒ–é‡‡æ ·
            z = self.reparameterize(mu, logvar)  # [batch_size, latent_dim]
            
            mu_list.append(mu)
            logvar_list.append(logvar)
            z_list.append(z)
            prev_z = z
        
        return mu_list, logvar_list, z_list
    
    def reparameterize(self, mu, logvar):
        """é‡å‚æ•°åŒ–æŠ€å·§"""
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

class HierarchicalNoiseEncoder(nn.Module):
    """åˆ†å±‚å™ªå£°ç¼–ç å™¨ - ç¼–ç ODæµé‡çš„åŠ¨æ€å˜åŒ–ä¿¡æ¯
    
    å¯¹åº”è®ºæ–‡ä¸­çš„ q(zn|od_flows, z)
    """
    
    def __init__(self, od_dim, latent_dim=64, num_layers=3, hidden_dim=128):
        super(HierarchicalNoiseEncoder, self).__init__()
        
        self.od_dim = od_dim
        self.latent_dim = latent_dim
        self.num_layers = num_layers
        self.hidden_dim = hidden_dim
        
        # ODæµé‡ç‰¹å¾å¤„ç†ï¼šå°†od_dimæ˜ å°„åˆ°hidden_dim
        self.od_rdb = RDBBlock(od_dim, hidden_dim, num_rdb=2, output_dim=hidden_dim)
        
        # åˆ†å±‚ç¼–ç ç½‘ç»œ
        self.encoding_layers = nn.ModuleList()
        self.mu_layers = nn.ModuleList()
        self.logvar_layers = nn.ModuleList()
        
        for l in range(num_layers):
            # è¾“å…¥ç»´åº¦ï¼šODç‰¹å¾ + å†…å®¹æ½œåœ¨å˜é‡ + ä¸Šå±‚å™ªå£°æ½œåœ¨å˜é‡
            if l == 0:
                layer_input_dim = hidden_dim + latent_dim  # odç‰¹å¾ + z
            else:
                layer_input_dim = hidden_dim + latent_dim + latent_dim  # odç‰¹å¾ + z + ä¸Šå±‚zn
            
            self.encoding_layers.append(
                nn.Sequential(
                    nn.Linear(layer_input_dim, hidden_dim),
                    nn.LeakyReLU(0.2),
                    RDBBlock(hidden_dim, hidden_dim, num_rdb=1),
                    nn.Linear(hidden_dim, hidden_dim)
                )
            )
            
            self.mu_layers.append(nn.Linear(hidden_dim, latent_dim))
            self.logvar_layers.append(nn.Linear(hidden_dim, latent_dim))
    
    def forward(self, od_flows, z_list):
        """åˆ†å±‚å™ªå£°ç¼–ç 
        Args:
            od_flows: [batch_size, seq_len, od_dim] ODæµé‡
            z_list: å†…å®¹æ½œåœ¨å˜é‡åˆ—è¡¨
        Returns:
            mu_list: å„å±‚å™ªå£°æ½œåœ¨å˜é‡å‡å€¼åˆ—è¡¨
            logvar_list: å„å±‚å™ªå£°æ½œåœ¨å˜é‡å¯¹æ•°æ–¹å·®åˆ—è¡¨  
            zn_list: å„å±‚é‡‡æ ·çš„å™ªå£°æ½œåœ¨å˜é‡åˆ—è¡¨
        """
        batch_size, seq_len, _ = od_flows.shape
        
        # ODæµé‡ç‰¹å¾å¤„ç†
        od_features = self.od_rdb(od_flows)  # [batch_size, seq_len, hidden_dim]
        
        mu_list = []
        logvar_list = []
        zn_list = []
        
        # è‡ªé¡¶å‘ä¸‹çš„åˆ†å±‚ç¼–ç 
        prev_zn = None
        for l in range(self.num_layers):
            # å‡†å¤‡å½“å‰å±‚è¾“å…¥
            z_l = z_list[l].unsqueeze(1).expand(-1, seq_len, -1)  # [batch_size, seq_len, latent_dim]
            
            if l == 0:
                layer_input = torch.cat([od_features, z_l], dim=-1)
            else:
                prev_zn_expanded = prev_zn.unsqueeze(1).expand(-1, seq_len, -1)
                layer_input = torch.cat([od_features, z_l, prev_zn_expanded], dim=-1)
            
            # ç¼–ç 
            h = self.encoding_layers[l](layer_input)  # [batch_size, seq_len, hidden_dim]
            
            # è®¡ç®—å‡å€¼å’Œæ–¹å·®
            h_pooled = h.mean(dim=1)  # [batch_size, hidden_dim]
            mu = self.mu_layers[l](h_pooled)  # [batch_size, latent_dim]
            logvar = self.logvar_layers[l](h_pooled)  # [batch_size, latent_dim]
            
            # é‡å‚æ•°åŒ–é‡‡æ ·
            zn = self.reparameterize(mu, logvar)  # [batch_size, latent_dim]
            
            mu_list.append(mu)
            logvar_list.append(logvar)
            zn_list.append(zn)
            prev_zn = zn
        
        return mu_list, logvar_list, zn_list
    
    def reparameterize(self, mu, logvar):
        """é‡å‚æ•°åŒ–æŠ€å·§"""
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

# ========== åˆ†å±‚VAEè§£ç å™¨ç»„ä»¶ ==========

class HierarchicalDecoder(nn.Module):
    """åˆ†å±‚è§£ç å™¨ - SeNM-VAEçš„ç”Ÿæˆç»„ä»¶
    
    å®ç°è®ºæ–‡ä¸­çš„ç”Ÿæˆæ¨¡å‹ï¼š
    - p(features|z): ç‰¹å¾ç”Ÿæˆ
    - p(od_flows|z, zn): ODæµé‡ç”Ÿæˆ
    - p(zn|z): å™ªå£°å…ˆéªŒ
    """
    
    def __init__(self, latent_dim=64, output_dim=6, num_layers=3, hidden_dim=128, seq_len=28):
        super(HierarchicalDecoder, self).__init__()
        
        self.latent_dim = latent_dim
        self.output_dim = output_dim
        self.num_layers = num_layers
        self.hidden_dim = hidden_dim
        self.seq_len = seq_len
        
        # ç‰¹å¾ç”Ÿæˆç½‘ç»œ p(features|z)
        self.feature_decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.LeakyReLU(0.2),
            RDBBlock(hidden_dim, hidden_dim, num_rdb=2),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LeakyReLU(0.2),
            nn.Linear(hidden_dim, output_dim * seq_len)
        )
        
        # ODæµé‡ç”Ÿæˆç½‘ç»œ p(od_flows|z, zn)
        self.od_decoder = nn.Sequential(
            nn.Linear(latent_dim * 2, hidden_dim),  # z + zn
            nn.LeakyReLU(0.2),
            RDBBlock(hidden_dim, hidden_dim, num_rdb=2),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LeakyReLU(0.2),
            nn.Linear(hidden_dim, 2 * seq_len)  # ODæµé‡ç»´åº¦ä¸º2
        )
        
        # å™ªå£°å…ˆéªŒç½‘ç»œ p(zn|z)
        self.noise_prior_layers = nn.ModuleList()
        for l in range(num_layers):
            self.noise_prior_layers.append(
                nn.Sequential(
                    nn.Linear(latent_dim, hidden_dim),
                    nn.LeakyReLU(0.2),
                    nn.Linear(hidden_dim, latent_dim * 2)  # å‡å€¼å’Œæ–¹å·®
                )
            )
    
    def decode_features(self, z_list):
        """è§£ç ç‰¹å¾ p(features|z)
        Args:
            z_list: å†…å®¹æ½œåœ¨å˜é‡åˆ—è¡¨
        Returns:
            features: [batch_size, seq_len, feature_dim] é‡æ„çš„ç‰¹å¾
        """
        # ä½¿ç”¨æœ€é«˜å±‚çš„æ½œåœ¨å˜é‡è¿›è¡Œè§£ç 
        z_top = z_list[-1]  # [batch_size, latent_dim]
        
        decoded = self.feature_decoder(z_top)  # [batch_size, feature_dim * seq_len]
        features = decoded.view(-1, self.seq_len, self.output_dim)  # [batch_size, seq_len, feature_dim]
        
        return features
    
    def decode_od_flows(self, z_list, zn_list):
        """è§£ç ODæµé‡ p(od_flows|z, zn)
        Args:
            z_list: å†…å®¹æ½œåœ¨å˜é‡åˆ—è¡¨
            zn_list: å™ªå£°æ½œåœ¨å˜é‡åˆ—è¡¨
        Returns:
            od_flows: [batch_size, seq_len, 2] é‡æ„çš„ODæµé‡
        """
        # ä½¿ç”¨æœ€é«˜å±‚çš„æ½œåœ¨å˜é‡è¿›è¡Œè§£ç 
        z_top = z_list[-1]  # [batch_size, latent_dim]
        zn_top = zn_list[-1]  # [batch_size, latent_dim]
        
        # æ‹¼æ¥å†…å®¹å’Œå™ªå£°æ½œåœ¨å˜é‡
        combined = torch.cat([z_top, zn_top], dim=-1)  # [batch_size, latent_dim * 2]
        
        decoded = self.od_decoder(combined)  # [batch_size, 2 * seq_len]
        od_flows = decoded.view(-1, self.seq_len, 2)  # [batch_size, seq_len, 2]
        
        return od_flows
    
    def get_noise_prior(self, z_list):
        """è®¡ç®—å™ªå£°å…ˆéªŒ p(zn|z)
        Args:
            z_list: å†…å®¹æ½œåœ¨å˜é‡åˆ—è¡¨
        Returns:
            prior_mu_list: å„å±‚å™ªå£°å…ˆéªŒå‡å€¼åˆ—è¡¨
            prior_logvar_list: å„å±‚å™ªå£°å…ˆéªŒå¯¹æ•°æ–¹å·®åˆ—è¡¨
        """
        prior_mu_list = []
        prior_logvar_list = []
        
        for l in range(self.num_layers):
            z_l = z_list[l]  # [batch_size, latent_dim]
            
            # è®¡ç®—å…ˆéªŒå‚æ•°
            prior_params = self.noise_prior_layers[l](z_l)  # [batch_size, latent_dim * 2]
            prior_mu = prior_params[:, :self.latent_dim]  # [batch_size, latent_dim]
            prior_logvar = prior_params[:, self.latent_dim:]  # [batch_size, latent_dim]
            
            prior_mu_list.append(prior_mu)
            prior_logvar_list.append(prior_logvar)
        
        return prior_mu_list, prior_logvar_list

# ========== SeNM-VAEä¸»æ¨¡å‹ ==========

class SeNMVAEODFlowPredictor(nn.Module):
    """åŸºäºSeNM-VAEçš„ODæµé‡é¢„æµ‹æ¨¡å‹
    
    ä¸»è¦åˆ›æ–°ç‚¹ï¼š
    1. åŠç›‘ç£å­¦ä¹ ï¼šåˆ©ç”¨é…å¯¹ã€æºåŸŸã€ç›®æ ‡åŸŸä¸‰ç§æ•°æ®
    2. åˆ†å±‚VAEï¼šå¤šå±‚æ½œåœ¨å˜é‡å¢å¼ºè¡¨ç¤ºèƒ½åŠ›
    3. åŒæ½œåœ¨å˜é‡ï¼šzæ•æ‰å†…å®¹ä¿¡æ¯ï¼Œznæ•æ‰åŠ¨æ€ä¿¡æ¯
    4. æ··åˆæ¨ç†ï¼šq(z|features,od_flows) = p1*q(z|features) + p2*q(z|od_flows)
    5. RDBç½‘ç»œï¼šå¢å¼ºç‰¹å¾æå–èƒ½åŠ›
    """
    
    def __init__(self, input_dim=6, hidden_dim=128, latent_dim=64, time_steps=28, output_dim=2, num_layers=3):
        super(SeNMVAEODFlowPredictor, self).__init__()
        
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.latent_dim = latent_dim
        self.time_steps = time_steps
        self.output_dim = output_dim
        self.num_layers = num_layers
        
        # ç¼–ç å™¨ç»„ä»¶
        self.feature_encoder = HierarchicalEncoder(
            input_dim=input_dim,
            latent_dim=latent_dim,
            num_layers=num_layers,
            hidden_dim=hidden_dim
        )
        
        self.od_encoder = HierarchicalEncoder(
            input_dim=output_dim,
            latent_dim=latent_dim,
            num_layers=num_layers,
            hidden_dim=hidden_dim
        )
        
        self.noise_encoder = HierarchicalNoiseEncoder(
            od_dim=output_dim,
            latent_dim=latent_dim,
            num_layers=num_layers,
            hidden_dim=hidden_dim
        )
        
        # è§£ç å™¨ç»„ä»¶
        self.decoder = HierarchicalDecoder(
            latent_dim=latent_dim,
            output_dim=input_dim,
            num_layers=num_layers,
            hidden_dim=hidden_dim,
            seq_len=time_steps
        )
        
        # æ··åˆæƒé‡
        self.p1 = 0.5  # ç‰¹å¾ç¼–ç å™¨æƒé‡
        self.p2 = 0.5  # ODæµé‡ç¼–ç å™¨æƒé‡
        
        # KLæƒé‡å‚æ•°
        self.lambda_kl = 1e-7  # è®ºæ–‡ä¸­çš„Î»å‚æ•°
        
        # æŸå¤±å‡½æ•°
        self.mse_loss = nn.MSELoss()
        self.mae_loss = nn.L1Loss()
    
    def forward(self, features, target_od=None, mode='train'):
        """
        å‰å‘ä¼ æ’­
        Args:
            features: [batch_size, time_steps, input_dim] è¾“å…¥ç‰¹å¾
            target_od: [batch_size, time_steps, output_dim] ç›®æ ‡ODæµé‡
            mode: 'train' æˆ– 'eval'
        Returns:
            ç»“æœå­—å…¸
        """
        batch_size = features.size(0)
        
        if mode == 'train' and target_od is not None:
            return self._forward_train(features, target_od)
        else:
            return self._forward_eval(features)
    
    def _forward_train(self, features, target_od):
        """è®­ç»ƒæ¨¡å¼å‰å‘ä¼ æ’­ - å®ç°SeNM-VAEçš„åŠç›‘ç£å­¦ä¹ """
        
        # ====== é…å¯¹åŸŸå¤„ç† (Paired Domain) ======
        # ç¼–ç ç‰¹å¾å’ŒODæµé‡
        z_feat_mu, z_feat_logvar, z_feat_list = self.feature_encoder(features)
        z_od_mu, z_od_logvar, z_od_list = self.od_encoder(target_od)
        
        # æ··åˆæ¨ç†æ¨¡å‹ï¼šq(z|features,od_flows) = p1*q(z|features) + p2*q(z|od_flows)
        mixed_z_list = []
        for l in range(self.num_layers):
            z_mixed = self.p1 * z_feat_list[l] + self.p2 * z_od_list[l]
            mixed_z_list.append(z_mixed)
        
        # ç¼–ç å™ªå£°ä¿¡æ¯
        zn_mu, zn_logvar, zn_list = self.noise_encoder(target_od, mixed_z_list)
        
        # è§£ç 
        reconstructed_features = self.decoder.decode_features(mixed_z_list)
        reconstructed_od = self.decoder.decode_od_flows(mixed_z_list, zn_list)
        
        # å™ªå£°å…ˆéªŒ
        noise_prior_mu, noise_prior_logvar = self.decoder.get_noise_prior(mixed_z_list)
        
        # ====== æŸå¤±è®¡ç®— ======
        
        # é…å¯¹åŸŸæŸå¤± (Loss_p)
        # é‡æ„æŸå¤±
        feat_recon_loss = self.mse_loss(reconstructed_features, features)
        od_recon_loss = self.mse_loss(reconstructed_od, target_od)
        
        # KLæ•£åº¦æŸå¤±
        # KL(q(z|y)||q(z|x)) - è®ºæ–‡å…¬å¼7
        kl_z_loss = 0
        for l in range(self.num_layers):
            kl_z_loss += self._kl_divergence_gaussian(
                z_od_mu[l], z_od_logvar[l],
                z_feat_mu[l], z_feat_logvar[l]
            )
        
        # KL(q(zn|y,z)||p(zn|z)) - è®ºæ–‡ä¸­çš„å™ªå£°å…ˆéªŒKLæ•£åº¦
        kl_zn_loss = 0
        for l in range(self.num_layers):
            kl_zn_loss += self._kl_divergence_gaussian(
                zn_mu[l], zn_logvar[l],
                noise_prior_mu[l], noise_prior_logvar[l]
            )
        
        # é…å¯¹åŸŸæ€»æŸå¤±
        loss_p = (od_recon_loss + feat_recon_loss + 
                  self.lambda_kl * kl_z_loss + kl_zn_loss)
        
        # ====== æºåŸŸæŸå¤± (Loss_s) ======
        # æºåŸŸï¼šä»…ç‰¹å¾é‡æ„
        z_source_mu, z_source_logvar, z_source_list = self.feature_encoder(features)
        reconstructed_source_features = self.decoder.decode_features(z_source_list)
        loss_s = self.mse_loss(reconstructed_source_features, features)
        
        # ====== ç›®æ ‡åŸŸæŸå¤± (Loss_t) ======
        # ç›®æ ‡åŸŸï¼šä»…ODæµé‡é‡æ„
        z_target_mu, z_target_logvar, z_target_list = self.od_encoder(target_od)
        zn_target_mu, zn_target_logvar, zn_target_list = self.noise_encoder(target_od, z_target_list)
        reconstructed_target_od = self.decoder.decode_od_flows(z_target_list, zn_target_list)
        
        # ç›®æ ‡åŸŸå™ªå£°å…ˆéªŒ
        target_noise_prior_mu, target_noise_prior_logvar = self.decoder.get_noise_prior(z_target_list)
        
        # ç›®æ ‡åŸŸKLæŸå¤±
        kl_target_zn_loss = 0
        for l in range(self.num_layers):
            kl_target_zn_loss += self._kl_divergence_gaussian(
                zn_target_mu[l], zn_target_logvar[l],
                target_noise_prior_mu[l], target_noise_prior_logvar[l]
            )
        
        loss_t = self.mse_loss(reconstructed_target_od, target_od) + kl_target_zn_loss
        
        # ====== æ€»æŸå¤± ======
        total_loss = loss_p + loss_s + loss_t
        
        # é¢å¤–çš„è¯„ä¼°æŒ‡æ ‡
        mae_loss = self.mae_loss(reconstructed_od, target_od)
        
        return {
            'od_flows': reconstructed_od,
            'total_loss': total_loss,
            'loss_p': loss_p,
            'loss_s': loss_s,
            'loss_t': loss_t,
            'feat_recon_loss': feat_recon_loss,
            'od_recon_loss': od_recon_loss,
            'kl_z_loss': kl_z_loss,
            'kl_zn_loss': kl_zn_loss,
            'mse_loss': od_recon_loss,  # ä¸ºäº†å…¼å®¹æ€§
            'mae_loss': mae_loss
        }
    
    def _forward_eval(self, features):
        """æ¨ç†æ¨¡å¼å‰å‘ä¼ æ’­ - æ¡ä»¶ç”Ÿæˆ"""
        
        with torch.no_grad():
            # ä»ç‰¹å¾ç¼–ç å¾—åˆ°å†…å®¹æ½œåœ¨å˜é‡
            z_mu, z_logvar, z_list = self.feature_encoder(features)
            
            # ä»å™ªå£°å…ˆéªŒé‡‡æ ·å¾—åˆ°å™ªå£°æ½œåœ¨å˜é‡
            noise_prior_mu, noise_prior_logvar = self.decoder.get_noise_prior(z_list)
            zn_list = []
            for l in range(self.num_layers):
                zn = self._sample_gaussian(noise_prior_mu[l], noise_prior_logvar[l])
                zn_list.append(zn)
            
            # è§£ç ç”ŸæˆODæµé‡
            predicted_od = self.decoder.decode_od_flows(z_list, zn_list)
            
            return {
                'od_flows': predicted_od
            }
    
    def _kl_divergence_gaussian(self, mu1, logvar1, mu2, logvar2):
        """è®¡ç®—ä¸¤ä¸ªé«˜æ–¯åˆ†å¸ƒä¹‹é—´çš„KLæ•£åº¦"""
        var1 = torch.exp(logvar1)
        var2 = torch.exp(logvar2)
        
        kl = 0.5 * (logvar2 - logvar1 + (var1 + (mu1 - mu2).pow(2)) / var2 - 1)
        return kl.sum(dim=-1).mean()
    
    def _sample_gaussian(self, mu, logvar):
        """ä»é«˜æ–¯åˆ†å¸ƒé‡‡æ ·"""
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    def generate(self, features):
        """ç”ŸæˆODæµé‡é¢„æµ‹ - ä¿æŒä¸åŸä»£ç æ¥å£ä¸€è‡´"""
        with torch.no_grad():
            result = self.forward(features, mode='eval')
            return result['od_flows']

# ========== ç®€åŒ–çš„æ•°æ®é›†ç±»ï¼ˆä¿æŒä¸åŸä»£ç ä¸€è‡´ï¼‰==========
class SimpleODFlowDataset(Dataset):
    """ç®€åŒ–çš„ODæµé‡æ•°æ®é›† - ä¸åŸä»£ç å®Œå…¨ä¿æŒä¸€è‡´"""
    def __init__(self, io_flow_path, graph_path, od_matrix_path, test_ratio=0.2, val_ratio=0.1, seed=42):
        super().__init__()
        
        # åŠ è½½æ•°æ®
        self.io_flow = np.load(io_flow_path)  # (æ—¶é—´æ­¥, ç«™ç‚¹æ•°, 4)
        self.graph = np.load(graph_path)      # (ç«™ç‚¹æ•°, ç«™ç‚¹æ•°)  
        self.od_matrix = np.load(od_matrix_path)  # (æ—¶é—´æ­¥, ç«™ç‚¹æ•°, ç«™ç‚¹æ•°)
        
        # è½¬æ¢ç»´åº¦é¡ºåºï¼šä» (æ—¶é—´æ­¥, ç«™ç‚¹æ•°, 4) åˆ° (ç«™ç‚¹æ•°, æ—¶é—´æ­¥, 4)
        if self.io_flow.shape[0] == 28:  # å¦‚æœç¬¬ä¸€ä¸ªç»´åº¦æ˜¯æ—¶é—´æ­¥
            self.io_flow = np.transpose(self.io_flow, (1, 0, 2))
        
        # è½¬æ¢ç»´åº¦é¡ºåºï¼šä» (æ—¶é—´æ­¥, ç«™ç‚¹æ•°, ç«™ç‚¹æ•°) åˆ° (ç«™ç‚¹æ•°, ç«™ç‚¹æ•°, æ—¶é—´æ­¥)  
        if self.od_matrix.shape[0] == 28:  # å¦‚æœç¬¬ä¸€ä¸ªç»´åº¦æ˜¯æ—¶é—´æ­¥
            self.od_matrix = np.transpose(self.od_matrix, (1, 2, 0))
        
        self.num_nodes = self.io_flow.shape[0]
        self.time_steps = self.io_flow.shape[1]
        
        print(f"æ•°æ®ç»´åº¦: IOæµé‡{self.io_flow.shape}, å›¾{self.graph.shape}, ODçŸ©é˜µ{self.od_matrix.shape}")
        
        # æ•°æ®ä¸€è‡´æ€§éªŒè¯ - ç¡®ä¿æ‰€æœ‰æ•°æ®çš„èŠ‚ç‚¹ç»´åº¦åŒ¹é…
        assert self.graph.shape[0] == self.graph.shape[1], f"å›¾æ•°æ®å¿…é¡»æ˜¯æ–¹é˜µ: {self.graph.shape}"
        assert self.io_flow.shape[0] == self.graph.shape[0], f"IOæµé‡èŠ‚ç‚¹æ•°ä¸å›¾èŠ‚ç‚¹æ•°ä¸åŒ¹é…: {self.io_flow.shape[0]} vs {self.graph.shape[0]}"
        assert self.od_matrix.shape[0] == self.graph.shape[0], f"ODçŸ©é˜µèŠ‚ç‚¹æ•°ä¸å›¾èŠ‚ç‚¹æ•°ä¸åŒ¹é…: {self.od_matrix.shape[0]} vs {self.graph.shape[0]}"
        assert self.od_matrix.shape[1] == self.graph.shape[0], f"ODçŸ©é˜µèŠ‚ç‚¹æ•°ä¸å›¾èŠ‚ç‚¹æ•°ä¸åŒ¹é…: {self.od_matrix.shape[1]} vs {self.graph.shape[0]}"
        
        print(f"âœ… æ•°æ®ä¸€è‡´æ€§éªŒè¯é€šè¿‡: {self.num_nodes}ä¸ªèŠ‚ç‚¹, {self.time_steps}ä¸ªæ—¶é—´æ­¥")
        
        # ç«™ç‚¹å¯¹åˆ—è¡¨ - ä¸åŸç‰ˆä¿æŒä¸€è‡´ï¼Œä½¿ç”¨æ‰€æœ‰ç«™ç‚¹å¯¹
        self.od_pairs = []
        for i in range(self.num_nodes):
            for j in range(i + 1, self.num_nodes):
                self.od_pairs.append((i, j))
        
        print(f"ç”Ÿæˆ{len(self.od_pairs)}ä¸ªç«™ç‚¹å¯¹ç”¨äºè®­ç»ƒ")
        
        # åŠ è½½ç«™ç‚¹äººå£å¯†åº¦æ•°æ® - ä¼˜å…ˆä½¿ç”¨52èŠ‚ç‚¹ç‰ˆæœ¬
        population_files = [
            "/private/od/data_NYTaxi/grid_population_density_52nodes.json",  # ä¼˜å…ˆä½¿ç”¨52èŠ‚ç‚¹ç‰ˆæœ¬
            "/private/od/data_NYTaxi/grid_population_density.json",  # å¤‡ç”¨ç‰ˆæœ¬
            "/private/od/data/station_p.json"  # åŸå§‹å¤‡ç”¨
        ]
        
        self.station_data = []
        for pop_file in population_files:
            if os.path.exists(pop_file):
                try:
                    with open(pop_file, "r", encoding="utf-8") as f:
                        self.station_data = json.load(f)
                    print(f"âœ… åŠ è½½äººå£å¯†åº¦æ•°æ®: {pop_file}, å…±{len(self.station_data)}ä¸ªåŒºåŸŸ")
                    break
                except Exception as e:
                    print(f"âš ï¸ åŠ è½½{pop_file}å¤±è´¥: {str(e)}")
                    continue
        
        if not self.station_data:
            print("âš ï¸ æ‰€æœ‰äººå£å¯†åº¦æ•°æ®æ–‡ä»¶éƒ½æ— æ³•åŠ è½½ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            self.station_data = []
        
        # éªŒè¯äººå£å¯†åº¦æ•°æ®æ•°é‡
        if self.station_data and len(self.station_data) != self.num_nodes:
            print(f"âš ï¸ äººå£å¯†åº¦æ•°æ®æ•°é‡({len(self.station_data)})ä¸èŠ‚ç‚¹æ•°é‡({self.num_nodes})ä¸åŒ¹é…")
        
        # æ•°æ®é›†åˆ’åˆ† - ä½¿ç”¨8:1:1çš„ä¸¥æ ¼åˆ’åˆ†
        all_indices = list(range(len(self.od_pairs)))
        random.seed(seed)
        random.shuffle(all_indices)
        
        total_samples = len(all_indices)
        
        # è®¡ç®—åˆ’åˆ†ç‚¹ - ç¡®ä¿8:1:1çš„æ¯”ä¾‹
        train_size = int(total_samples * 0.8)  # 80%è®­ç»ƒé›†
        val_size = int(total_samples * 0.1)    # 10%éªŒè¯é›†  
        test_size = total_samples - train_size - val_size  # å‰©ä½™ä¸ºæµ‹è¯•é›†
        
        # é‡æ–°åˆ’åˆ†ï¼Œç¡®ä¿æ²¡æœ‰é‡å 
        self.train_indices = all_indices[:train_size]
        self.val_indices = all_indices[train_size:train_size + val_size]
        self.test_indices = all_indices[train_size + val_size:]
        
        print(f"æ•°æ®é›†åˆ’åˆ†å®Œæˆ:")
        print(f"  è®­ç»ƒé›†: {len(self.train_indices)} æ ·æœ¬ ({len(self.train_indices)/total_samples:.1%})")
        print(f"  éªŒè¯é›†: {len(self.val_indices)} æ ·æœ¬ ({len(self.val_indices)/total_samples:.1%})")
        print(f"  æµ‹è¯•é›†: {len(self.test_indices)} æ ·æœ¬ ({len(self.test_indices)/total_samples:.1%})")
        
        self.set_mode('train')
    
    def set_mode(self, mode):
        """è®¾ç½®æ•°æ®é›†æ¨¡å¼"""
        if mode == 'train':
            self.current_indices = self.train_indices
        elif mode == 'val':
            self.current_indices = self.val_indices
        elif mode == 'test':
            self.current_indices = self.test_indices
        else:
            raise ValueError(f"Unknown mode: {mode}")
    
    def __len__(self):
        return len(self.current_indices)
    
    def __getitem__(self, idx):
        # è·å–ç«™ç‚¹å¯¹
        site_pair_idx = self.current_indices[idx]
        site_i, site_j = self.od_pairs[site_pair_idx]
        
        # è·å–ODæµé‡
        od_i_to_j = self.od_matrix[site_i, site_j, :]  # (æ—¶é—´æ­¥,)
        od_j_to_i = self.od_matrix[site_j, site_i, :]  # (æ—¶é—´æ­¥,)
        od_flows = np.stack([od_i_to_j, od_j_to_i], axis=1)  # (æ—¶é—´æ­¥, 2)
        
        # è·å–IOæµé‡
        io_flow_i = self.io_flow[site_i, :, :]  # (æ—¶é—´æ­¥, 2)
        io_flow_j = self.io_flow[site_j, :, :]  # (æ—¶é—´æ­¥, 2)
        
        # ç®€å•å½’ä¸€åŒ–
        def normalize_data(data):
            data = np.nan_to_num(data, nan=0.0)
            data_min = np.min(data)
            data_max = np.max(data)
            if data_max > data_min:
                return (data - data_min) / (data_max - data_min)
            else:
                return data * 0
        
        io_flow_i = normalize_data(io_flow_i)
        io_flow_j = normalize_data(io_flow_j)
        od_flows = normalize_data(od_flows)
        
        # è·å–è·ç¦»ç‰¹å¾
        distance = self.graph[site_i, site_j]
        distance_normalized = distance / np.max(self.graph) if np.max(self.graph) > 0 else 0
        
        # è·å–ç«™ç‚¹äººå£å¯†åº¦å¹¶å½’ä¸€åŒ– - ä¸åŸç‰ˆä¿æŒä¸€è‡´
        if hasattr(self, 'station_data') and len(self.station_data) > 0:
            # ç¡®ä¿ç«™ç‚¹ç´¢å¼•ä¸è¶…è¿‡å¯ç”¨çš„ç«™ç‚¹æ•°æ®
            if site_i < len(self.station_data) and site_j < len(self.station_data):
                pop_density_i = self.station_data[site_i].get('grid_population_density', 0.0)
                pop_density_j = self.station_data[site_j].get('grid_population_density', 0.0)
            else:
                # å¦‚æœç«™ç‚¹ç´¢å¼•è¶…å‡ºèŒƒå›´ï¼Œä½¿ç”¨é»˜è®¤å€¼
                pop_density_i = 0.0
                pop_density_j = 0.0
                
            # è®¡ç®—äººå£å¯†åº¦ç‰¹å¾ï¼ˆä¸¤ç«™ç‚¹äººå£å¯†åº¦çš„å¹³å‡å€¼ï¼‰
            pop_density = (pop_density_i + pop_density_j) / 2
            
            # äººå£å¯†åº¦å½’ä¸€åŒ– - ä½¿ç”¨æ‰€æœ‰ç«™ç‚¹çš„æœ€å¤§äººå£å¯†åº¦å½’ä¸€åŒ–
            max_pop_density = max([station.get('grid_population_density', 1.0) for station in self.station_data])
            if max_pop_density == 0:
                max_pop_density = 1.0
            
            pop_density_normalized = pop_density / max_pop_density
        else:
            # å¦‚æœæ²¡æœ‰äººå£å¯†åº¦æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤å€¼
            pop_density_normalized = 0.0
        
        # æ„å»ºç‰¹å¾ï¼šIOæµé‡ + è·ç¦»ç‰¹å¾ + äººå£å¯†åº¦ç‰¹å¾
        distance_feature = np.ones((self.time_steps, 1)) * distance_normalized
        pop_density_feature = np.ones((self.time_steps, 1)) * pop_density_normalized
        features = np.concatenate([io_flow_i, io_flow_j, distance_feature, pop_density_feature], axis=1)  # (æ—¶é—´æ­¥, 6)
        
        return torch.FloatTensor(features), torch.FloatTensor(od_flows)

# ========== è¯„ä¼°æŒ‡æ ‡è®¡ç®—å‡½æ•° ==========
def calculate_metrics(model, dataloader, device, desc="Evaluating"):
    """è®¡ç®—è¯¦ç»†çš„è¯„ä¼°æŒ‡æ ‡ï¼šMSEã€RMSEã€MAEã€PCC"""
    model.eval()
    all_predictions = []
    all_targets = []
    total_losses = []
    
    with torch.no_grad():
        progress = tqdm(dataloader, desc=desc, leave=False)
        for features, od_flows in progress:
            features = features.to(device)
            od_flows = od_flows.to(device)
            
            # ç”Ÿæˆé¢„æµ‹
            predicted = model.generate(features)
            
            # è®¡ç®—æŸå¤±
            loss = F.mse_loss(predicted, od_flows)
            total_losses.append(loss.item())
            
            # æ”¶é›†é¢„æµ‹ç»“æœ
            all_predictions.append(predicted.cpu().numpy())
            all_targets.append(od_flows.cpu().numpy())
            
            progress.set_postfix({'MSE': f'{loss.item():.6f}'})
    
    # åˆå¹¶æ‰€æœ‰é¢„æµ‹ç»“æœ
    all_predictions = np.concatenate(all_predictions, axis=0)
    all_targets = np.concatenate(all_targets, axis=0)
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    mse = np.mean((all_predictions - all_targets) ** 2)
    rmse = np.sqrt(mse)
    mae = np.mean(np.abs(all_predictions - all_targets))
    
    # è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°(PCC) - ä¼˜åŒ–è®¡ç®—ä»¥æé«˜å‡†ç¡®æ€§
    pred_flat = all_predictions.flatten()
    target_flat = all_targets.flatten()
    
    # æ›´ä¸¥æ ¼çš„æ•°æ®æ¸…ç†
    valid_mask = ~(np.isnan(pred_flat) | np.isnan(target_flat) | np.isinf(pred_flat) | np.isinf(target_flat))
    
    if np.sum(valid_mask) > 10:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æœ‰æ•ˆæ•°æ®ç‚¹
        pred_valid = pred_flat[valid_mask]
        target_valid = target_flat[valid_mask]
        
        # æ£€æŸ¥æ–¹å·®æ˜¯å¦ä¸º0ï¼ˆé¿å…é™¤é›¶é”™è¯¯ï¼‰
        if np.var(pred_valid) > 1e-10 and np.var(target_valid) > 1e-10:
            try:
                correlation_matrix = np.corrcoef(pred_valid, target_valid)
                pcc = correlation_matrix[0, 1]
                
                # ç¡®ä¿PCCåœ¨åˆç†èŒƒå›´å†…
                if np.isnan(pcc) or np.isinf(pcc):
                    pcc = 0.0
                else:
                    pcc = np.clip(pcc, -1.0, 1.0)  # é™åˆ¶åœ¨[-1, 1]èŒƒå›´å†…
            except Exception as e:
                print(f"âš ï¸ PCCè®¡ç®—å¼‚å¸¸: {e}")
                pcc = 0.0
        else:
            # å¦‚æœæ–¹å·®ä¸º0ï¼Œè¯´æ˜é¢„æµ‹å€¼æˆ–ç›®æ ‡å€¼æ˜¯å¸¸æ•°
            pcc = 0.0
    else:
        pcc = 0.0
    
    avg_loss = np.mean(total_losses)
    
    return {
        'loss': float(avg_loss),
        'mse': float(mse), 
        'rmse': float(rmse),
        'mae': float(mae),
        'pcc': float(pcc)
    }

# ========== SeNM-VAEè®­ç»ƒå‡½æ•° ==========
def train_senm_vae_model(args):
    """è®­ç»ƒSeNM-VAE ODæµé‡é¢„æµ‹æ¨¡å‹"""
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = SimpleODFlowDataset(
        io_flow_path=args.io_flow_path,
        graph_path=args.graph_path,
        od_matrix_path=args.od_matrix_path,
        test_ratio=args.test_ratio,
        val_ratio=args.val_ratio,
        seed=args.seed
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        dataset, 
        batch_size=args.batch_size, 
        shuffle=True, 
        num_workers=0
    )
    
    dataset.set_mode('val')
    val_loader = DataLoader(
        dataset, 
        batch_size=args.batch_size, 
        shuffle=False, 
        num_workers=0
    )
    
    dataset.set_mode('test')
    test_loader = DataLoader(
        dataset, 
        batch_size=args.batch_size, 
        shuffle=False, 
        num_workers=0
    )
    
    dataset.set_mode('train')
    
    # åˆ›å»ºSeNM-VAEæ¨¡å‹
    model = SeNMVAEODFlowPredictor(
        input_dim=6,
        hidden_dim=args.hidden_dim,
        latent_dim=args.latent_dim,
        time_steps=28,
        output_dim=2,
        num_layers=args.num_layers
    ).to(device)
    
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"SeNM-VAEæ¨¡å‹åˆ›å»ºæˆåŠŸï¼")
    print(f"  æ€»å‚æ•°æ•°é‡: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"  éšè—ç»´åº¦: {args.hidden_dim}")
    print(f"  æ½œåœ¨ç»´åº¦: {args.latent_dim}")
    print(f"  åˆ†å±‚æ•°é‡: {args.num_layers}")
    
    # ä¼˜åŒ–å™¨ - ä½¿ç”¨Adamä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam(
        model.parameters(), 
        lr=args.lr, 
        betas=(0.9, 0.999),
        weight_decay=args.weight_decay
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=args.patience, verbose=True, min_lr=1e-6
    )
    
    # è®­ç»ƒå¾ªç¯å˜é‡
    best_val_loss = float('inf')
    best_model_path = os.path.join(args.output_dir, 'best_senm_vae_od_model.pth')
    epochs_without_improvement = 0
    train_history = []
    
    print(f"\nå¼€å§‹è®­ç»ƒSeNM-VAE ODæµé‡é¢„æµ‹æ¨¡å‹...")
    print(f"æ¨¡å‹å°†ä¿å­˜åˆ°: {best_model_path}")
    print(f"æ—©åœç­–ç•¥: éªŒè¯æŸå¤±{args.early_stop_patience}è½®æ— æ”¹å–„æ—¶åœæ­¢è®­ç»ƒ")
    print("="*80)
    
    for epoch in range(args.epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_losses = []
        train_loss_p = []
        train_loss_s = []
        train_loss_t = []
        train_feat_recon_losses = []
        train_od_recon_losses = []
        train_kl_z_losses = []
        train_kl_zn_losses = []
        
        train_progress = tqdm(train_loader, desc=f"Epoch {epoch+1:3d}/{args.epochs} [è®­ç»ƒ]")
        for features, od_flows in train_progress:
            features = features.to(device)
            od_flows = od_flows.to(device)
            
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            outputs = model(features, od_flows, mode='train')
            total_loss = outputs['total_loss']
            loss_p = outputs['loss_p']
            loss_s = outputs['loss_s']
            loss_t = outputs['loss_t']
            feat_recon_loss = outputs['feat_recon_loss']
            od_recon_loss = outputs['od_recon_loss']
            kl_z_loss = outputs['kl_z_loss']
            kl_zn_loss = outputs['kl_zn_loss']
            
            # åå‘ä¼ æ’­
            total_loss.backward()
            
            # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            # è®°å½•æŸå¤±
            train_losses.append(total_loss.item())
            train_loss_p.append(loss_p.item())
            train_loss_s.append(loss_s.item())
            train_loss_t.append(loss_t.item())
            train_feat_recon_losses.append(feat_recon_loss.item())
            train_od_recon_losses.append(od_recon_loss.item())
            train_kl_z_losses.append(kl_z_loss.item())
            train_kl_zn_losses.append(kl_zn_loss.item())
            
            # æ›´æ–°è¿›åº¦æ¡
            train_progress.set_postfix({
                'Total': f'{total_loss.item():.4f}',
                'Lp': f'{loss_p.item():.4f}',
                'Ls': f'{loss_s.item():.4f}',
                'Lt': f'{loss_t.item():.4f}',
                'OD': f'{od_recon_loss.item():.4f}'
            })
        
        # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
        avg_train_loss = np.mean(train_losses)
        avg_train_loss_p = np.mean(train_loss_p)
        avg_train_loss_s = np.mean(train_loss_s)
        avg_train_loss_t = np.mean(train_loss_t)
        avg_train_feat_recon = np.mean(train_feat_recon_losses)
        avg_train_od_recon = np.mean(train_od_recon_losses)
        avg_train_kl_z = np.mean(train_kl_z_losses)
        avg_train_kl_zn = np.mean(train_kl_zn_losses)
        
        # éªŒè¯é˜¶æ®µ - è®¡ç®—è¯¦ç»†æŒ‡æ ‡
        print(f"  ğŸ” è®¡ç®—éªŒè¯é›†æŒ‡æ ‡...")
        val_metrics = calculate_metrics(model, val_loader, device, desc="éªŒè¯é›†è¯„ä¼°")
        
        # å­¦ä¹ ç‡è°ƒæ•´
        scheduler.step(val_metrics['loss'])
        current_lr = optimizer.param_groups[0]['lr']
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
        is_best = val_metrics['loss'] < best_val_loss
        test_metrics = None
        
        if is_best:
            # åªåœ¨éªŒè¯é›†æ€§èƒ½æå‡æ—¶è¯„ä¼°æµ‹è¯•é›†
            print(f"  ğŸ¯ æ–°æœ€ä½³éªŒè¯æŸå¤±! è¯„ä¼°æµ‹è¯•é›†...")
            test_metrics = calculate_metrics(model, test_loader, device, desc="æµ‹è¯•é›†è¯„ä¼°")
            best_val_loss = val_metrics['loss']
            epochs_without_improvement = 0
        else:
            epochs_without_improvement += 1
            # ä½¿ç”¨ä¸Šä¸€æ¬¡æœ€ä½³çš„æµ‹è¯•æŒ‡æ ‡
            if os.path.exists(best_model_path):
                try:
                    checkpoint = torch.load(best_model_path, map_location=device)
                    test_metrics = checkpoint.get('test_metrics', {})
                except:
                    test_metrics = {}
        
        # æ‰“å°è¯¦ç»†ç»“æœ
        print(f"\nğŸ“Š Epoch {epoch+1:3d}/{args.epochs} è®­ç»ƒå®Œæˆ:")
        print(f"{'='*80}")
        print(f"ğŸ”¹ è®­ç»ƒé›†:")
        print(f"   æ€»æŸå¤±: {avg_train_loss:.6f} | é…å¯¹åŸŸæŸå¤±: {avg_train_loss_p:.6f}")
        print(f"   æºåŸŸæŸå¤±: {avg_train_loss_s:.6f} | ç›®æ ‡åŸŸæŸå¤±: {avg_train_loss_t:.6f}")
        print(f"   ç‰¹å¾é‡æ„: {avg_train_feat_recon:.6f} | ODé‡æ„: {avg_train_od_recon:.6f}")
        print(f"   KL_zæŸå¤±: {avg_train_kl_z:.6f} | KL_znæŸå¤±: {avg_train_kl_zn:.6f}")
        
        print(f"ğŸ”¹ éªŒè¯é›†:")
        print(f"   æ€»æŸå¤±: {val_metrics['loss']:.6f} | MSE: {val_metrics['mse']:.6f}")
        print(f"   RMSE: {val_metrics['rmse']:.6f} | MAE: {val_metrics['mae']:.6f} | PCC: {val_metrics['pcc']:.6f}")
        
        if test_metrics:
            print(f"ğŸ”¹ æµ‹è¯•é›†:")  
            print(f"   æ€»æŸå¤±: {test_metrics.get('loss', 0):.6f} | MSE: {test_metrics.get('mse', 0):.6f}")
            print(f"   RMSE: {test_metrics.get('rmse', 0):.6f} | MAE: {test_metrics.get('mae', 0):.6f} | PCC: {test_metrics.get('pcc', 0):.6f}")
        else:
            print(f"ğŸ”¹ æµ‹è¯•é›†: æœªè¯„ä¼° (ä»…åœ¨éªŒè¯é›†æ”¹å–„æ—¶è¯„ä¼°)")
        
        print(f"ğŸ”¹ å­¦ä¹ ç‡: {current_lr:.2e}")
        
        # ä¿å­˜è®­ç»ƒå†å² - è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹
        epoch_history = {
            'epoch': int(epoch + 1),
            'train_loss': float(avg_train_loss),
            'train_loss_p': float(avg_train_loss_p),
            'train_loss_s': float(avg_train_loss_s),
            'train_loss_t': float(avg_train_loss_t),
            'train_feat_recon_loss': float(avg_train_feat_recon),
            'train_od_recon_loss': float(avg_train_od_recon),
            'train_kl_z_loss': float(avg_train_kl_z),
            'train_kl_zn_loss': float(avg_train_kl_zn),
            'val_loss': float(val_metrics['loss']),
            'val_mse': float(val_metrics['mse']),
            'val_rmse': float(val_metrics['rmse']),
            'val_mae': float(val_metrics['mae']),
            'val_pcc': float(val_metrics['pcc']),
            'lr': float(current_lr),
            'is_best': bool(is_best)
        }
        
        # æ·»åŠ æµ‹è¯•é›†æŒ‡æ ‡ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if test_metrics:
            epoch_history.update({
                'test_loss': float(test_metrics.get('loss', 0)),
                'test_mse': float(test_metrics.get('mse', 0)),
                'test_rmse': float(test_metrics.get('rmse', 0)),
                'test_mae': float(test_metrics.get('mae', 0)),
                'test_pcc': float(test_metrics.get('pcc', 0))
            })
        
        train_history.append(epoch_history)
        
        # è¾¹è®­ç»ƒè¾¹ä¿å­˜è®­ç»ƒæ—¥å¿— - ä½¿ç”¨æ–‡æœ¬æ ¼å¼
        log_file = os.path.join(args.output_dir, "training_log.txt")
        try:
            # å¦‚æœæ˜¯ç¬¬ä¸€è½®ï¼Œåˆ›å»ºæ–°æ–‡ä»¶ï¼›å¦åˆ™è¿½åŠ 
            mode = 'w' if epoch == 0 else 'a'
            with open(log_file, mode, encoding='utf-8') as f:
                if epoch == 0:
                    f.write("SeNM-VAE ODæµé‡é¢„æµ‹æ¨¡å‹è®­ç»ƒæ—¥å¿—\n")
                    f.write("=" * 50 + "\n")
                
                f.write(f"Epoch {epoch+1}/{args.epochs}\n")
                f.write(f"   Training - Total: {avg_train_loss:.6f}, Lp: {avg_train_loss_p:.6f}, Ls: {avg_train_loss_s:.6f}, Lt: {avg_train_loss_t:.6f}\n")
                f.write(f"   Training - FeatRecon: {avg_train_feat_recon:.6f}, ODRecon: {avg_train_od_recon:.6f}, KL_z: {avg_train_kl_z:.6f}, KL_zn: {avg_train_kl_zn:.6f}\n")
                f.write(f"   Validation - Loss: {val_metrics['loss']:.6f}, RMSE: {val_metrics['rmse']:.6f}, MAE: {val_metrics['mae']:.6f}, PCC: {val_metrics['pcc']:.6f}\n")
                
                if test_metrics:
                    f.write(f"   Test - Loss: {test_metrics.get('loss', 0):.6f}, RMSE: {test_metrics.get('rmse', 0):.6f}, MAE: {test_metrics.get('mae', 0):.6f}, PCC: {test_metrics.get('pcc', 0):.6f}\n")
                
                if is_best:
                    f.write(f"   New best model saved (Val Loss: {best_val_loss:.6f}, Val RMSE: {val_metrics['rmse']:.6f}, Val PCC: {val_metrics['pcc']:.6f})\n")
                else:
                    f.write(f"   No improvement ({epochs_without_improvement}/{args.early_stop_patience} epochs without improvement)\n")
                
                f.write(f"   Learning Rate: {current_lr:.2e}\n")
                f.write("\n")
                f.flush()
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜è®­ç»ƒæ—¥å¿—å¤±è´¥: {e}")
        
        # ä»ç„¶ä¿å­˜JSONæ ¼å¼çš„è¯¦ç»†å†å²æ•°æ®ç”¨äºåç»­åˆ†æ
        history_file = os.path.join(args.output_dir, "training_history.json")
        try:
            with open(history_file, "w", encoding='utf-8') as f:
                json.dump(train_history, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜è¯¦ç»†å†å²å¤±è´¥: {e}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            torch.save({
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'epoch': epoch,
                'val_loss': val_metrics['loss'],
                'val_metrics': val_metrics,
                'test_metrics': test_metrics,
                'train_history': train_history,
                'args': args
            }, best_model_path)
            print(f"ğŸ¯ âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (éªŒè¯æŸå¤±: {best_val_loss:.6f})")
        else:
            print(f"â³ éªŒè¯æŸå¤±æœªæ”¹å–„ ({epochs_without_improvement}/{args.early_stop_patience}è½®)")
        
        # æ—©åœæ£€æŸ¥
        if epochs_without_improvement >= args.early_stop_patience:
            print(f"\nğŸ›‘ æ—©åœè§¦å‘! éªŒè¯æŸå¤±å·²{args.early_stop_patience}è½®æœªæ”¹å–„ï¼Œåœæ­¢è®­ç»ƒ")
            print(f"   æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f} (æ¥è‡ªç¬¬{epoch - epochs_without_improvement + 2}è½®)")
            break
        
        # å­¦ä¹ ç‡è¿‡å°æ£€æŸ¥
        if current_lr < 1e-6:
            print(f"\nğŸ›‘ å­¦ä¹ ç‡è¿‡å° ({current_lr:.2e})ï¼Œåœæ­¢è®­ç»ƒ")
            break
        
        print("="*80)
    
    log_file = os.path.join(args.output_dir, "training_log.txt")
    history_file = os.path.join(args.output_dir, "training_history.json")
    print(f"ğŸ“ è®­ç»ƒæ—¥å¿—å·²å®æ—¶ä¿å­˜åˆ°: {log_file}")
    print(f"ğŸ“ è¯¦ç»†å†å²æ•°æ®å·²ä¿å­˜åˆ°: {history_file}")
    
    # æœ€ç»ˆæµ‹è¯•é˜¶æ®µ - åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆè¯„ä¼°
    print(f"\n{'='*60}")
    print("ğŸ¯ æœ€ç»ˆæµ‹è¯•é˜¶æ®µ - ä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œè¯„ä¼°")
    print(f"{'='*60}")
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    if os.path.exists(best_model_path):
        checkpoint = torch.load(best_model_path, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        best_epoch = checkpoint['epoch'] + 1
        best_val_metrics = checkpoint.get('val_metrics', {})
        best_test_metrics = checkpoint.get('test_metrics', {})
        print(f"âœ… å·²åŠ è½½æœ€ä½³æ¨¡å‹ (æ¥è‡ªç¬¬{best_epoch}è½®)")
        
        # å±•ç¤ºæœ€ä½³æ¨¡å‹çš„æ€§èƒ½
        print(f"\nğŸ† æœ€ä½³æ¨¡å‹æ€§èƒ½ (ç¬¬{best_epoch}è½®):")
        print(f"ğŸ”¸ éªŒè¯é›†: Loss={best_val_metrics.get('loss', 0):.6f}, RMSE={best_val_metrics.get('rmse', 0):.6f}, MAE={best_val_metrics.get('mae', 0):.6f}, PCC={best_val_metrics.get('pcc', 0):.6f}")
        print(f"ğŸ”¸ æµ‹è¯•é›†: Loss={best_test_metrics.get('loss', 0):.6f}, RMSE={best_test_metrics.get('rmse', 0):.6f}, MAE={best_test_metrics.get('mae', 0):.6f}, PCC={best_test_metrics.get('pcc', 0):.6f}")
        
        # ä½¿ç”¨ä¿å­˜çš„æµ‹è¯•æŒ‡æ ‡ä½œä¸ºæœ€ç»ˆç»“æœ
        final_test_metrics = best_test_metrics
    else:
        print("âš ï¸ æœ€ä½³æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨å½“å‰æ¨¡å‹è¿›è¡Œæœ€ç»ˆæµ‹è¯•")
        final_test_metrics = calculate_metrics(model, test_loader, device, desc="æœ€ç»ˆæµ‹è¯•")
        best_epoch = "å½“å‰"
    
    print(f"\n{'='*60}")
    print("ğŸ‰ SeNM-VAE ODæµé‡é¢„æµ‹æ¨¡å‹ - æœ€ç»ˆæµ‹è¯•ç»“æœ")
    print(f"{'='*60}")
    print(f"ğŸ“Š æœ€ç»ˆæµ‹è¯•æŒ‡æ ‡ (åŸºäºç¬¬{best_epoch}è½®æœ€ä½³æ¨¡å‹):")
    print(f"   ğŸ“ˆ å‡æ–¹è¯¯å·® (MSE):     {final_test_metrics.get('mse', 0):.6f}")
    print(f"   ğŸ“ˆ å‡æ–¹æ ¹è¯¯å·® (RMSE):   {final_test_metrics.get('rmse', 0):.6f}")
    print(f"   ğŸ“ˆ å¹³å‡ç»å¯¹è¯¯å·® (MAE):  {final_test_metrics.get('mae', 0):.6f}")
    print(f"   ğŸ“ˆ çš®å°”é€Šç›¸å…³ç³»æ•° (PCC): {final_test_metrics.get('pcc', 0):.6f}")
    print(f"   ğŸ“ˆ æµ‹è¯•æŸå¤±:          {final_test_metrics.get('loss', 0):.6f}")
    print(f"{'='*60}")
    
    # ä¸ºäº†å…¼å®¹æ€§ï¼Œè®¾ç½®è¿™äº›å˜é‡
    mse = final_test_metrics.get('mse', 0)
    rmse = final_test_metrics.get('rmse', 0) 
    mae = final_test_metrics.get('mae', 0)
    pcc = final_test_metrics.get('pcc', 0)
    avg_test_loss = final_test_metrics.get('loss', 0)
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    results_file = os.path.join(args.output_dir, "senm_vae_od_results.txt")
    with open(results_file, "w", encoding='utf-8') as f:
        f.write("åŸºäºSeNM-VAEçš„ODæµé‡é¢„æµ‹æ¨¡å‹æµ‹è¯•ç»“æœ\n")
        f.write("="*50 + "\n")
        f.write("è®ºæ–‡: SeNM-VAE: Semi-Supervised Noise Modeling with Hierarchical Variational Autoencoder (CVPR 2024)\n")
        f.write("æ¨¡å‹æ¶æ„æ ¸å¿ƒç‰¹ç‚¹:\n")
        f.write("  - åŠç›‘ç£å­¦ä¹ æ¡†æ¶ (Semi-Supervised Learning Framework)\n")
        f.write("  - åˆ†å±‚VAEæ¶æ„ (Hierarchical VAE Architecture)\n")
        f.write("  - åŒæ½œåœ¨å˜é‡è®¾è®¡ (Dual Latent Variables Design)\n")
        f.write("  - æ··åˆæ¨ç†æ¨¡å‹ (Mixture Inference Model)\n")
        f.write("  - RDBæ®‹å·®å¯†é›†å— (Residual Dense Blocks)\n")
        f.write("  - ä¸‰åŸŸæŸå¤±å‡½æ•° (Three-Domain Loss Functions)\n")
        f.write("\n")
        f.write(f"æ¨¡å‹å‚æ•°:\n")
        f.write(f"  - æ€»å‚æ•°æ•°é‡: {total_params:,}\n")
        f.write(f"  - å¯è®­ç»ƒå‚æ•°: {trainable_params:,}\n")
        f.write(f"  - éšè—ç»´åº¦: {args.hidden_dim}\n")
        f.write(f"  - æ½œåœ¨ç»´åº¦: {args.latent_dim}\n")
        f.write(f"  - åˆ†å±‚æ•°é‡: {args.num_layers}\n")
        f.write(f"  - è®­ç»ƒè½®æ•°: {args.epochs}\n")
        f.write(f"  - æ‰¹æ¬¡å¤§å°: {args.batch_size}\n")
        f.write(f"  - å­¦ä¹ ç‡: {args.lr}\n")
        f.write("\n")
        f.write("æµ‹è¯•ç»“æœ:\n")
        f.write(f"  å‡æ–¹è¯¯å·® (MSE):     {mse:.6f}\n")
        f.write(f"  å‡æ–¹æ ¹è¯¯å·® (RMSE):   {rmse:.6f}\n")
        f.write(f"  å¹³å‡ç»å¯¹è¯¯å·® (MAE):  {mae:.6f}\n")
        f.write(f"  çš®å°”é€Šç›¸å…³ç³»æ•° (PCC): {pcc:.6f}\n")
        f.write(f"  æµ‹è¯•æŸå¤±:          {avg_test_loss:.6f}\n")
        f.write(f"  æœ€ä½³éªŒè¯æŸå¤±:       {best_val_loss:.6f}\n")
        f.write(f"\n")
        f.write(f"æ•°æ®é›†ä¿¡æ¯:\n")
        f.write(f"  è®­ç»ƒæ ·æœ¬æ•°: {len(dataset.train_indices)}\n")
        f.write(f"  éªŒè¯æ ·æœ¬æ•°: {len(dataset.val_indices)}\n")
        f.write(f"  æµ‹è¯•æ ·æœ¬æ•°: {len(dataset.test_indices)}\n")
        f.write(f"  è¾“å…¥ç‰¹å¾ç»´åº¦: [batch_size, 28, 6]\n")
        f.write(f"  è¾“å‡ºæµé‡ç»´åº¦: [batch_size, 28, 2]\n")
    
    print(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    print(f"ğŸ“ æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: {best_model_path}")
    
    return best_model_path

# ========== ä¸»å‡½æ•° ==========
def main():
    parser = argparse.ArgumentParser(description="åŸºäºSeNM-VAEçš„ODæµé‡é¢„æµ‹æ¨¡å‹")
    
    # æ•°æ®å‚æ•° - æ›´æ–°ä¸º52èŠ‚ç‚¹æ•°æ®ç»“æ„è·¯å¾„
    parser.add_argument("--io_flow_path", type=str, default="/private/od/data_NYTaxi/io_flow_daily.npy", 
                       help="IOæµé‡æ•°æ®è·¯å¾„")
    parser.add_argument("--graph_path", type=str, default="/private/od/data_NYTaxi/graph.npy", 
                       help="å›¾ç»“æ„æ•°æ®è·¯å¾„")
    parser.add_argument("--od_matrix_path", type=str, default="/private/od/data_NYTaxi/od_matrix_daily.npy", 
                       help="ODçŸ©é˜µæ•°æ®è·¯å¾„")
    
    # SeNM-VAEæ¨¡å‹å‚æ•°
    parser.add_argument("--hidden_dim", type=int, default=128, 
                       help="éšè—ç»´åº¦ (ç¼–ç å™¨è§£ç å™¨éšè—å±‚å¤§å°)")
    parser.add_argument("--latent_dim", type=int, default=64, 
                       help="æ½œåœ¨ç©ºé—´ç»´åº¦ (VAEæ½œåœ¨å˜é‡ç»´åº¦)")
    parser.add_argument("--num_layers", type=int, default=3, 
                       help="åˆ†å±‚VAEå±‚æ•°")
    
    # è®­ç»ƒå‚æ•°  
    parser.add_argument("--batch_size", type=int, default=32, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--epochs", type=int, default=100, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--lr", type=float, default=0.001, help="å­¦ä¹ ç‡")
    parser.add_argument("--weight_decay", type=float, default=1e-5, help="æƒé‡è¡°å‡")
    parser.add_argument("--test_ratio", type=float, default=0.1, help="æµ‹è¯•é›†æ¯”ä¾‹ (å›ºå®š8:1:1åˆ’åˆ†)")
    parser.add_argument("--val_ratio", type=float, default=0.1, help="éªŒè¯é›†æ¯”ä¾‹ (å›ºå®š8:1:1åˆ’åˆ†)")
    parser.add_argument("--seed", type=int, default=82, help="éšæœºç§å­")
    
    # æ—©åœå’Œå­¦ä¹ ç‡è°ƒæ•´å‚æ•°
    parser.add_argument("--early_stop_patience", type=int, default=15, help="æ—©åœç­–ç•¥ï¼šéªŒè¯æŸå¤±å¤šå°‘è½®æ— æ”¹å–„æ—¶åœæ­¢è®­ç»ƒ")
    parser.add_argument("--patience", type=int, default=8, help="å­¦ä¹ ç‡è°ƒæ•´ç­–ç•¥ï¼šéªŒè¯æŸå¤±å¤šå°‘è½®æ— æ”¹å–„æ—¶é™ä½å­¦ä¹ ç‡")
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument("--output_dir", type=str, default="/private/od/paper_ny/SeNM_VAE", help="è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = create_dynamic_output_dir(args.output_dir)
    args.output_dir = output_dir
    
    print("="*60)
    print("ğŸš€ SeNM-VAE ODæµé‡é¢„æµ‹æ¨¡å‹")
    print("="*60)
    print("ğŸ“– è®ºæ–‡: SeNM-VAE: Semi-Supervised Noise Modeling with Hierarchical Variational Autoencoder")
    print("ğŸ“– ä¼šè®®: CVPR 2024")
    print("ğŸ“– ä½œè€…: Dihan Zheng, Yihang Zou, Xiaowen Zhang, Chenglong Bao")
    print()
    print("ğŸ”§ æ¨¡å‹åˆ›æ–°ç‚¹:")
    print("  âœ… åŠç›‘ç£å­¦ä¹ æ¡†æ¶ - åŒæ—¶åˆ©ç”¨é…å¯¹ã€æºåŸŸã€ç›®æ ‡åŸŸæ•°æ®")
    print("  âœ… åˆ†å±‚VAEæ¶æ„ - å¤šå±‚æ½œåœ¨å˜é‡å¢å¼ºè¡¨ç¤ºèƒ½åŠ›")
    print("  âœ… åŒæ½œåœ¨å˜é‡è®¾è®¡ - zæ•æ‰å†…å®¹ï¼Œznæ•æ‰åŠ¨æ€ä¿¡æ¯")
    print("  âœ… æ··åˆæ¨ç†æ¨¡å‹ - èåˆç‰¹å¾å’ŒODæµé‡çš„ç¼–ç ä¿¡æ¯")
    print("  âœ… RDBæ®‹å·®å¯†é›†å— - å¢å¼ºç‰¹å¾æå–å’Œæ¢¯åº¦æµåŠ¨")
    print("  âœ… ä¸‰åŸŸæŸå¤±å‡½æ•° - é…å¯¹åŸŸ+æºåŸŸ+ç›®æ ‡åŸŸè”åˆä¼˜åŒ–")
    print()
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print("="*60)
    
    # è®­ç»ƒæ¨¡å‹
    try:
        best_model_path = train_senm_vae_model(args)
        print("\nğŸ‰ SeNM-VAEæ¨¡å‹è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“ æœ€ä½³æ¨¡å‹ä¿å­˜ä½ç½®: {best_model_path}")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)