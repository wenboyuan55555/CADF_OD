import os
import sys
import logging
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from datetime import datetime
from tqdm import tqdm
import math
import argparse
import random
import traceback
import time
import json
import warnings
import pytz
warnings.filterwarnings("ignore")

# ========== è®¾ç½®éšæœºç§å­ ==========
def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# ========== æ—¥å¿—æ–‡ä»¶å†™å…¥å·¥å…· ==========
class FileLogger:
    def __init__(self, log_path):
        log_dir = os.path.dirname(log_path)
        if log_dir and not os.path.exists(log_dir):
            os.makedirs(log_dir, exist_ok=True)
        self.log_file = open(log_path, 'w', encoding='utf-8')
    
    def info(self, msg):
        beijing_tz = pytz.timezone('Asia/Shanghai')
        timestamp = datetime.now(beijing_tz).strftime("%Y-%m-%d %H:%M:%S")
        self.log_file.write(f"[{timestamp}] {msg}\n")
        self.log_file.flush()
    
    def close(self):
        if hasattr(self, 'log_file') and self.log_file:
            self.log_file.close()

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
)
logger = logging.getLogger(__name__)
file_logger = None

def create_dynamic_output_dir(base_dir):
    import datetime
    beijing_tz = pytz.timezone('Asia/Shanghai')
    timestamp = datetime.datetime.now(beijing_tz).strftime("%Y%m%d_%H%M%S")
    dynamic_dir = os.path.join(base_dir, f"adaptive_run_{timestamp}")
    os.makedirs(dynamic_dir, exist_ok=True)
    return dynamic_dir

# ========== ADAPTIVE æ ¸å¿ƒç»„ä»¶ ==========

class TuckERKnowledgeGraphEmbedding(nn.Module):
    """TuckERçŸ¥è¯†å›¾è°±åµŒå…¥æ¨¡å— - ADAPTIVEæ ¸å¿ƒç»„ä»¶ä¹‹ä¸€
    
    ç”¨äºå­¦ä¹ åŸå¸‚çŸ¥è¯†å›¾è°±ä¸­å®ä½“çš„ä½ç»´è¡¨ç¤º
    Reference: TuckER: Tensor Factorization for Knowledge Graph Completion (EMNLP 2019)
    """
    def __init__(self, num_entities, num_relations, entity_dim=64, relation_dim=64):
        super(TuckERKnowledgeGraphEmbedding, self).__init__()
        self.num_entities = num_entities
        self.num_relations = num_relations
        self.entity_dim = entity_dim
        self.relation_dim = relation_dim
        
        # å®ä½“åµŒå…¥çŸ©é˜µ
        self.entity_embeddings = nn.Embedding(num_entities, entity_dim)
        # å…³ç³»åµŒå…¥çŸ©é˜µ
        self.relation_embeddings = nn.Embedding(num_relations, relation_dim)
        
        # TuckERæ ¸å¿ƒå¼ é‡ - ç”¨äºå»ºæ¨¡ä¸‰å…ƒç»„çš„äº¤äº’
        self.core_tensor = nn.Parameter(torch.randn(entity_dim, relation_dim, entity_dim))
        
        # Dropout
        self.dropout = nn.Dropout(0.1)
        
        # åˆå§‹åŒ–å‚æ•°
        self._init_parameters()
        
    def _init_parameters(self):
        """åˆå§‹åŒ–å‚æ•°"""
        nn.init.xavier_uniform_(self.entity_embeddings.weight)
        nn.init.xavier_uniform_(self.relation_embeddings.weight)
        nn.init.xavier_uniform_(self.core_tensor)
    
    def forward(self, entities):
        """
        å‰å‘ä¼ æ’­è·å–å®ä½“åµŒå…¥
        Args:
            entities: å®ä½“IDåˆ—è¡¨ [batch_size] æˆ– [num_entities]
        Returns:
            å®ä½“åµŒå…¥ [batch_size, entity_dim] æˆ– [num_entities, entity_dim]
        """
        return self.entity_embeddings(entities)
    
    def compute_tucker_score(self, head, relation, tail):
        """
        è®¡ç®—TuckERä¸‰å…ƒç»„è¯„åˆ†
        Args:
            head: å¤´å®ä½“åµŒå…¥ [batch_size, entity_dim]
            relation: å…³ç³»åµŒå…¥ [batch_size, relation_dim]  
            tail: å°¾å®ä½“åµŒå…¥ [batch_size, entity_dim]
        Returns:
            è¯„åˆ† [batch_size]
        """
        # TuckERä¸‰é‡çº¿æ€§ç§¯è®¡ç®—
        # x = head @ core_tensor @ relation @ tail
        x = torch.einsum('bi,ijk,bj,bk->b', head, self.core_tensor, relation, tail)
        return torch.sigmoid(x)

class GraphConvolutionalNetwork(nn.Module):
    """å›¾å·ç§¯ç½‘ç»œæ¨¡å— - ADAPTIVEæ ¸å¿ƒç»„ä»¶
    
    ç”¨äºåœ¨åŸºç«™å›¾ä¸Šä¼ æ’­ç©ºé—´ä¿¡æ¯
    Reference: Semi-Supervised Classification with Graph Convolutional Networks (ICLR 2017)
    """
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, dropout=0.1):
        super(GraphConvolutionalNetwork, self).__init__()
        self.num_layers = num_layers
        self.dropout = dropout
        
        # GCNå±‚
        self.gcn_layers = nn.ModuleList()
        
        # ç¬¬ä¸€å±‚ï¼šinput_dim -> hidden_dim
        self.gcn_layers.append(nn.Linear(input_dim, hidden_dim))
        
        # ä¸­é—´å±‚ï¼šhidden_dim -> hidden_dim
        for _ in range(num_layers - 2):
            self.gcn_layers.append(nn.Linear(hidden_dim, hidden_dim))
        
        # æœ€åä¸€å±‚ï¼šhidden_dim -> output_dim
        if num_layers > 1:
            self.gcn_layers.append(nn.Linear(hidden_dim, output_dim))
        else:
            # åªæœ‰ä¸€å±‚çš„æƒ…å†µ
            self.gcn_layers[0] = nn.Linear(input_dim, output_dim)
    
    def forward(self, node_features, adjacency_matrix):
        """
        GCNå‰å‘ä¼ æ’­
        Args:
            node_features: èŠ‚ç‚¹ç‰¹å¾ [num_nodes, input_dim]
            adjacency_matrix: é‚»æ¥çŸ©é˜µ [num_nodes, num_nodes]
        Returns:
            è¾“å‡ºç‰¹å¾ [num_nodes, output_dim]
        """
        # è®¡ç®—åº¦çŸ©é˜µå’Œå½’ä¸€åŒ–é‚»æ¥çŸ©é˜µ
        degree_matrix = torch.diag(torch.sum(adjacency_matrix, dim=1))
        degree_matrix_inv_sqrt = torch.pow(degree_matrix, -0.5)
        degree_matrix_inv_sqrt[torch.isinf(degree_matrix_inv_sqrt)] = 0
        
        # å¯¹ç§°å½’ä¸€åŒ–: D^(-1/2) A D^(-1/2)
        normalized_adj = torch.mm(torch.mm(degree_matrix_inv_sqrt, adjacency_matrix), degree_matrix_inv_sqrt)
        
        x = node_features
        for i, layer in enumerate(self.gcn_layers):
            # å›¾å·ç§¯æ“ä½œ: A * X * W
            x = torch.mm(normalized_adj, x)
            x = layer(x)
            
            # æœ€åä¸€å±‚ä¸ä½¿ç”¨æ¿€æ´»å‡½æ•°å’Œdropout
            if i < len(self.gcn_layers) - 1:
                x = F.relu(x)
                x = F.dropout(x, p=self.dropout, training=self.training)
        
        return x

class AttentionDrivenMatching(nn.Module):
    """æ³¨æ„åŠ›é©±åŠ¨åŒ¹é…æ¨¡å— - ADAPTIVEæ ¸å¿ƒç»„ä»¶
    
    ç”¨äºå°†æ—¶é—´æ¨¡å¼ä»æºåŸå¸‚ä¼ é€’åˆ°ç›®æ ‡åŸå¸‚
    """
    def __init__(self, feature_dim, num_clusters=4):
        super(AttentionDrivenMatching, self).__init__()
        self.feature_dim = feature_dim
        self.num_clusters = num_clusters
        
        # æ³¨æ„åŠ›æƒé‡è®¡ç®—ç½‘ç»œ
        self.attention_net = nn.Sequential(
            nn.Linear(feature_dim, feature_dim // 2),
            nn.ReLU(),
            nn.Linear(feature_dim // 2, 1)
        )
        
        # ç°‡ä¸­å¿ƒè¡¨ç¤ºï¼ˆå¯å­¦ä¹ å‚æ•°ï¼‰
        self.cluster_centers = nn.Parameter(torch.randn(num_clusters, feature_dim))
        
        # æ¨¡å¼åŒ¹é…ç½‘ç»œ
        self.pattern_matching = nn.Sequential(
            nn.Linear(feature_dim, feature_dim),
            nn.ReLU(),
            nn.Linear(feature_dim, feature_dim)
        )
        
    def forward(self, node_representations):
        """
        æ³¨æ„åŠ›é©±åŠ¨çš„æ¨¡å¼åŒ¹é…
        Args:
            node_representations: èŠ‚ç‚¹è¡¨ç¤º [num_nodes, feature_dim]
        Returns:
            åŒ¹é…åçš„è¡¨ç¤º [num_nodes, feature_dim]
        """
        num_nodes = node_representations.size(0)
        
        # è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹ä¸ç°‡ä¸­å¿ƒçš„ç›¸ä¼¼åº¦
        similarities = []
        for i in range(self.num_clusters):
            center = self.cluster_centers[i].unsqueeze(0).expand(num_nodes, -1)  # [num_nodes, feature_dim]
            sim = F.cosine_similarity(node_representations, center, dim=1)  # [num_nodes]
            similarities.append(sim.unsqueeze(1))
        
        similarities = torch.cat(similarities, dim=1)  # [num_nodes, num_clusters]
        
        # æ³¨æ„åŠ›æƒé‡è®¡ç®—
        attention_weights = F.softmax(similarities, dim=1)  # [num_nodes, num_clusters]
        
        # åŠ æƒèšåˆç°‡ä¸­å¿ƒç‰¹å¾
        weighted_centers = torch.mm(attention_weights, self.cluster_centers)  # [num_nodes, feature_dim]
        
        # æ¨¡å¼åŒ¹é…ç½‘ç»œå¤„ç†
        matched_representations = self.pattern_matching(weighted_centers)
        
        # æ®‹å·®è¿æ¥
        output = node_representations + matched_representations
        
        return output

class FeatureEnhancedGenerator(nn.Module):
    """ç‰¹å¾å¢å¼ºç”Ÿæˆå™¨ - ADAPTIVEæ ¸å¿ƒç»„ä»¶
    
    ç»“åˆå¤šç§æ¨¡å¼ç”Ÿæˆæµé‡æ•°æ®ï¼ŒæŒ‰ç…§åŸè®ºæ–‡GANæ¶æ„å®ç°
    """
    def __init__(self, input_dim, feature_dim, hidden_dim, time_steps, noise_dim=32, output_dim=2):
        super(FeatureEnhancedGenerator, self).__init__()
        self.input_dim = input_dim
        self.feature_dim = feature_dim
        self.hidden_dim = hidden_dim
        self.time_steps = time_steps
        self.noise_dim = noise_dim
        self.output_dim = output_dim
        
        # å™ªå£°æŠ•å½± - GANçš„å…³é”®ç»„ä»¶
        self.noise_projection = nn.Linear(noise_dim, hidden_dim)
        
        # è¾“å…¥ç‰¹å¾æŠ•å½±
        self.input_projection = nn.Linear(input_dim, hidden_dim)
        
        # èŠ‚ç‚¹ç‰¹å¾æŠ•å½±
        self.feature_projection = nn.Linear(feature_dim, hidden_dim)
        
        # å¤šå°ºåº¦ç”Ÿæˆå™¨ï¼šæ—¥æ¨¡å¼ã€å‘¨æ¨¡å¼ã€æ®‹å·®æ¨¡å¼
        self.daily_generator = nn.Sequential(
            nn.Linear(hidden_dim * 3, hidden_dim),
            nn.LeakyReLU(0.2),
            nn.BatchNorm1d(hidden_dim),  # æ·»åŠ æ‰¹å½’ä¸€åŒ–
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LeakyReLU(0.2),
            nn.Linear(hidden_dim // 2, output_dim)  # ç§»é™¤Tanh
        )
        
        self.weekly_generator = nn.Sequential(
            nn.Linear(hidden_dim * 3, hidden_dim),
            nn.LeakyReLU(0.2),
            nn.BatchNorm1d(hidden_dim),  # æ·»åŠ æ‰¹å½’ä¸€åŒ–
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LeakyReLU(0.2),
            nn.Linear(hidden_dim // 2, output_dim)  # ç§»é™¤Tanh
        )
        
        self.residual_generator = nn.Sequential(
            nn.Linear(hidden_dim * 3, hidden_dim),
            nn.LeakyReLU(0.2),
            nn.BatchNorm1d(hidden_dim),  # æ·»åŠ æ‰¹å½’ä¸€åŒ–
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LeakyReLU(0.2),
            nn.Linear(hidden_dim // 2, output_dim)  # ç§»é™¤Tanh
        )
        
        # æ—¶åºå»ºæ¨¡ç½‘ç»œ
        self.temporal_net = nn.LSTM(
            input_size=hidden_dim * 3,  # åŒ…å«å™ªå£°
            hidden_size=hidden_dim,
            num_layers=2,
            batch_first=True,
            dropout=0.1
        )
        
        # æœ€ç»ˆèåˆç½‘ç»œ - ç§»é™¤Tanhï¼Œè®©sigmoidåœ¨forwardä¸­å¤„ç†
        self.fusion_net = nn.Sequential(
            nn.Linear(output_dim * 3, hidden_dim),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.1),  # æ·»åŠ dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LeakyReLU(0.2),
            nn.Linear(hidden_dim // 2, output_dim)  # ç§»é™¤Tanhï¼Œä½¿ç”¨çº¿æ€§è¾“å‡º
        )
        
    def forward(self, input_features, node_representations, noise=None):
        """
        ç”Ÿæˆå™¨å‰å‘ä¼ æ’­ - æŒ‰ç…§GANæ¶æ„
        Args:
            input_features: è¾“å…¥ç‰¹å¾ [batch_size, time_steps, input_dim]
            node_representations: èŠ‚ç‚¹è¡¨ç¤º [batch_size, feature_dim]
            noise: éšæœºå™ªå£° [batch_size, noise_dim] æˆ– None
        Returns:
            ç”Ÿæˆçš„ODæµé‡ [batch_size, time_steps, output_dim]
        """
        batch_size, time_steps, _ = input_features.size()
        
        # ç”Ÿæˆå™ªå£°ï¼ˆå¦‚æœæœªæä¾›ï¼‰
        if noise is None:
            noise = torch.randn(batch_size, self.noise_dim, device=input_features.device)
        
        # æŠ•å½±å™ªå£°
        projected_noise = self.noise_projection(noise)  # [batch_size, hidden_dim]
        projected_noise = projected_noise.unsqueeze(1).expand(-1, time_steps, -1)  # [batch_size, time_steps, hidden_dim]
        
        # æŠ•å½±è¾“å…¥ç‰¹å¾
        projected_input = self.input_projection(input_features)  # [batch_size, time_steps, hidden_dim]
        
        # æŠ•å½±èŠ‚ç‚¹ç‰¹å¾å¹¶æ‰©å±•æ—¶é—´ç»´åº¦
        projected_features = self.feature_projection(node_representations)  # [batch_size, hidden_dim]
        projected_features = projected_features.unsqueeze(1).expand(-1, time_steps, -1)  # [batch_size, time_steps, hidden_dim]
        
        # æ‹¼æ¥ç‰¹å¾ï¼šè¾“å…¥ + èŠ‚ç‚¹ç‰¹å¾ + å™ªå£°
        combined_features = torch.cat([projected_input, projected_features, projected_noise], dim=-1)  # [batch_size, time_steps, hidden_dim*3]
        
        # LSTMæ—¶åºå»ºæ¨¡
        lstm_output, _ = self.temporal_net(combined_features)  # [batch_size, time_steps, hidden_dim]
        
        # æ›´æ–°ç»„åˆç‰¹å¾ç”¨äºç”Ÿæˆå™¨
        combined_for_gen = torch.cat([lstm_output, projected_features, projected_noise], dim=-1)  # [batch_size, time_steps, hidden_dim*3]
        
        # å¤šå°ºåº¦ç”Ÿæˆ - éœ€è¦é‡å¡‘ç»´åº¦ä»¥é€‚åº”BatchNorm1d
        batch_size, time_steps, feature_dim = combined_for_gen.shape
        combined_reshaped = combined_for_gen.view(-1, feature_dim)  # [batch_size*time_steps, feature_dim]
        
        daily_pattern = self.daily_generator(combined_reshaped).view(batch_size, time_steps, -1)
        weekly_pattern = self.weekly_generator(combined_reshaped).view(batch_size, time_steps, -1)
        residual_pattern = self.residual_generator(combined_reshaped).view(batch_size, time_steps, -1)
        
        # æ‹¼æ¥æ‰€æœ‰æ¨¡å¼
        all_patterns = torch.cat([daily_pattern, weekly_pattern, residual_pattern], dim=-1)  # [batch_size, time_steps, output_dim*3]
        
        # æœ€ç»ˆèåˆ - é‡å¡‘ç»´åº¦å¤„ç†
        all_patterns_reshaped = all_patterns.view(-1, all_patterns.size(-1))  # [batch_size*time_steps, output_dim*3]
        output = self.fusion_net(all_patterns_reshaped).view(batch_size, time_steps, -1)  # [batch_size, time_steps, output_dim]
        
        return output

class MultiScaleDiscriminator(nn.Module):
    """å¤šå°ºåº¦åˆ¤åˆ«å™¨ - ADAPTIVE GANæ ¸å¿ƒç»„ä»¶
    
    æŒ‰ç…§åŸè®ºæ–‡å®ç°ï¼ŒåŒºåˆ†çœŸå®ä¸ç”Ÿæˆçš„æµé‡æ•°æ®
    """
    def __init__(self, input_dim=2, feature_dim=64, time_steps=28):
        super(MultiScaleDiscriminator, self).__init__()
        self.input_dim = input_dim
        self.feature_dim = feature_dim
        self.time_steps = time_steps
        
        # è¾“å…¥æŠ•å½±
        self.input_projection = nn.Conv1d(input_dim, feature_dim, kernel_size=1)
        
        # å¤šå°ºåº¦å·ç§¯åˆ¤åˆ«å™¨
        self.conv_blocks = nn.ModuleList([
            # ç¬¬ä¸€å°ºåº¦ï¼šæ•æ‰å±€éƒ¨æ¨¡å¼
            nn.Sequential(
                nn.Conv1d(feature_dim, feature_dim * 2, kernel_size=3, stride=1, padding=1),
                nn.LeakyReLU(0.2),
                nn.Conv1d(feature_dim * 2, feature_dim * 2, kernel_size=3, stride=2, padding=1),
                nn.LeakyReLU(0.2),
                nn.Dropout(0.2)
            ),
            # ç¬¬äºŒå°ºåº¦ï¼šæ•æ‰ä¸­ç­‰æ¨¡å¼
            nn.Sequential(
                nn.Conv1d(feature_dim * 2, feature_dim * 4, kernel_size=5, stride=1, padding=2),
                nn.LeakyReLU(0.2),
                nn.Conv1d(feature_dim * 4, feature_dim * 4, kernel_size=5, stride=2, padding=2),
                nn.LeakyReLU(0.2),
                nn.Dropout(0.2)
            ),
            # ç¬¬ä¸‰å°ºåº¦ï¼šæ•æ‰å…¨å±€æ¨¡å¼
            nn.Sequential(
                nn.Conv1d(feature_dim * 4, feature_dim * 8, kernel_size=7, stride=1, padding=3),
                nn.LeakyReLU(0.2),
                nn.Conv1d(feature_dim * 8, feature_dim * 8, kernel_size=7, stride=2, padding=3),
                nn.LeakyReLU(0.2),
                nn.Dropout(0.2)
            )
        ])
        
        # è‡ªé€‚åº”å¹³å‡æ± åŒ–
        self.adaptive_pool = nn.AdaptiveAvgPool1d(1)
        
        # æœ€ç»ˆåˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(feature_dim * 8, feature_dim * 4),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),
            nn.Linear(feature_dim * 4, feature_dim),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),
            nn.Linear(feature_dim, 1)  # Wasserstein GANä¸éœ€è¦sigmoid
        )
        
        # æ¢¯åº¦æƒ©ç½šæƒé‡
        self.apply(self._init_weights)
        
    def _init_weights(self, module):
        """æƒé‡åˆå§‹åŒ–"""
        if isinstance(module, (nn.Conv1d, nn.Linear)):
            nn.init.normal_(module.weight, 0.0, 0.02)
            if module.bias is not None:
                nn.init.constant_(module.bias, 0)
    
    def forward(self, x):
        """
        åˆ¤åˆ«å™¨å‰å‘ä¼ æ’­
        Args:
            x: è¾“å…¥æµé‡æ•°æ® [batch_size, time_steps, input_dim]
        Returns:
            åˆ¤åˆ«åˆ†æ•° [batch_size, 1]
        """
        # è½¬æ¢ä¸ºå·ç§¯æ ¼å¼: [batch_size, input_dim, time_steps]
        x = x.transpose(1, 2)
        
        # è¾“å…¥æŠ•å½±
        x = self.input_projection(x)
        
        # å¤šå°ºåº¦å·ç§¯å¤„ç†
        for conv_block in self.conv_blocks:
            x = conv_block(x)
        
        # å…¨å±€å¹³å‡æ± åŒ–
        x = self.adaptive_pool(x)  # [batch_size, feature_dim*8, 1]
        x = x.view(x.size(0), -1)  # [batch_size, feature_dim*8]
        
        # æœ€ç»ˆåˆ†ç±»
        output = self.classifier(x)  # [batch_size, 1]
        
        return output

class ADAPTIVEODFlowPredictor(nn.Module):
    """åŸºäºADAPTIVEæ¶æ„çš„ODæµé‡é¢„æµ‹GANæ¨¡å‹
    
    ä¸»è¦åˆ›æ–°ç‚¹ï¼š
    1. ä½¿ç”¨TuckERçŸ¥è¯†å›¾è°±åµŒå…¥æå–åŸå¸‚ç¯å¢ƒç‰¹å¾
    2. é‡‡ç”¨GCNæ•æ‰ç©ºé—´ä¾èµ–å…³ç³»
    3. åº”ç”¨æ³¨æ„åŠ›é©±åŠ¨åŒ¹é…ä¼ é€’æ—¶é—´æ¨¡å¼
    4. ä½¿ç”¨ç‰¹å¾å¢å¼ºç”Ÿæˆå™¨è¿›è¡Œå¤šæ¨¡å¼ç”Ÿæˆ
    5. é‡‡ç”¨å¤šå°ºåº¦åˆ¤åˆ«å™¨å®ç°å¯¹æŠ—è®­ç»ƒï¼ˆä¸¥æ ¼æŒ‰ç…§åŸè®ºæ–‡GANæ¶æ„ï¼‰
    """
    def __init__(self, input_dim=6, hidden_dim=64, time_steps=28, 
                 num_entities=100, num_relations=10, entity_dim=64,
                 gcn_layers=2, num_clusters=4, noise_dim=32):
        super(ADAPTIVEODFlowPredictor, self).__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.time_steps = time_steps
        self.num_entities = num_entities
        self.entity_dim = entity_dim
        self.noise_dim = noise_dim
        
        # 1. çŸ¥è¯†å›¾è°±åµŒå…¥æ¨¡å—
        self.knowledge_graph_embedding = TuckERKnowledgeGraphEmbedding(
            num_entities=num_entities,
            num_relations=num_relations,
            entity_dim=entity_dim,
            relation_dim=entity_dim
        )
        
        # 2. å›¾å·ç§¯ç½‘ç»œ
        self.gcn = GraphConvolutionalNetwork(
            input_dim=entity_dim,
            hidden_dim=hidden_dim,
            output_dim=hidden_dim,
            num_layers=gcn_layers
        )
        
        # 3. æ³¨æ„åŠ›é©±åŠ¨åŒ¹é…
        self.attention_matching = AttentionDrivenMatching(
            feature_dim=hidden_dim,
            num_clusters=num_clusters
        )
        
        # 4. ç‰¹å¾å¢å¼ºç”Ÿæˆå™¨ (Generator)
        self.generator = FeatureEnhancedGenerator(
            input_dim=input_dim,
            feature_dim=hidden_dim,
            hidden_dim=hidden_dim,
            time_steps=time_steps,
            noise_dim=noise_dim,
            output_dim=2
        )
        
        # 5. å¤šå°ºåº¦åˆ¤åˆ«å™¨ (Discriminator) - æŒ‰ç…§åŸè®ºæ–‡GANæ¶æ„
        self.discriminator = MultiScaleDiscriminator(
            input_dim=2,
            feature_dim=hidden_dim,
            time_steps=time_steps
        )
        
        # è¾…åŠ©ç½‘ç»œï¼šPOIåˆ†å¸ƒé‡æ„ä»»åŠ¡
        self.poi_reconstruction = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 10),  # å‡è®¾10ä¸ªPOIç±»åˆ«
            nn.Softmax(dim=-1)
        )
        
        # æŸå¤±å‡½æ•°
        self.mse_loss = nn.MSELoss()
        self.mae_loss = nn.L1Loss()
        self.kl_loss = nn.KLDivLoss(reduction='batchmean')
        
        # æ¨¡æ‹Ÿç®€å•çš„åŸå¸‚çŸ¥è¯†å›¾è°±ç»“æ„
        self._init_mock_knowledge_graph()
        
    def pearson_correlation_loss(self, pred, target):
        """è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°æŸå¤±ï¼Œç”¨äºç›´æ¥ä¼˜åŒ–PCC"""
        # å±•å¹³å¼ é‡
        pred_flat = pred.view(-1)
        target_flat = target.view(-1)
        
        # è®¡ç®—å‡å€¼
        pred_mean = torch.mean(pred_flat)
        target_mean = torch.mean(target_flat)
        
        # è®¡ç®—åæ–¹å·®å’Œæ ‡å‡†å·®
        pred_centered = pred_flat - pred_mean
        target_centered = target_flat - target_mean
        
        covariance = torch.mean(pred_centered * target_centered)
        pred_std = torch.sqrt(torch.mean(pred_centered ** 2) + 1e-8)
        target_std = torch.sqrt(torch.mean(target_centered ** 2) + 1e-8)
        
        # è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°
        correlation = covariance / (pred_std * target_std + 1e-8)
        
        # è¿”å›è´Ÿç›¸å…³ç³»æ•°ä½œä¸ºæŸå¤±ï¼ˆæœ€å¤§åŒ–ç›¸å…³æ€§ï¼‰
        return 1.0 - correlation
        
        
    def _init_mock_knowledge_graph(self):
        """åˆå§‹åŒ–æ¨¡æ‹Ÿçš„çŸ¥è¯†å›¾è°±ç»“æ„"""
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„é‚»æ¥çŸ©é˜µï¼ˆè·ç¦»åæ¯”ï¼‰
        self.register_buffer('adjacency_matrix', torch.eye(self.num_entities))
        
        # ä¸ºæ¯ä¸ªå®ä½“åˆ†é…éšæœºIDï¼ˆåœ¨çœŸå®åº”ç”¨ä¸­åº”è¯¥ä»æ•°æ®ä¸­è·å–ï¼‰
        self.register_buffer('entity_ids', torch.arange(self.num_entities))
    
    def compute_gradient_penalty(self, real_data, fake_data):
        """
        è®¡ç®—Wasserstein GANçš„æ¢¯åº¦æƒ©ç½šé¡¹
        Args:
            real_data: çœŸå®æ•°æ® [batch_size, time_steps, 2]
            fake_data: ç”Ÿæˆæ•°æ® [batch_size, time_steps, 2]
        Returns:
            æ¢¯åº¦æƒ©ç½šæŸå¤±
        """
        batch_size = real_data.size(0)
        device = real_data.device
        
        # éšæœºæ’å€¼
        alpha = torch.rand(batch_size, 1, 1, device=device)
        interpolated = alpha * real_data + (1 - alpha) * fake_data
        interpolated.requires_grad_(True)
        
        # è®¡ç®—åˆ¤åˆ«å™¨å¯¹æ’å€¼æ•°æ®çš„è¾“å‡º
        disc_interpolated = self.discriminator(interpolated)
        
        # è®¡ç®—æ¢¯åº¦
        gradients = torch.autograd.grad(
            outputs=disc_interpolated,
            inputs=interpolated,
            grad_outputs=torch.ones_like(disc_interpolated),
            create_graph=True,
            retain_graph=True,
            only_inputs=True
        )[0]
        
        # è®¡ç®—æ¢¯åº¦æƒ©ç½š
        gradients = gradients.reshape(batch_size, -1)
        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()
        
        return gradient_penalty
        
    def forward(self, features, target_od=None, mode='train', train_discriminator=True):
        """
        å‰å‘ä¼ æ’­ - æ”¯æŒGANå¯¹æŠ—è®­ç»ƒ
        Args:
            features: è¾“å…¥ç‰¹å¾ [batch_size, time_steps=28, input_dim] (input_dimåŠ¨æ€è®¡ç®—ï¼Œé€šå¸¸ä¸º6æˆ–10)
            target_od: ç›®æ ‡ODæµé‡ [batch_size, time_steps=28, 2]
            mode: 'train' æˆ– 'eval'
            train_discriminator: æ˜¯å¦è®­ç»ƒåˆ¤åˆ«å™¨ï¼ˆç”¨äºäº¤æ›¿è®­ç»ƒï¼‰
        Returns:
            ç»“æœå­—å…¸
        """
        batch_size = features.size(0)
        
        # 1. è·å–çŸ¥è¯†å›¾è°±åµŒå…¥ï¼ˆæ¨¡æ‹Ÿï¼šæ¯ä¸ªbatchä½¿ç”¨å‰batch_sizeä¸ªå®ä½“ï¼‰
        entity_indices = self.entity_ids[:batch_size]
        kg_embeddings = self.knowledge_graph_embedding(entity_indices)  # [batch_size, entity_dim]
        
        # 2. æ„å»ºæ‰¹æ¬¡çš„é‚»æ¥çŸ©é˜µ
        batch_adj = self.adjacency_matrix[:batch_size, :batch_size]
        
        # 3. GCNå¤„ç†ç©ºé—´å…³ç³»
        spatial_representations = self.gcn(kg_embeddings, batch_adj)  # [batch_size, hidden_dim]
        
        # 4. æ³¨æ„åŠ›é©±åŠ¨åŒ¹é…ï¼ˆä¼ é€’æ—¶é—´æ¨¡å¼ï¼‰
        matched_representations = self.attention_matching(spatial_representations)  # [batch_size, hidden_dim]
        
        # 5. ç”Ÿæˆå™¨ç”ŸæˆODæµé‡
        noise = torch.randn(batch_size, self.noise_dim, device=features.device)
        generated_od = self.generator(features, matched_representations, noise)  # [batch_size, time_steps, 2]
        
        # æ”¹è¿›è¾“å‡ºèŒƒå›´å¤„ç† - ä½¿ç”¨æ›´å¹³æ»‘çš„æ˜ å°„
        # ä½¿ç”¨sigmoidæ¿€æ´»ç¡®ä¿è¾“å‡ºåœ¨[0,1]èŒƒå›´å†…ï¼Œä¿æŒæ¢¯åº¦æµç•…
        generated_od = torch.sigmoid(generated_od)  # ç›´æ¥æ˜ å°„åˆ°[0,1]ï¼Œä¿æŒç›¸å…³æ€§
        
        if mode == 'train' and target_od is not None:
            # å½’ä¸€åŒ–ç›®æ ‡æ•°æ®åˆ°[0,1]èŒƒå›´ï¼ˆç”¨äºä¸ç”Ÿæˆå™¨è¾“å‡ºåŒ¹é…ï¼‰
            target_od_normalized = target_od  # å‡è®¾å·²ç»å½’ä¸€åŒ–
            
            if train_discriminator:
                # ====== è®­ç»ƒåˆ¤åˆ«å™¨ ======
                # çœŸå®æ•°æ®çš„åˆ¤åˆ«å™¨åˆ†æ•°
                real_score = self.discriminator(target_od_normalized)
                
                # ç”Ÿæˆæ•°æ®çš„åˆ¤åˆ«å™¨åˆ†æ•°ï¼ˆdetaché˜²æ­¢æ¢¯åº¦ä¼ æ’­åˆ°ç”Ÿæˆå™¨ï¼‰
                fake_score = self.discriminator(generated_od.detach())
                
                # WassersteinæŸå¤±
                d_loss_real = -torch.mean(real_score)
                d_loss_fake = torch.mean(fake_score)
                
                # æ¢¯åº¦æƒ©ç½š
                gradient_penalty = self.compute_gradient_penalty(target_od_normalized, generated_od.detach())
                
                # åˆ¤åˆ«å™¨æ€»æŸå¤±
                d_loss = d_loss_real + d_loss_fake + 10.0 * gradient_penalty
                
                return {
                    'od_flows': generated_od,
                    'd_loss': d_loss,
                    'd_loss_real': d_loss_real,
                    'd_loss_fake': d_loss_fake,
                    'gradient_penalty': gradient_penalty
                }
            else:
                # ====== è®­ç»ƒç”Ÿæˆå™¨ ======
                # ç”Ÿæˆå™¨çš„å¯¹æŠ—æŸå¤±
                fake_score = self.discriminator(generated_od)
                g_loss_adv = -torch.mean(fake_score)
                
                # é‡æ„æŸå¤±ï¼ˆä¸çœŸå®æ•°æ®çš„ç›¸ä¼¼æ€§ï¼‰
                mse_loss = self.mse_loss(generated_od, target_od_normalized)
                mae_loss = self.mae_loss(generated_od, target_od_normalized)
                
                # PCCæŸå¤±ï¼ˆç›´æ¥ä¼˜åŒ–ç›¸å…³æ€§ï¼‰
                pcc_loss = self.pearson_correlation_loss(generated_od, target_od_normalized)
                
                # POIé‡æ„æŸå¤±ï¼ˆè‡ªç›‘ç£ä»»åŠ¡ï¼‰
                poi_pred = self.poi_reconstruction(matched_representations)  # [batch_size, 10]
                # æ¨¡æ‹ŸPOIçœŸå®åˆ†å¸ƒï¼ˆåœ¨çœŸå®åº”ç”¨ä¸­åº”è¯¥ä»æ•°æ®ä¸­è·å–ï¼‰
                poi_target = F.softmax(torch.randn_like(poi_pred), dim=-1)
                poi_loss = self.kl_loss(F.log_softmax(poi_pred, dim=-1), poi_target)
                
                # ç”Ÿæˆå™¨æ€»æŸå¤± - åŠ å…¥PCCæŸå¤±ï¼Œé‡ç‚¹ä¼˜åŒ–ç›¸å…³æ€§
                # é™ä½å¯¹æŠ—æŸå¤±æƒé‡ï¼Œå¢åŠ PCCæŸå¤±æƒé‡
                g_loss = 0.05 * g_loss_adv + 20.0 * mse_loss + 10.0 * mae_loss + 30.0 * pcc_loss + 0.1 * poi_loss
                
                return {
                    'od_flows': generated_od,
                    'g_loss': g_loss,
                    'g_loss_adv': g_loss_adv,
                    'mse_loss': mse_loss,
                    'mae_loss': mae_loss,
                    'pcc_loss': pcc_loss,
                    'poi_loss': poi_loss
                }
        else:
            return {
                'od_flows': generated_od
            }
    
    def generate(self, features, noise=None):
        """ç”ŸæˆODæµé‡é¢„æµ‹ - ä¿æŒä¸åŸä»£ç æ¥å£ä¸€è‡´"""
        with torch.no_grad():
            batch_size = features.size(0)
            
            # è·å–è¡¨ç¤º
            entity_indices = self.entity_ids[:batch_size]
            kg_embeddings = self.knowledge_graph_embedding(entity_indices)
            batch_adj = self.adjacency_matrix[:batch_size, :batch_size]
            spatial_representations = self.gcn(kg_embeddings, batch_adj)
            matched_representations = self.attention_matching(spatial_representations)
            
            # ç”Ÿæˆ
            if noise is None:
                noise = torch.randn(batch_size, self.noise_dim, device=features.device)
            generated_od = self.generator(features, matched_representations, noise)
            # ä½¿ç”¨sigmoidç¡®ä¿è¾“å‡ºåœ¨[0,1]èŒƒå›´å†…
            generated_od = torch.sigmoid(generated_od)
            
            return generated_od

# ========== ç®€åŒ–çš„æ•°æ®é›†ç±»ï¼ˆä¿æŒä¸åŸä»£ç ä¸€è‡´ï¼‰==========
class SimpleODFlowDataset(Dataset):
    """ç®€åŒ–çš„ODæµé‡æ•°æ®é›† - ä¸åŸä»£ç å®Œå…¨ä¿æŒä¸€è‡´"""
    def __init__(self, io_flow_path, graph_path, od_matrix_path, test_ratio=0.2, val_ratio=0.1, seed=42):
        super().__init__()
        
        # åŠ è½½æ•°æ®
        self.io_flow = np.load(io_flow_path)  # (æ—¶é—´æ­¥, ç«™ç‚¹æ•°, 4)
        self.graph = np.load(graph_path)      # (ç«™ç‚¹æ•°, ç«™ç‚¹æ•°)  
        self.od_matrix = np.load(od_matrix_path)  # (æ—¶é—´æ­¥, ç«™ç‚¹æ•°, ç«™ç‚¹æ•°)
        
        # è½¬æ¢ç»´åº¦é¡ºåºï¼šä» (æ—¶é—´æ­¥, ç«™ç‚¹æ•°, 4) åˆ° (ç«™ç‚¹æ•°, æ—¶é—´æ­¥, 4)
        if self.io_flow.shape[0] == 28:  # å¦‚æœç¬¬ä¸€ä¸ªç»´åº¦æ˜¯æ—¶é—´æ­¥
            self.io_flow = np.transpose(self.io_flow, (1, 0, 2))
        
        # è½¬æ¢ç»´åº¦é¡ºåºï¼šä» (æ—¶é—´æ­¥, ç«™ç‚¹æ•°, ç«™ç‚¹æ•°) åˆ° (ç«™ç‚¹æ•°, ç«™ç‚¹æ•°, æ—¶é—´æ­¥)  
        if self.od_matrix.shape[0] == 28:  # å¦‚æœç¬¬ä¸€ä¸ªç»´åº¦æ˜¯æ—¶é—´æ­¥
            self.od_matrix = np.transpose(self.od_matrix, (1, 2, 0))
        
        self.num_nodes = self.io_flow.shape[0]
        self.time_steps = self.io_flow.shape[1]
        
        # æ•°æ®ä¸€è‡´æ€§éªŒè¯ - æŒ‰ç…§æŒ‡å—è¦æ±‚
        print(f"æ•°æ®ç»´åº¦: IOæµé‡{self.io_flow.shape}, å›¾{self.graph.shape}, ODçŸ©é˜µ{self.od_matrix.shape}")
        
        # éªŒè¯æ•°æ®ç»´åº¦ä¸€è‡´æ€§
        assert self.io_flow.shape[0] == 28 or self.io_flow.shape[1] == 28, f"IOæµé‡æ•°æ®æ—¶é—´æ­¥æ•°ä¸æ­£ç¡®: {self.io_flow.shape}"
        assert self.io_flow.shape[2] == 2 or self.io_flow.shape[2] == 4, f"IOæµé‡æ•°æ®ç‰¹å¾æ•°ä¸æ­£ç¡®: {self.io_flow.shape} (åº”è¯¥æ˜¯2æˆ–4ä¸ªç‰¹å¾)"
        assert self.graph.shape[0] == self.graph.shape[1], f"å›¾æ•°æ®ä¸æ˜¯æ–¹é˜µ: {self.graph.shape}"
        assert self.graph.shape[0] == self.num_nodes, f"å›¾æ•°æ®ç»´åº¦ä¸èŠ‚ç‚¹æ•°ä¸åŒ¹é…: {self.graph.shape[0]} vs {self.num_nodes}"
        assert self.od_matrix.shape[0] == self.num_nodes and self.od_matrix.shape[1] == self.num_nodes, f"ODçŸ©é˜µç»´åº¦ä¸èŠ‚ç‚¹æ•°ä¸åŒ¹é…: {self.od_matrix.shape} vs ({self.num_nodes}, {self.num_nodes})"
        assert self.od_matrix.shape[2] == self.time_steps, f"ODçŸ©é˜µæ—¶é—´æ­¥æ•°ä¸åŒ¹é…: {self.od_matrix.shape[2]} vs {self.time_steps}"
        
        print(f"âœ… æ•°æ®ä¸€è‡´æ€§éªŒè¯é€šè¿‡: {self.num_nodes}ä¸ªèŠ‚ç‚¹, {self.time_steps}ä¸ªæ—¶é—´æ­¥")
        
        # ç«™ç‚¹å¯¹åˆ—è¡¨ - ä¸åŸç‰ˆä¿æŒä¸€è‡´ï¼Œä½¿ç”¨æ‰€æœ‰ç«™ç‚¹å¯¹
        self.od_pairs = []
        for i in range(self.num_nodes):
            for j in range(i + 1, self.num_nodes):
                self.od_pairs.append((i, j))
        
        print(f"ç”Ÿæˆ{len(self.od_pairs)}ä¸ªç«™ç‚¹å¯¹ç”¨äºè®­ç»ƒ")
        
        # åŠ è½½ç«™ç‚¹äººå£å¯†åº¦æ•°æ® - ä¼˜å…ˆä½¿ç”¨52èŠ‚ç‚¹ç‰ˆæœ¬
        population_files = [
            "/private/od/data_NYTaxi/grid_population_density_52nodes.json",  # ä¼˜å…ˆä½¿ç”¨52èŠ‚ç‚¹ç‰ˆæœ¬
            "/private/od/data_NYTaxi/grid_population_density.json",  # åŸå§‹å¤‡ç”¨
            "/private/od/data/station_p.json"  # æ—§ç‰ˆæœ¬å¤‡ç”¨
        ]
        
        self.station_data = []
        for pop_file in population_files:
            if os.path.exists(pop_file):
                try:
                    with open(pop_file, 'r', encoding='utf-8') as f:
                        self.station_data = json.load(f)
                    print(f"âœ… åŠ è½½äººå£å¯†åº¦æ•°æ®: {pop_file}, å…±{len(self.station_data)}ä¸ªåŒºåŸŸ")
                    break
                except Exception as e:
                    print(f"âš ï¸ åŠ è½½äººå£å¯†åº¦æ•°æ®å¤±è´¥ {pop_file}: {str(e)}")
                    continue
        
        if not self.station_data:
            print(f"âš ï¸ æ‰€æœ‰äººå£å¯†åº¦æ•°æ®æ–‡ä»¶éƒ½æ— æ³•åŠ è½½ï¼Œä½¿ç”¨ç©ºæ•°æ®")
            self.station_data = []
        else:
            # éªŒè¯äººå£å¯†åº¦æ•°æ®ä¸èŠ‚ç‚¹æ•°é‡çš„ä¸€è‡´æ€§
            if len(self.station_data) != self.num_nodes:
                print(f"âš ï¸ äººå£å¯†åº¦æ•°æ®æ•°é‡({len(self.station_data)})ä¸èŠ‚ç‚¹æ•°é‡({self.num_nodes})ä¸åŒ¹é…")
                if len(self.station_data) > self.num_nodes:
                    print(f"   æˆªå–å‰{self.num_nodes}ä¸ªäººå£å¯†åº¦æ•°æ®")
                    self.station_data = self.station_data[:self.num_nodes]
                else:
                    print(f"   äººå£å¯†åº¦æ•°æ®ä¸è¶³ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼å¡«å……")
            else:
                print(f"âœ… äººå£å¯†åº¦æ•°æ®æ•°é‡ä¸èŠ‚ç‚¹æ•°é‡åŒ¹é…: {len(self.station_data)}ä¸ª")
        
        # æ•°æ®é›†åˆ’åˆ† - ä½¿ç”¨8:1:1çš„ä¸¥æ ¼åˆ’åˆ†
        all_indices = list(range(len(self.od_pairs)))
        random.seed(seed)
        random.shuffle(all_indices)
        
        total_samples = len(all_indices)
        
        # è®¡ç®—åˆ’åˆ†ç‚¹ - ç¡®ä¿8:1:1çš„æ¯”ä¾‹
        train_size = int(total_samples * 0.8)  # 80%è®­ç»ƒé›†
        val_size = int(total_samples * 0.1)    # 10%éªŒè¯é›†  
        test_size = total_samples - train_size - val_size  # å‰©ä½™ä¸ºæµ‹è¯•é›†
        
        # é‡æ–°åˆ’åˆ†ï¼Œç¡®ä¿æ²¡æœ‰é‡å 
        self.train_indices = all_indices[:train_size]
        self.val_indices = all_indices[train_size:train_size + val_size]
        self.test_indices = all_indices[train_size + val_size:]
        
        print(f"æ•°æ®é›†åˆ’åˆ†å®Œæˆ:")
        print(f"  è®­ç»ƒé›†: {len(self.train_indices)} æ ·æœ¬ ({len(self.train_indices)/total_samples:.1%})")
        print(f"  éªŒè¯é›†: {len(self.val_indices)} æ ·æœ¬ ({len(self.val_indices)/total_samples:.1%})")
        print(f"  æµ‹è¯•é›†: {len(self.test_indices)} æ ·æœ¬ ({len(self.test_indices)/total_samples:.1%})")
        
        self.set_mode('train')
    
    def set_mode(self, mode):
        """è®¾ç½®æ•°æ®é›†æ¨¡å¼"""
        if mode == 'train':
            self.current_indices = self.train_indices
        elif mode == 'val':
            self.current_indices = self.val_indices
        elif mode == 'test':
            self.current_indices = self.test_indices
        else:
            raise ValueError(f"Unknown mode: {mode}")
    
    def __len__(self):
        return len(self.current_indices)
    
    def __getitem__(self, idx):
        # è·å–ç«™ç‚¹å¯¹
        site_pair_idx = self.current_indices[idx]
        site_i, site_j = self.od_pairs[site_pair_idx]
        
        # è·å–ODæµé‡
        od_i_to_j = self.od_matrix[site_i, site_j, :]  # (æ—¶é—´æ­¥,)
        od_j_to_i = self.od_matrix[site_j, site_i, :]  # (æ—¶é—´æ­¥,)
        od_flows = np.stack([od_i_to_j, od_j_to_i], axis=1)  # (æ—¶é—´æ­¥, 2)
        
        # è·å–IOæµé‡
        io_flow_i = self.io_flow[site_i, :, :]  # (æ—¶é—´æ­¥, 2)
        io_flow_j = self.io_flow[site_j, :, :]  # (æ—¶é—´æ­¥, 2)
        
        # ç®€å•å½’ä¸€åŒ–
        def normalize_data(data):
            data = np.nan_to_num(data, nan=0.0)
            data_min = np.min(data)
            data_max = np.max(data)
            if data_max > data_min:
                return (data - data_min) / (data_max - data_min)
            else:
                return data * 0
        
        io_flow_i = normalize_data(io_flow_i)
        io_flow_j = normalize_data(io_flow_j)
        od_flows = normalize_data(od_flows)
        
        # è·å–è·ç¦»ç‰¹å¾
        distance = self.graph[site_i, site_j]
        distance_normalized = distance / np.max(self.graph) if np.max(self.graph) > 0 else 0
        
        # è·å–ç«™ç‚¹äººå£å¯†åº¦å¹¶å½’ä¸€åŒ– - ä¸åŸç‰ˆä¿æŒä¸€è‡´
        if hasattr(self, 'station_data') and len(self.station_data) > 0:
            # ç¡®ä¿ç«™ç‚¹ç´¢å¼•ä¸è¶…è¿‡å¯ç”¨çš„ç«™ç‚¹æ•°æ®
            if site_i < len(self.station_data) and site_j < len(self.station_data):
                pop_density_i = self.station_data[site_i].get('grid_population_density', 0.0)
                pop_density_j = self.station_data[site_j].get('grid_population_density', 0.0)
            else:
                # å¦‚æœç«™ç‚¹ç´¢å¼•è¶…å‡ºèŒƒå›´ï¼Œä½¿ç”¨é»˜è®¤å€¼
                pop_density_i = 0.0
                pop_density_j = 0.0
                
            # è®¡ç®—äººå£å¯†åº¦ç‰¹å¾ï¼ˆä¸¤ç«™ç‚¹äººå£å¯†åº¦çš„å¹³å‡å€¼ï¼‰
            pop_density = (pop_density_i + pop_density_j) / 2
            
            # äººå£å¯†åº¦å½’ä¸€åŒ– - ä½¿ç”¨æ‰€æœ‰ç«™ç‚¹çš„æœ€å¤§äººå£å¯†åº¦å½’ä¸€åŒ–
            max_pop_density = max([station.get('grid_population_density', 1.0) for station in self.station_data])
            if max_pop_density == 0:
                max_pop_density = 1.0
            
            pop_density_normalized = pop_density / max_pop_density
        else:
            # å¦‚æœæ²¡æœ‰äººå£å¯†åº¦æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤å€¼
            pop_density_normalized = 0.0
        
        # æ„å»ºç‰¹å¾ï¼šIOæµé‡ + è·ç¦»ç‰¹å¾ + äººå£å¯†åº¦ç‰¹å¾
        distance_feature = np.ones((self.time_steps, 1)) * distance_normalized
        pop_density_feature = np.ones((self.time_steps, 1)) * pop_density_normalized
        features = np.concatenate([io_flow_i, io_flow_j, distance_feature, pop_density_feature], axis=1)  
        # ç‰¹å¾ç»´åº¦: (æ—¶é—´æ­¥, io_flow_features*2 + 2) = (æ—¶é—´æ­¥, 2*2+2=6) æˆ– (æ—¶é—´æ­¥, 4*2+2=10)
        
        return torch.FloatTensor(features), torch.FloatTensor(od_flows)

# ========== è¯„ä¼°æŒ‡æ ‡è®¡ç®—å‡½æ•° ==========
def calculate_metrics(model, dataloader, device, desc="Evaluating"):
    """è®¡ç®—è¯¦ç»†çš„è¯„ä¼°æŒ‡æ ‡ï¼šMSEã€RMSEã€MAEã€PCC"""
    model.eval()
    all_predictions = []
    all_targets = []
    total_losses = []
    
    with torch.no_grad():
        progress = tqdm(dataloader, desc=desc, leave=False)
        for features, od_flows in progress:
            features = features.to(device)
            od_flows = od_flows.to(device)
            
            # ç”Ÿæˆé¢„æµ‹
            predicted = model.generate(features)
            
            # è®¡ç®—æŸå¤±
            loss = F.mse_loss(predicted, od_flows)
            total_losses.append(loss.item())
            
            # æ”¶é›†é¢„æµ‹ç»“æœ
            all_predictions.append(predicted.cpu().numpy())
            all_targets.append(od_flows.cpu().numpy())
            
            progress.set_postfix({'MSE': f'{loss.item():.6f}'})
    
    # åˆå¹¶æ‰€æœ‰é¢„æµ‹ç»“æœ
    all_predictions = np.concatenate(all_predictions, axis=0)
    all_targets = np.concatenate(all_targets, axis=0)
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    mse = np.mean((all_predictions - all_targets) ** 2)
    rmse = np.sqrt(mse)
    mae = np.mean(np.abs(all_predictions - all_targets))
    
    # è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°(PCC)
    pred_flat = all_predictions.flatten()
    target_flat = all_targets.flatten()
    valid_mask = ~(np.isnan(pred_flat) | np.isnan(target_flat))
    if np.sum(valid_mask) > 0:
        pcc = np.corrcoef(pred_flat[valid_mask], target_flat[valid_mask])[0, 1]
        if np.isnan(pcc):
            pcc = 0.0
    else:
        pcc = 0.0
    
    avg_loss = np.mean(total_losses)
    
    return {
        'loss': float(avg_loss),
        'mse': float(mse), 
        'rmse': float(rmse),
        'mae': float(mae),
        'pcc': float(pcc)
    }

# ========== ADAPTIVE GANè®­ç»ƒå‡½æ•° ==========
def train_adaptive_model(args):
    """è®­ç»ƒADAPTIVE ODæµé‡é¢„æµ‹GANæ¨¡å‹"""
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = SimpleODFlowDataset(
        io_flow_path=args.io_flow_path,
        graph_path=args.graph_path,
        od_matrix_path=args.od_matrix_path,
        test_ratio=args.test_ratio,
        val_ratio=args.val_ratio,
        seed=args.seed
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        dataset, 
        batch_size=args.batch_size, 
        shuffle=True, 
        num_workers=0
    )
    
    dataset.set_mode('val')
    val_loader = DataLoader(
        dataset, 
        batch_size=args.batch_size, 
        shuffle=False, 
        num_workers=0
    )
    
    dataset.set_mode('test')
    test_loader = DataLoader(
        dataset, 
        batch_size=args.batch_size, 
        shuffle=False, 
        num_workers=0
    )
    
    dataset.set_mode('train')
    
    # åŠ¨æ€è®¡ç®—è¾“å…¥ç‰¹å¾ç»´åº¦
    # ç‰¹å¾æ„æˆ: io_flow_i + io_flow_j + distance + population_density
    # = io_flow_features*2 + 2
    io_flow_features = dataset.io_flow.shape[2]  # 2 æˆ– 4
    input_dim = io_flow_features * 2 + 2  # 6 æˆ– 10
    print(f"âœ… åŠ¨æ€è®¡ç®—è¾“å…¥ç‰¹å¾ç»´åº¦: {input_dim} (IOæµé‡ç‰¹å¾: {io_flow_features})")
    
    # åˆ›å»ºADAPTIVE GANæ¨¡å‹
    model = ADAPTIVEODFlowPredictor(
        input_dim=input_dim,
        hidden_dim=args.hidden_dim,
        time_steps=28,
        num_entities=min(args.num_entities, len(dataset.od_pairs)),  # ä½¿ç”¨å®é™…çš„ç«™ç‚¹å¯¹æ•°é‡
        num_relations=args.num_relations,
        entity_dim=args.entity_dim,
        gcn_layers=args.gcn_layers,
        num_clusters=args.num_clusters,
        noise_dim=args.noise_dim
    ).to(device)
    
    total_params = sum(p.numel() for p in model.parameters())
    generator_params = sum(p.numel() for p in model.generator.parameters())
    discriminator_params = sum(p.numel() for p in model.discriminator.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"ADAPTIVE GANæ¨¡å‹åˆ›å»ºæˆåŠŸï¼")
    print(f"  æ€»å‚æ•°æ•°é‡: {total_params:,}")
    print(f"  ç”Ÿæˆå™¨å‚æ•°: {generator_params:,}")
    print(f"  åˆ¤åˆ«å™¨å‚æ•°: {discriminator_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"  éšè—ç»´åº¦: {args.hidden_dim}")
    print(f"  å®ä½“æ•°é‡: {min(args.num_entities, len(dataset.od_pairs))}")
    print(f"  GCNå±‚æ•°: {args.gcn_layers}")
    print(f"  ç°‡æ•°é‡: {args.num_clusters}")
    print(f"  å™ªå£°ç»´åº¦: {args.noise_dim}")
    
    # åˆ†åˆ«åˆ›å»ºç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨çš„ä¼˜åŒ–å™¨ - GANè®­ç»ƒçš„æ ‡å‡†åšæ³•
    generator_params = list(model.knowledge_graph_embedding.parameters()) + \
                      list(model.gcn.parameters()) + \
                      list(model.attention_matching.parameters()) + \
                      list(model.generator.parameters()) + \
                      list(model.poi_reconstruction.parameters())
    
    discriminator_params = list(model.discriminator.parameters())
    
    optimizer_g = torch.optim.Adam(
        generator_params, 
        lr=args.lr, 
        betas=(0.5, 0.999),
        weight_decay=args.weight_decay
    )
    
    optimizer_d = torch.optim.Adam(
        discriminator_params, 
        lr=args.lr, 
        betas=(0.5, 0.999),
        weight_decay=args.weight_decay
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler_g = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer_g, mode='min', factor=0.5, patience=args.patience, verbose=True, min_lr=1e-6
    )
    scheduler_d = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer_d, mode='min', factor=0.5, patience=args.patience, verbose=True, min_lr=1e-6
    )
    
    # è®­ç»ƒå¾ªç¯å˜é‡
    best_val_loss = float('inf')
    best_model_path = os.path.join(args.output_dir, 'best_adaptive_od_model.pth')
    epochs_without_improvement = 0
    train_history = []
    
    print(f"\nå¼€å§‹è®­ç»ƒADAPTIVE GAN ODæµé‡é¢„æµ‹æ¨¡å‹...")
    print(f"æ¨¡å‹å°†ä¿å­˜åˆ°: {best_model_path}")
    print(f"æ—©åœç­–ç•¥: éªŒè¯æŸå¤±{args.early_stop_patience}è½®æ— æ”¹å–„æ—¶åœæ­¢è®­ç»ƒ")
    print(f"GANè®­ç»ƒ: åˆ¤åˆ«å™¨æ›´æ–°{args.n_critic}æ¬¡ï¼Œç”Ÿæˆå™¨æ›´æ–°1æ¬¡")
    print("="*80)
    
    for epoch in range(args.epochs):
        # è®­ç»ƒé˜¶æ®µ - GANå¯¹æŠ—è®­ç»ƒ
        model.train()
        d_losses = []
        g_losses = []
        mse_losses = []
        mae_losses = []
        pcc_losses = []
        poi_losses = []
        
        train_progress = tqdm(train_loader, desc=f"Epoch {epoch+1:3d}/{args.epochs} [GANè®­ç»ƒ]")
        for batch_idx, (features, od_flows) in enumerate(train_progress):
            features = features.to(device)
            od_flows = od_flows.to(device)
            
            # ========== è®­ç»ƒåˆ¤åˆ«å™¨ ==========
            for _ in range(args.n_critic):
                optimizer_d.zero_grad()
                
                # åˆ¤åˆ«å™¨å‰å‘ä¼ æ’­
                d_outputs = model(features, od_flows, mode='train', train_discriminator=True)
                d_loss = d_outputs['d_loss']
                
                # æ£€æŸ¥NaN/Inf
                if torch.isnan(d_loss) or torch.isinf(d_loss):
                    print(f"âš ï¸ æ£€æµ‹åˆ°åˆ¤åˆ«å™¨NaN/InfæŸå¤±: {d_loss.item()}")
                    print(f"   d_loss_real: {d_outputs.get('d_loss_real', 'N/A')}")
                    print(f"   d_loss_fake: {d_outputs.get('d_loss_fake', 'N/A')}")
                    print(f"   gradient_penalty: {d_outputs.get('gradient_penalty', 'N/A')}")
                    continue
                
                # åˆ¤åˆ«å™¨åå‘ä¼ æ’­
                d_loss.backward()
                
                # æ£€æŸ¥æ¢¯åº¦
                grad_norm_d = torch.nn.utils.clip_grad_norm_(discriminator_params, max_norm=0.5)
                if torch.isnan(grad_norm_d) or torch.isinf(grad_norm_d):
                    print(f"âš ï¸ æ£€æµ‹åˆ°åˆ¤åˆ«å™¨æ¢¯åº¦å¼‚å¸¸: {grad_norm_d}")
                    continue
                    
                optimizer_d.step()
                
                d_losses.append(d_loss.item())
            
            # ========== è®­ç»ƒç”Ÿæˆå™¨ ==========
            optimizer_g.zero_grad()
            
            # ç”Ÿæˆå™¨å‰å‘ä¼ æ’­
            g_outputs = model(features, od_flows, mode='train', train_discriminator=False)
            g_loss = g_outputs['g_loss']
            mse_loss = g_outputs['mse_loss']
            mae_loss = g_outputs['mae_loss']
            pcc_loss = g_outputs.get('pcc_loss', torch.tensor(0.0))
            poi_loss = g_outputs.get('poi_loss', torch.tensor(0.0))
            
            # æ£€æŸ¥NaN/Inf
            if torch.isnan(g_loss) or torch.isinf(g_loss) or \
               torch.isnan(mse_loss) or torch.isinf(mse_loss) or \
               torch.isnan(mae_loss) or torch.isinf(mae_loss):
                print(f"âš ï¸ æ£€æµ‹åˆ°ç”Ÿæˆå™¨NaN/InfæŸå¤±:")
                print(f"   g_loss: {g_loss.item()}")
                print(f"   g_loss_adv: {g_outputs.get('g_loss_adv', 'N/A')}")
                print(f"   mse_loss: {mse_loss.item()}")
                print(f"   mae_loss: {mae_loss.item()}")
                print(f"   poi_loss: {poi_loss.item()}")
                continue
            
            # ç”Ÿæˆå™¨åå‘ä¼ æ’­
            g_loss.backward()
            
            # æ£€æŸ¥æ¢¯åº¦
            grad_norm_g = torch.nn.utils.clip_grad_norm_(generator_params, max_norm=0.5)
            if torch.isnan(grad_norm_g) or torch.isinf(grad_norm_g):
                print(f"âš ï¸ æ£€æµ‹åˆ°ç”Ÿæˆå™¨æ¢¯åº¦å¼‚å¸¸: {grad_norm_g}")
                continue
                
            optimizer_g.step()
            
            # è®°å½•æŸå¤±
            g_losses.append(g_loss.item())
            mse_losses.append(mse_loss.item())
            mae_losses.append(mae_loss.item())
            pcc_losses.append(pcc_loss.item())
            poi_losses.append(poi_loss.item())
            
            # æ›´æ–°è¿›åº¦æ¡ - å®‰å…¨æ˜¾ç¤ºæŸå¤±å€¼
            if len(d_losses) > 0 and len(g_losses) > 0:
                train_progress.set_postfix({
                    'D_Loss': f'{d_losses[-1]:.4f}',
                    'G_Loss': f'{g_losses[-1]:.4f}',
                    'MSE': f'{mse_losses[-1]:.4f}' if len(mse_losses) > 0 else 'N/A',
                    'MAE': f'{mae_losses[-1]:.4f}' if len(mae_losses) > 0 else 'N/A'
                })
            else:
                train_progress.set_postfix({
                    'D_Loss': 'N/A',
                    'G_Loss': 'N/A',
                    'MSE': 'N/A',
                    'MAE': 'N/A'
                })
        
        # è®¡ç®—è®­ç»ƒæŒ‡æ ‡ - å®‰å…¨å¤„ç†ç©ºåˆ—è¡¨
        avg_d_loss = np.mean(d_losses) if len(d_losses) > 0 else float('nan')
        avg_g_loss = np.mean(g_losses) if len(g_losses) > 0 else float('nan')
        avg_mse_loss = np.mean(mse_losses) if len(mse_losses) > 0 else float('nan')
        avg_mae_loss = np.mean(mae_losses) if len(mae_losses) > 0 else float('nan')
        avg_pcc_loss = np.mean(pcc_losses) if len(pcc_losses) > 0 else float('nan')
        avg_poi_loss = np.mean(poi_losses) if len(poi_losses) > 0 else float('nan')
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰batchéƒ½å¤±è´¥äº†
        if len(d_losses) == 0 or len(g_losses) == 0:
            print(f"âš ï¸ è­¦å‘Š: Epoch {epoch+1} ä¸­æ‰€æœ‰batchéƒ½å‡ºç°äº†NaNï¼Œè·³è¿‡æœ¬è½®")
            continue
        
        # éªŒè¯é˜¶æ®µ - è®¡ç®—è¯¦ç»†æŒ‡æ ‡
        print(f"  ğŸ” è®¡ç®—éªŒè¯é›†æŒ‡æ ‡...")
        val_metrics = calculate_metrics(model, val_loader, device, desc="éªŒè¯é›†è¯„ä¼°")
        
        # å­¦ä¹ ç‡è°ƒæ•´
        scheduler_g.step(val_metrics['loss'])
        scheduler_d.step(val_metrics['loss'])
        current_lr_g = optimizer_g.param_groups[0]['lr']
        current_lr_d = optimizer_d.param_groups[0]['lr']
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
        is_best = val_metrics['loss'] < best_val_loss
        test_metrics = None
        
        if is_best:
            # åªåœ¨éªŒè¯é›†æ€§èƒ½æå‡æ—¶è¯„ä¼°æµ‹è¯•é›†
            print(f"  ğŸ¯ æ–°æœ€ä½³éªŒè¯æŸå¤±! è¯„ä¼°æµ‹è¯•é›†...")
            test_metrics = calculate_metrics(model, test_loader, device, desc="æµ‹è¯•é›†è¯„ä¼°")
            best_val_loss = val_metrics['loss']
            epochs_without_improvement = 0
        else:
            epochs_without_improvement += 1
            # ä½¿ç”¨ä¸Šä¸€æ¬¡æœ€ä½³çš„æµ‹è¯•æŒ‡æ ‡
            if os.path.exists(best_model_path):
                try:
                    checkpoint = torch.load(best_model_path, map_location=device)
                    test_metrics = checkpoint.get('test_metrics', {})
                except:
                    test_metrics = {}
        
        # æ‰“å°è¯¦ç»†ç»“æœ
        print(f"\nğŸ“Š Epoch {epoch+1:3d}/{args.epochs} GANè®­ç»ƒå®Œæˆ:")
        print(f"{'='*80}")
        print(f"ğŸ”¹ è®­ç»ƒé›†:")
        print(f"   åˆ¤åˆ«å™¨æŸå¤±: {avg_d_loss:.6f} | ç”Ÿæˆå™¨æŸå¤±: {avg_g_loss:.6f}")
        print(f"   MSE: {avg_mse_loss:.6f} | MAE: {avg_mae_loss:.6f} | PCCæŸå¤±: {avg_pcc_loss:.6f} | POI: {avg_poi_loss:.6f}")
        
        print(f"ğŸ”¹ éªŒè¯é›†:")
        print(f"   æ€»æŸå¤±: {val_metrics['loss']:.6f} | MSE: {val_metrics['mse']:.6f}")
        print(f"   RMSE: {val_metrics['rmse']:.6f} | MAE: {val_metrics['mae']:.6f} | PCC: {val_metrics['pcc']:.6f}")
        
        if test_metrics:
            print(f"ğŸ”¹ æµ‹è¯•é›†:")  
            print(f"   æ€»æŸå¤±: {test_metrics.get('loss', 0):.6f} | MSE: {test_metrics.get('mse', 0):.6f}")
            print(f"   RMSE: {test_metrics.get('rmse', 0):.6f} | MAE: {test_metrics.get('mae', 0):.6f} | PCC: {test_metrics.get('pcc', 0):.6f}")
        else:
            print(f"ğŸ”¹ æµ‹è¯•é›†: æœªè¯„ä¼° (ä»…åœ¨éªŒè¯é›†æ”¹å–„æ—¶è¯„ä¼°)")
        
        print(f"ğŸ”¹ å­¦ä¹ ç‡: G={current_lr_g:.2e}, D={current_lr_d:.2e}")
        
        # ä¿å­˜è®­ç»ƒå†å² - è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹
        epoch_history = {
            'epoch': int(epoch + 1),
            'train_d_loss': float(avg_d_loss),
            'train_g_loss': float(avg_g_loss),
            'train_mse': float(avg_mse_loss),
            'train_mae': float(avg_mae_loss),
            'train_pcc_loss': float(avg_pcc_loss),
            'train_poi': float(avg_poi_loss),
            'val_loss': float(val_metrics['loss']),
            'val_mse': float(val_metrics['mse']),
            'val_rmse': float(val_metrics['rmse']),
            'val_mae': float(val_metrics['mae']),
            'val_pcc': float(val_metrics['pcc']),
            'lr_g': float(optimizer_g.param_groups[0]['lr']),
            'lr_d': float(optimizer_d.param_groups[0]['lr']),
            'is_best': bool(is_best)
        }
        
        # æ·»åŠ æµ‹è¯•é›†æŒ‡æ ‡ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if test_metrics:
            epoch_history.update({
                'test_loss': float(test_metrics.get('loss', 0)),
                'test_mse': float(test_metrics.get('mse', 0)),
                'test_rmse': float(test_metrics.get('rmse', 0)),
                'test_mae': float(test_metrics.get('mae', 0)),
                'test_pcc': float(test_metrics.get('pcc', 0))
            })
        
        train_history.append(epoch_history)
        
        # è¾¹è®­ç»ƒè¾¹ä¿å­˜è®­ç»ƒæ—¥å¿— - ä½¿ç”¨æ–‡æœ¬æ ¼å¼
        log_file = os.path.join(args.output_dir, "training_log.txt")
        try:
            # å¦‚æœæ˜¯ç¬¬ä¸€è½®ï¼Œåˆ›å»ºæ–°æ–‡ä»¶ï¼›å¦åˆ™è¿½åŠ 
            mode = 'w' if epoch == 0 else 'a'
            with open(log_file, mode, encoding='utf-8') as f:
                if epoch == 0:
                    f.write("ADAPTIVE ODæµé‡é¢„æµ‹æ¨¡å‹è®­ç»ƒæ—¥å¿—\n")
                    f.write("=" * 50 + "\n")
                
                f.write(f"Epoch {epoch+1}/{args.epochs}\n")
                f.write(f"   Training - D_Loss: {avg_d_loss:.6f}, G_Loss: {avg_g_loss:.6f}, MSE: {avg_mse_loss:.6f}, MAE: {avg_mae_loss:.6f}, PCC_Loss: {avg_pcc_loss:.6f}, POI: {avg_poi_loss:.6f}\n")
                f.write(f"   Validation - Loss: {val_metrics['loss']:.6f}, RMSE: {val_metrics['rmse']:.6f}, MAE: {val_metrics['mae']:.6f}, PCC: {val_metrics['pcc']:.6f}\n")
                
                if test_metrics:
                    f.write(f"   Test - Loss: {test_metrics.get('loss', 0):.6f}, RMSE: {test_metrics.get('rmse', 0):.6f}, MAE: {test_metrics.get('mae', 0):.6f}, PCC: {test_metrics.get('pcc', 0):.6f}\n")
                
                if is_best:
                    f.write(f"   New best model saved (Val Loss: {best_val_loss:.6f}, Val RMSE: {val_metrics['rmse']:.6f}, Val PCC: {val_metrics['pcc']:.6f})\n")
                else:
                    f.write(f"   No improvement ({epochs_without_improvement}/{args.early_stop_patience} epochs without improvement)\n")
                
                f.write(f"   Learning Rate: G={current_lr_g:.2e}, D={current_lr_d:.2e}\n")
                f.write("\n")
                f.flush()
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜è®­ç»ƒæ—¥å¿—å¤±è´¥: {e}")
        
        # ä»ç„¶ä¿å­˜JSONæ ¼å¼çš„è¯¦ç»†å†å²æ•°æ®ç”¨äºåç»­åˆ†æ
        history_file = os.path.join(args.output_dir, "training_history.json")
        try:
            with open(history_file, "w", encoding='utf-8') as f:
                json.dump(train_history, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜è¯¦ç»†å†å²å¤±è´¥: {e}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            torch.save({
                'model_state_dict': model.state_dict(),
                'optimizer_g_state_dict': optimizer_g.state_dict(),
                'optimizer_d_state_dict': optimizer_d.state_dict(),
                'scheduler_g_state_dict': scheduler_g.state_dict(),
                'scheduler_d_state_dict': scheduler_d.state_dict(),
                'epoch': epoch,
                'val_loss': val_metrics['loss'],
                'val_metrics': val_metrics,
                'test_metrics': test_metrics,
                'train_history': train_history,
                'args': args
            }, best_model_path)
            print(f"ğŸ¯ âœ… ä¿å­˜æœ€ä½³GANæ¨¡å‹ (éªŒè¯æŸå¤±: {best_val_loss:.6f})")
        else:
            print(f"â³ éªŒè¯æŸå¤±æœªæ”¹å–„ ({epochs_without_improvement}/{args.early_stop_patience}è½®)")
        
        # æ—©åœæ£€æŸ¥
        if epochs_without_improvement >= args.early_stop_patience:
            print(f"\nğŸ›‘ æ—©åœè§¦å‘! éªŒè¯æŸå¤±å·²{args.early_stop_patience}è½®æœªæ”¹å–„ï¼Œåœæ­¢è®­ç»ƒ")
            print(f"   æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f} (æ¥è‡ªç¬¬{epoch - epochs_without_improvement + 2}è½®)")
            break
        
        # å­¦ä¹ ç‡è¿‡å°æ£€æŸ¥
        if current_lr_g < 1e-6 and current_lr_d < 1e-6:
            print(f"\nğŸ›‘ å­¦ä¹ ç‡è¿‡å° (G={current_lr_g:.2e}, D={current_lr_d:.2e})ï¼Œåœæ­¢è®­ç»ƒ")
            break
        
        print("="*80)
    
    log_file = os.path.join(args.output_dir, "training_log.txt")
    history_file = os.path.join(args.output_dir, "training_history.json")
    print(f"ğŸ“ è®­ç»ƒæ—¥å¿—å·²å®æ—¶ä¿å­˜åˆ°: {log_file}")
    print(f"ğŸ“ è¯¦ç»†å†å²æ•°æ®å·²ä¿å­˜åˆ°: {history_file}")
    
    # æœ€ç»ˆæµ‹è¯•é˜¶æ®µ - åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆè¯„ä¼°
    print(f"\n{'='*60}")
    print("ğŸ¯ æœ€ç»ˆæµ‹è¯•é˜¶æ®µ - ä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œè¯„ä¼°")
    print(f"{'='*60}")
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    if os.path.exists(best_model_path):
        checkpoint = torch.load(best_model_path, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        best_epoch = checkpoint['epoch'] + 1
        best_val_metrics = checkpoint.get('val_metrics', {})
        best_test_metrics = checkpoint.get('test_metrics', {})
        print(f"âœ… å·²åŠ è½½æœ€ä½³æ¨¡å‹ (æ¥è‡ªç¬¬{best_epoch}è½®)")
        
        # å±•ç¤ºæœ€ä½³æ¨¡å‹çš„æ€§èƒ½
        print(f"\nğŸ† æœ€ä½³æ¨¡å‹æ€§èƒ½ (ç¬¬{best_epoch}è½®):")
        print(f"ğŸ”¸ éªŒè¯é›†: Loss={best_val_metrics.get('loss', 0):.6f}, RMSE={best_val_metrics.get('rmse', 0):.6f}, MAE={best_val_metrics.get('mae', 0):.6f}, PCC={best_val_metrics.get('pcc', 0):.6f}")
        print(f"ğŸ”¸ æµ‹è¯•é›†: Loss={best_test_metrics.get('loss', 0):.6f}, RMSE={best_test_metrics.get('rmse', 0):.6f}, MAE={best_test_metrics.get('mae', 0):.6f}, PCC={best_test_metrics.get('pcc', 0):.6f}")
        
        # ä½¿ç”¨ä¿å­˜çš„æµ‹è¯•æŒ‡æ ‡ä½œä¸ºæœ€ç»ˆç»“æœ
        final_test_metrics = best_test_metrics
    else:
        print("âš ï¸ æœ€ä½³æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨å½“å‰æ¨¡å‹è¿›è¡Œæœ€ç»ˆæµ‹è¯•")
        final_test_metrics = calculate_metrics(model, test_loader, device, desc="æœ€ç»ˆæµ‹è¯•")
        best_epoch = "å½“å‰"
    
    print(f"\n{'='*60}")
    print("ğŸ‰ ADAPTIVE ODæµé‡é¢„æµ‹æ¨¡å‹ - æœ€ç»ˆæµ‹è¯•ç»“æœ")
    print(f"{'='*60}")
    print(f"ğŸ“Š æœ€ç»ˆæµ‹è¯•æŒ‡æ ‡ (åŸºäºç¬¬{best_epoch}è½®æœ€ä½³æ¨¡å‹):")
    print(f"   ğŸ“ˆ å‡æ–¹è¯¯å·® (MSE):     {final_test_metrics.get('mse', 0):.6f}")
    print(f"   ğŸ“ˆ å‡æ–¹æ ¹è¯¯å·® (RMSE):   {final_test_metrics.get('rmse', 0):.6f}")
    print(f"   ğŸ“ˆ å¹³å‡ç»å¯¹è¯¯å·® (MAE):  {final_test_metrics.get('mae', 0):.6f}")
    print(f"   ğŸ“ˆ çš®å°”é€Šç›¸å…³ç³»æ•° (PCC): {final_test_metrics.get('pcc', 0):.6f}")
    print(f"   ğŸ“ˆ æµ‹è¯•æŸå¤±:          {final_test_metrics.get('loss', 0):.6f}")
    print(f"{'='*60}")
    
    # ä¸ºäº†å…¼å®¹æ€§ï¼Œè®¾ç½®è¿™äº›å˜é‡
    mse = final_test_metrics.get('mse', 0)
    rmse = final_test_metrics.get('rmse', 0) 
    mae = final_test_metrics.get('mae', 0)
    pcc = final_test_metrics.get('pcc', 0)
    avg_test_loss = final_test_metrics.get('loss', 0)
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    results_file = os.path.join(args.output_dir, "adaptive_od_results.txt")
    with open(results_file, "w", encoding='utf-8') as f:
        f.write("åŸºäºADAPTIVEçš„ODæµé‡é¢„æµ‹æ¨¡å‹æµ‹è¯•ç»“æœ\n")
        f.write("="*50 + "\n")
        f.write("è®ºæ–‡: Deep Transfer Learning for City-scale Cellular Traffic Generation through Urban Knowledge Graph (KDD 2023)\n")
        f.write("æ¨¡å‹æ¶æ„æ ¸å¿ƒç‰¹ç‚¹:\n")
        f.write("  - åŸå¸‚çŸ¥è¯†å›¾è°± (Urban Knowledge Graph)\n")
        f.write("  - TuckERçŸ¥è¯†å›¾è°±åµŒå…¥ (TuckER Knowledge Graph Embedding)\n")
        f.write("  - å›¾å·ç§¯ç½‘ç»œ (Graph Convolutional Network)\n")
        f.write("  - æ³¨æ„åŠ›é©±åŠ¨åŒ¹é… (Attention-driven Matching)\n")
        f.write("  - ç‰¹å¾å¢å¼ºç”Ÿæˆç½‘ç»œ (Feature-enhanced Generator)\n")
        f.write("  - POIåˆ†å¸ƒé‡æ„ä»»åŠ¡ (POI Distribution Reconstruction)\n")
        f.write("\n")
        f.write(f"æ¨¡å‹å‚æ•°:\n")
        f.write(f"  - æ€»å‚æ•°æ•°é‡: {total_params:,}\n")
        f.write(f"  - å¯è®­ç»ƒå‚æ•°: {trainable_params:,}\n")
        f.write(f"  - éšè—ç»´åº¦: {args.hidden_dim}\n")
        f.write(f"  - å®ä½“æ•°é‡: {min(args.num_entities, len(dataset.od_pairs))}\n")
        f.write(f"  - å…³ç³»æ•°é‡: {args.num_relations}\n")
        f.write(f"  - å®ä½“åµŒå…¥ç»´åº¦: {args.entity_dim}\n")
        f.write(f"  - GCNå±‚æ•°: {args.gcn_layers}\n")
        f.write(f"  - ç°‡æ•°é‡: {args.num_clusters}\n")
        f.write(f"  - è®­ç»ƒè½®æ•°: {args.epochs}\n")
        f.write(f"  - æ‰¹æ¬¡å¤§å°: {args.batch_size}\n")
        f.write(f"  - å­¦ä¹ ç‡: {args.lr}\n")
        f.write("\n")
        f.write("æµ‹è¯•ç»“æœ:\n")
        f.write(f"  å‡æ–¹è¯¯å·® (MSE):     {mse:.6f}\n")
        f.write(f"  å‡æ–¹æ ¹è¯¯å·® (RMSE):   {rmse:.6f}\n")
        f.write(f"  å¹³å‡ç»å¯¹è¯¯å·® (MAE):  {mae:.6f}\n")
        f.write(f"  çš®å°”é€Šç›¸å…³ç³»æ•° (PCC): {pcc:.6f}\n")
        f.write(f"  æµ‹è¯•æŸå¤±:          {avg_test_loss:.6f}\n")
        f.write(f"  æœ€ä½³éªŒè¯æŸå¤±:       {best_val_loss:.6f}\n")
        f.write(f"\n")
        f.write(f"æ•°æ®é›†ä¿¡æ¯:\n")
        f.write(f"  è®­ç»ƒæ ·æœ¬æ•°: {len(dataset.train_indices)}\n")
        f.write(f"  éªŒè¯æ ·æœ¬æ•°: {len(dataset.val_indices)}\n")
        f.write(f"  æµ‹è¯•æ ·æœ¬æ•°: {len(dataset.test_indices)}\n")
        f.write(f"  è¾“å…¥ç‰¹å¾ç»´åº¦: [batch_size, 28, 6]\n")
        f.write(f"  è¾“å‡ºæµé‡ç»´åº¦: [batch_size, 28, 2]\n")
    
    print(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    print(f"ğŸ“ æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: {best_model_path}")
    
    return best_model_path

# ========== ä¸»å‡½æ•° ==========
def main():
    parser = argparse.ArgumentParser(description="åŸºäºADAPTIVEçš„ODæµé‡é¢„æµ‹æ¨¡å‹")
    
    # æ•°æ®å‚æ•°
    parser.add_argument("--io_flow_path", type=str, default="/private/od/data_NYTaxi/io_flow_daily.npy", 
                       help="IOæµé‡æ•°æ®è·¯å¾„")
    parser.add_argument("--graph_path", type=str, default="/private/od/data_NYTaxi/graph.npy", 
                       help="å›¾ç»“æ„æ•°æ®è·¯å¾„")
    parser.add_argument("--od_matrix_path", type=str, default="/private/od/data_NYTaxi/od_matrix_daily.npy", 
                       help="ODçŸ©é˜µæ•°æ®è·¯å¾„")
    
    # ADAPTIVE GANæ¨¡å‹å‚æ•°
    parser.add_argument("--hidden_dim", type=int, default=64, 
                       help="éšè—ç»´åº¦")
    parser.add_argument("--num_entities", type=int, default=100, 
                       help="çŸ¥è¯†å›¾è°±å®ä½“æ•°é‡ (å»ºè®®è®¾ç½®ä¸ºèŠ‚ç‚¹æ•°é‡çš„1-2å€)")
    parser.add_argument("--num_relations", type=int, default=10, 
                       help="çŸ¥è¯†å›¾è°±å…³ç³»æ•°é‡")
    parser.add_argument("--entity_dim", type=int, default=64, 
                       help="å®ä½“åµŒå…¥ç»´åº¦")
    parser.add_argument("--gcn_layers", type=int, default=2, 
                       help="GCNå±‚æ•°")
    parser.add_argument("--num_clusters", type=int, default=4, 
                       help="æ—¶é—´æ¨¡å¼ç°‡æ•°é‡")
    parser.add_argument("--noise_dim", type=int, default=32, 
                       help="GANå™ªå£°ç»´åº¦")
    parser.add_argument("--n_critic", type=int, default=5, 
                       help="æ¯æ¬¡ç”Ÿæˆå™¨æ›´æ–°å‰åˆ¤åˆ«å™¨æ›´æ–°æ¬¡æ•°")
    
    # è®­ç»ƒå‚æ•°  
    parser.add_argument("--batch_size", type=int, default=32, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--epochs", type=int, default=100, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--lr", type=float, default=0.0001, help="å­¦ä¹ ç‡ (GANæ¨èè¾ƒå°å€¼)")
    parser.add_argument("--weight_decay", type=float, default=1e-5, help="æƒé‡è¡°å‡")
    parser.add_argument("--test_ratio", type=float, default=0.1, help="æµ‹è¯•é›†æ¯”ä¾‹ (å›ºå®š8:1:1åˆ’åˆ†)")
    parser.add_argument("--val_ratio", type=float, default=0.1, help="éªŒè¯é›†æ¯”ä¾‹ (å›ºå®š8:1:1åˆ’åˆ†)")
    parser.add_argument("--seed", type=int, default=82, help="éšæœºç§å­")
    
    # æ—©åœå’Œå­¦ä¹ ç‡è°ƒæ•´å‚æ•°
    parser.add_argument("--early_stop_patience", type=int, default=15, help="æ—©åœç­–ç•¥ï¼šéªŒè¯æŸå¤±å¤šå°‘è½®æ— æ”¹å–„æ—¶åœæ­¢è®­ç»ƒ")
    parser.add_argument("--patience", type=int, default=8, help="å­¦ä¹ ç‡è°ƒæ•´ç­–ç•¥ï¼šéªŒè¯æŸå¤±å¤šå°‘è½®æ— æ”¹å–„æ—¶é™ä½å­¦ä¹ ç‡")
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument("--output_dir", type=str, default="/private/od/paper_ny/ADAPTIVE", help="è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = create_dynamic_output_dir(args.output_dir)
    args.output_dir = output_dir
    
    print("="*60)
    print("ğŸš€ ADAPTIVE GAN ODæµé‡é¢„æµ‹æ¨¡å‹")
    print("="*60)
    print("ğŸ“– è®ºæ–‡: Deep Transfer Learning for City-scale Cellular Traffic Generation through Urban Knowledge Graph")
    print("ğŸ“– ä¼šè®®: KDD 2023")
    print("ğŸ“– ä½œè€…: Shiyuan Zhang, et al.")
    print("ğŸ“– æ¶æ„: ç‰¹å¾å¢å¼ºç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (Feature-enhanced GAN)")
    print()
    print("ğŸ”§ æ¨¡å‹åˆ›æ–°ç‚¹:")
    print("  âœ… åŸå¸‚çŸ¥è¯†å›¾è°± - å»ºæ¨¡åŸå¸‚ç¯å¢ƒå®ä½“å…³ç³»")
    print("  âœ… TuckERåµŒå…¥ - å­¦ä¹ çŸ¥è¯†å›¾è°±å®ä½“è¡¨ç¤º")
    print("  âœ… å›¾å·ç§¯ç½‘ç»œ - æ•æ‰ç©ºé—´ä¾èµ–å…³ç³»")
    print("  âœ… æ³¨æ„åŠ›åŒ¹é… - ä¼ é€’æ—¶é—´æ¨¡å¼")
    print("  âœ… ç‰¹å¾å¢å¼ºç”Ÿæˆå™¨ - å¤šæ¨¡å¼æµé‡ç”Ÿæˆ")
    print("  âœ… å¤šå°ºåº¦åˆ¤åˆ«å™¨ - å¯¹æŠ—è®­ç»ƒ")
    print("  âœ… Wasserstein GAN - ç¨³å®šè®­ç»ƒè¿‡ç¨‹")
    print("  âœ… POIé‡æ„ä»»åŠ¡ - è‡ªç›‘ç£å­¦ä¹ ")
    print()
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print("="*60)
    
    # è®­ç»ƒæ¨¡å‹
    try:
        best_model_path = train_adaptive_model(args)
        print("\nğŸ‰ ADAPTIVE GANæ¨¡å‹è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“ æœ€ä½³æ¨¡å‹ä¿å­˜ä½ç½®: {best_model_path}")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)