import os
import sys
import logging
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from datetime import datetime
from tqdm import tqdm
import math
import argparse
import random
import traceback
import time
import json
import warnings
import pytz
warnings.filterwarnings("ignore")

# ========== è®¾ç½®éšæœºç§å­ ==========
def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# ========== æ—¥å¿—æ–‡ä»¶å†™å…¥å·¥å…· ==========
class FileLogger:
    def __init__(self, log_path):
        log_dir = os.path.dirname(log_path)
        if log_dir and not os.path.exists(log_dir):
            os.makedirs(log_dir, exist_ok=True)
        self.log_file = open(log_path, 'w', encoding='utf-8')
    
    def info(self, msg):
        beijing_tz = pytz.timezone('Asia/Shanghai')
        timestamp = datetime.now(beijing_tz).strftime("%Y-%m-%d %H:%M:%S")
        self.log_file.write(f"[{timestamp}] {msg}\n")
        self.log_file.flush()
    
    def close(self):
        if hasattr(self, 'log_file') and self.log_file:
            self.log_file.close()

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
)
logger = logging.getLogger(__name__)
file_logger = None

def create_dynamic_output_dir(base_dir):
    import datetime
    beijing_tz = pytz.timezone('Asia/Shanghai')
    timestamp = datetime.datetime.now(beijing_tz).strftime("%Y%m%d_%H%M%S")
    dynamic_dir = os.path.join(base_dir, f"true_mcgan_run_{timestamp}")
    os.makedirs(dynamic_dir, exist_ok=True)
    return dynamic_dir

# ========== ç¨³å®šçš„MCGANæ ¸å¿ƒç»„ä»¶ ==========

class StableMCGANGenerator(nn.Module):
    """ç¨³å®šçš„MCGANç”Ÿæˆå™¨"""
    def __init__(self, input_dim=6, hidden_dim=64, output_dim=2, num_layers=2, dropout=0.1):
        super(StableMCGANGenerator, self).__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        self.num_layers = num_layers
        
        # æ›´ç¨³å®šçš„ç‰¹å¾ç¼–ç å™¨
        self.feature_encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),  # æ·»åŠ LayerNormæé«˜ç¨³å®šæ€§
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # ç¨³å®šçš„æ—¶åºç”Ÿæˆå™¨
        self.temporal_generator = nn.LSTM(  # ä½¿ç”¨LSTMæ›¿ä»£GRUï¼Œæ›´ç¨³å®š
            input_size=hidden_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=False
        )
        
        # æ›´ç¨³å®šçš„ODç”Ÿæˆå¤´
        self.od_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, output_dim),
            nn.Sigmoid()
        )
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        """æ›´ç¨³å®šçš„æƒé‡åˆå§‹åŒ–"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight, gain=0.8)  # ç¨å¾®å¢åŠ gainä½†ä¿æŒç¨³å®š
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0.01)
            elif isinstance(module, nn.LSTM):
                for name, param in module.named_parameters():
                    if 'weight_ih' in name:
                        nn.init.xavier_uniform_(param.data, gain=0.8)
                    elif 'weight_hh' in name:
                        nn.init.orthogonal_(param.data, gain=0.8)
                    elif 'bias' in name:
                        nn.init.constant_(param.data, 0.01)
    
    def forward(self, features):
        batch_size, seq_len, _ = features.size()
        
        # ç¨³å®šçš„ç‰¹å¾ç¼–ç 
        encoded_features = self.feature_encoder(features)
        
        # ç¨³å®šçš„æ—¶åºç”Ÿæˆ
        temporal_output, _ = self.temporal_generator(encoded_features)
        
        # ODæµé‡ç”Ÿæˆ
        od_flows = self.od_head(temporal_output)
        
        # ç¡®ä¿è¾“å‡ºåœ¨åˆç†èŒƒå›´å†…
        od_flows = torch.clamp(od_flows, min=1e-6, max=1.0 - 1e-6)
        
        return od_flows

class StableMCGANDiscriminator(nn.Module):
    """ç¨³å®šçš„MCGANåˆ¤åˆ«å™¨"""
    def __init__(self, input_dim=2, hidden_dim=64, num_layers=2, dropout=0.1):
        super(StableMCGANDiscriminator, self).__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        
        # ç¨³å®šçš„ç‰¹å¾æå–å™¨
        self.feature_extractor = nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=True
        )
        
        # ç¨³å®šçš„åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.LeakyReLU(0.2),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.LeakyReLU(0.2),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid()
        )
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        """ç¨³å®šçš„æƒé‡åˆå§‹åŒ–"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight, gain=0.8)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0.01)
            elif isinstance(module, nn.LSTM):
                for name, param in module.named_parameters():
                    if 'weight_ih' in name:
                        nn.init.xavier_uniform_(param.data, gain=0.8)
                    elif 'weight_hh' in name:
                        nn.init.orthogonal_(param.data, gain=0.8)
                    elif 'bias' in name:
                        nn.init.constant_(param.data, 0.01)
    
    def forward(self, od_flows):
        batch_size, seq_len, _ = od_flows.size()
        
        # ç¡®ä¿è¾“å…¥åœ¨åˆç†èŒƒå›´
        od_flows = torch.clamp(od_flows, min=1e-6, max=1.0 - 1e-6)
        
        # ç‰¹å¾æå–
        features, _ = self.feature_extractor(od_flows)
        
        # åˆ¤åˆ«åˆ†ç±»
        validity = self.classifier(features)
        
        # ç¡®ä¿è¾“å‡ºåœ¨åˆç†èŒƒå›´
        validity = torch.clamp(validity, min=1e-6, max=1.0 - 1e-6)
        
        return validity

class TrueMCGANODFlowPredictor(nn.Module):
    """çœŸæ­£çš„MCGAN ODæµé‡é¢„æµ‹æ¨¡å‹ - ä¼˜åŒ–ç‰ˆ
    
    ä¿æŒMCGANæ ¸å¿ƒç‰¹æ€§ï¼š
    1. å›å½’æŸå¤±ï¼šL_R(Î¸;Ï†) = E[(D_Ï†(x) - E[D_Ï†(Ä)])Â²] âœ…
    2. Monte Carloä¼°è®¡å™¨ âœ…
    3. åˆ¤åˆ«å™¨å‚ä¸è®­ç»ƒ âœ… 
    4. ç¨³å®šçš„æ•°å€¼è®¡ç®— âœ…
    5. åˆ†é˜¶æ®µæ¸è¿›å¼è®­ç»ƒ âœ…
    """
    def __init__(self, input_dim=6, hidden_dim=64, output_dim=2, num_layers=2, dropout=0.1, 
                 mc_samples=3, lambda_regression=0.8, lambda_adversarial=0.2):
        super(TrueMCGANODFlowPredictor, self).__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        self.num_layers = num_layers
        self.mc_samples = mc_samples
        self.lambda_regression = lambda_regression
        self.lambda_adversarial = lambda_adversarial
        
        # è®­ç»ƒé˜¶æ®µæ§åˆ¶
        self.training_phase = 1  # 1: åŸºç¡€è®­ç»ƒ, 2: åˆ¤åˆ«å™¨é¢„è®­ç»ƒ, 3: å®Œæ•´MCGAN
        self.current_epoch = 0
        self.discriminator_pretrain_epochs = 10  # åˆ¤åˆ«å™¨é¢„è®­ç»ƒè½®æ•°
        self.mcgan_warmup_epochs = 20  # MCGANé¢„çƒ­è½®æ•°
        
        # ç¨³å®šçš„MCGANç½‘ç»œ
        self.generator = StableMCGANGenerator(
            input_dim=input_dim, 
            hidden_dim=hidden_dim, 
            output_dim=output_dim,
            num_layers=num_layers,
            dropout=dropout
        )
        
        self.discriminator = StableMCGANDiscriminator(
            input_dim=output_dim,
            hidden_dim=hidden_dim,
            num_layers=num_layers,
            dropout=dropout
        )
        
        # ç¨³å®šçš„æŸå¤±å‡½æ•°
        self.mse_loss = nn.MSELoss()
        self.bce_loss = nn.BCEWithLogitsLoss()  # æ›´ç¨³å®šçš„BCE
        
        # PCCä¼˜åŒ–æƒé‡ - é‡ç‚¹ä¼˜åŒ–PCCæŒ‡æ ‡
        self.lambda_pcc = 0.5  # PCCæŸå¤±æƒé‡
        
    def set_training_phase(self, epoch):
        """æ ¹æ®epochè®¾ç½®è®­ç»ƒé˜¶æ®µ"""
        self.current_epoch = epoch
        if epoch < self.discriminator_pretrain_epochs:
            self.training_phase = 1  # åŸºç¡€ç”Ÿæˆå™¨è®­ç»ƒ
        elif epoch < self.discriminator_pretrain_epochs + self.mcgan_warmup_epochs:
            self.training_phase = 2  # åˆ¤åˆ«å™¨é¢„è®­ç»ƒ + ç”Ÿæˆå™¨
        else:
            self.training_phase = 3  # å®Œæ•´MCGANè®­ç»ƒ
            
    def get_phase_description(self):
        """è·å–å½“å‰é˜¶æ®µæè¿°"""
        if self.training_phase == 1:
            return "åŸºç¡€ç”Ÿæˆå™¨è®­ç»ƒ"
        elif self.training_phase == 2:
            return "åˆ¤åˆ«å™¨é¢„è®­ç»ƒ"
        else:
            return "å®Œæ•´MCGANè®­ç»ƒ"
            
    def get_regression_weight(self):
        """æ¸è¿›å¼å›å½’æŸå¤±æƒé‡"""
        if self.training_phase == 1:
            return 0.0  # ç¬¬ä¸€é˜¶æ®µä¸ä½¿ç”¨å›å½’æŸå¤±
        elif self.training_phase == 2:
            # é¢„è®­ç»ƒé˜¶æ®µé€æ¸å¢åŠ æƒé‡
            progress = (self.current_epoch - self.discriminator_pretrain_epochs) / self.mcgan_warmup_epochs
            return self.lambda_regression * min(1.0, progress)
        else:
            # å®Œæ•´é˜¶æ®µä½¿ç”¨å…¨æƒé‡
            return self.lambda_regression
    
    def _compute_pcc_loss(self, pred, target):
        """è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°æŸå¤± - é‡ç‚¹ä¼˜åŒ–PCCæŒ‡æ ‡"""
        # å±•å¹³å¼ é‡
        pred_flat = pred.view(-1)
        target_flat = target.view(-1)
        
        # è®¡ç®—å‡å€¼
        pred_mean = torch.mean(pred_flat)
        target_mean = torch.mean(target_flat)
        
        # è®¡ç®—åæ–¹å·®å’Œæ ‡å‡†å·®
        pred_centered = pred_flat - pred_mean
        target_centered = target_flat - target_mean
        
        covariance = torch.mean(pred_centered * target_centered)
        pred_std = torch.sqrt(torch.mean(pred_centered ** 2) + 1e-8)
        target_std = torch.sqrt(torch.mean(target_centered ** 2) + 1e-8)
        
        # è®¡ç®—PCC
        pcc = covariance / (pred_std * target_std + 1e-8)
        
        # è¿”å›è´ŸPCCä½œä¸ºæŸå¤±ï¼ˆæœ€å¤§åŒ–PCCç­‰äºæœ€å°åŒ–-PCCï¼‰
        pcc_loss = 1.0 - pcc
        
        # æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§
        if torch.isnan(pcc_loss) or torch.isinf(pcc_loss):
            return torch.tensor(0.0, device=pred.device, requires_grad=True)
        
        return pcc_loss
    
    def forward(self, features, target_od=None, mode='train'):
        if mode == 'train':
            return self._forward_train(features, target_od)
        elif mode == 'predict':
            return self._forward_predict(features)
        else:  # eval mode
            return self._forward_eval(features, target_od)
    
    def _stable_monte_carlo_sampling(self, features, device):
        """è¶…ç¨³å®šçš„Monte Carloé‡‡æ · - ä¼˜åŒ–ç‰ˆ"""
        if self.training_phase == 1:
            # ç¬¬ä¸€é˜¶æ®µä¸è¿›è¡ŒMCé‡‡æ ·ï¼Œè¿”å›è™šæ‹Ÿå€¼
            batch_size, seq_len = features.size(0), features.size(1)
            return torch.full((batch_size, seq_len, 1), 0.5, device=device)
        
        mc_discriminator_outputs = []
        successful_samples = 0
        max_attempts = self.mc_samples * 2  # å…è®¸é‡è¯•
        
        for attempt in range(max_attempts):
            if successful_samples >= self.mc_samples:
                break
                
            try:
                # æ·»åŠ æ›´å°çš„å™ªå£°
                noise_scale = 0.001  # å‡å°å™ªå£°è§„æ¨¡
                noise = torch.randn_like(features) * noise_scale
                noisy_features = torch.clamp(features + noise, 0.0, 1.0)  # ç¡®ä¿åœ¨åˆç†èŒƒå›´
                
                # ä½¿ç”¨å½“å‰ç”Ÿæˆå™¨çŠ¶æ€ç”Ÿæˆæ ·æœ¬
                generated_sample = self.generator(noisy_features)
                
                # æ£€æŸ¥ç”Ÿæˆæ ·æœ¬çš„æœ‰æ•ˆæ€§
                if torch.isfinite(generated_sample).all():
                    # ä½¿ç”¨detaché¿å…æ¢¯åº¦å›ä¼ åˆ°ç”Ÿæˆå™¨
                    d_generated = self.discriminator(generated_sample.detach())
                    
                    # æ£€æŸ¥åˆ¤åˆ«å™¨è¾“å‡º
                    if torch.isfinite(d_generated).all():
                        mc_discriminator_outputs.append(d_generated)
                        successful_samples += 1
                        
            except Exception as e:
                # è·³è¿‡æœ‰é—®é¢˜çš„æ ·æœ¬
                continue
        
        if len(mc_discriminator_outputs) == 0:
            # å¦‚æœæ²¡æœ‰æˆåŠŸæ ·æœ¬ï¼Œè¿”å›å®‰å…¨çš„é»˜è®¤å€¼
            batch_size, seq_len = features.size(0), features.size(1)
            return torch.full((batch_size, seq_len, 1), 0.5, device=device, requires_grad=True)
        
        # ç¨³å®šçš„æœŸæœ›è®¡ç®—
        expected_d_generated = torch.stack(mc_discriminator_outputs, dim=0).mean(dim=0)
        
        # æœ€ç»ˆå®‰å…¨æ£€æŸ¥
        if not torch.isfinite(expected_d_generated).all():
            batch_size, seq_len = features.size(0), features.size(1)
            return torch.full((batch_size, seq_len, 1), 0.5, device=device, requires_grad=True)
            
        return expected_d_generated
    
    def _forward_train(self, features, target_od):
        """åˆ†é˜¶æ®µè®­ç»ƒæ¨¡å¼ - ä¼˜åŒ–ç‰ˆMCGAN"""
        batch_size, seq_len = features.size(0), features.size(1)
        device = features.device
        
        # 1. ç”ŸæˆODæµé‡
        generated_od = self.generator(features)
        
        # 2. é¢„æµ‹æŸå¤±ï¼ˆæ‰€æœ‰é˜¶æ®µéƒ½æœ‰ï¼‰
        prediction_loss = self.mse_loss(generated_od, target_od)
        
        # åˆå§‹åŒ–æ‰€æœ‰æŸå¤±é¡¹
        regression_loss = torch.tensor(0.0, device=device, requires_grad=True)
        discriminator_loss = torch.tensor(0.0, device=device, requires_grad=True)
        generator_adv_loss = torch.tensor(0.0, device=device, requires_grad=True)
        pcc_loss = torch.tensor(0.0, device=device, requires_grad=True)  # PCCæŸå¤±
        d_real_mean = torch.tensor(0.5, device=device)
        d_fake_mean = torch.tensor(0.5, device=device)
        expected_d_generated_mean = torch.tensor(0.5, device=device)
        
        # æ ¹æ®è®­ç»ƒé˜¶æ®µè®¡ç®—ä¸åŒçš„æŸå¤±
        if self.training_phase >= 2:  # é˜¶æ®µ2å’Œ3éœ€è¦åˆ¤åˆ«å™¨
            try:
                # 3. åˆ¤åˆ«å™¨ç›¸å…³è®¡ç®—
                d_real_logits = self.discriminator(target_od)
                d_fake_logits = self.discriminator(generated_od.detach())
                
                # æ£€æŸ¥logitsçš„æœ‰æ•ˆæ€§
                if torch.isfinite(d_real_logits).all() and torch.isfinite(d_fake_logits).all():
                    # åˆ¤åˆ«å™¨æŸå¤±
                    d_real_loss = self.bce_loss(d_real_logits.squeeze(-1), torch.ones_like(d_real_logits.squeeze(-1)))
                    d_fake_loss = self.bce_loss(d_fake_logits.squeeze(-1), torch.zeros_like(d_fake_logits.squeeze(-1)))
                    discriminator_loss = d_real_loss + d_fake_loss
                    
                    # ç”Ÿæˆå™¨å¯¹æŠ—æŸå¤±
                    d_fake_for_g_logits = self.discriminator(generated_od)
                    if torch.isfinite(d_fake_for_g_logits).all():
                        generator_adv_loss = self.bce_loss(d_fake_for_g_logits.squeeze(-1), torch.ones_like(d_fake_for_g_logits.squeeze(-1)))
                    
                    # è®¡ç®—åˆ¤åˆ«å™¨è¯„åˆ†
                    d_real_mean = torch.sigmoid(d_real_logits).mean()
                    d_fake_mean = torch.sigmoid(d_fake_logits).mean()
                
            except Exception as e:
                # å¦‚æœåˆ¤åˆ«å™¨è®¡ç®—å¤±è´¥ï¼Œä¿æŒé»˜è®¤å€¼
                pass
        
        # 4. MCGANå›å½’æŸå¤±ï¼ˆä»…åœ¨åˆé€‚çš„é˜¶æ®µï¼‰
        current_regression_weight = self.get_regression_weight()
        if current_regression_weight > 0 and self.training_phase >= 2:
            try:
                # Monte Carloä¼°è®¡æœŸæœ›åˆ¤åˆ«å™¨è¾“å‡º
                expected_d_generated = self._stable_monte_carlo_sampling(features, device)
                
                if torch.isfinite(expected_d_generated).all():
                    # çœŸå®æ•°æ®çš„åˆ¤åˆ«å™¨è¾“å‡º
                    d_real_for_regression = self.discriminator(target_od)
                    
                    if torch.isfinite(d_real_for_regression).all():
                        # MCGANæ ¸å¿ƒå›å½’æŸå¤±ï¼šL_R = E[(D(x) - E[D(Ä)])Â²]
                        regression_loss = self.mse_loss(d_real_for_regression, expected_d_generated)
                        expected_d_generated_mean = expected_d_generated.mean()
                        
                        # ç¡®ä¿å›å½’æŸå¤±æœ‰æ•ˆ
                        if not torch.isfinite(regression_loss):
                            regression_loss = torch.tensor(0.0, device=device, requires_grad=True)
                            
            except Exception as e:
                # å¦‚æœå›å½’æŸå¤±è®¡ç®—å¤±è´¥ï¼Œä¿æŒä¸º0
                regression_loss = torch.tensor(0.0, device=device, requires_grad=True)
        
        # 5. PCCæŸå¤±è®¡ç®— - é‡ç‚¹ä¼˜åŒ–PCCæŒ‡æ ‡
        try:
            pcc_loss = self._compute_pcc_loss(generated_od, target_od)
            if not torch.isfinite(pcc_loss):
                pcc_loss = torch.tensor(0.0, device=device, requires_grad=True)
        except Exception as e:
            pcc_loss = torch.tensor(0.0, device=device, requires_grad=True)
        
        # 6. æ€»æŸå¤±ç»„åˆï¼ˆæ ¹æ®é˜¶æ®µè°ƒæ•´æƒé‡ï¼Œé‡ç‚¹ä¼˜åŒ–PCCï¼‰
        if self.training_phase == 1:
            # é˜¶æ®µ1ï¼šé¢„æµ‹æŸå¤± + PCCæŸå¤±
            generator_total_loss = prediction_loss + self.lambda_pcc * pcc_loss
        elif self.training_phase == 2:
            # é˜¶æ®µ2ï¼šé¢„æµ‹æŸå¤± + æ¸è¿›å¼å›å½’æŸå¤± + å°‘é‡å¯¹æŠ—æŸå¤± + PCCæŸå¤±
            generator_total_loss = (
                0.7 * prediction_loss +                      # é™ä½é¢„æµ‹æŸå¤±æƒé‡
                current_regression_weight * regression_loss +
                0.1 * self.lambda_adversarial * generator_adv_loss +
                1.2 * self.lambda_pcc * pcc_loss            # é‡ç‚¹ä¼˜åŒ–PCC
            )
        else:
            # é˜¶æ®µ3ï¼šå®Œæ•´MCGAN + é‡ç‚¹PCCä¼˜åŒ–
            generator_total_loss = (
                0.6 * prediction_loss +                      # è¿›ä¸€æ­¥é™ä½é¢„æµ‹æŸå¤±æƒé‡
                current_regression_weight * regression_loss +
                self.lambda_adversarial * generator_adv_loss +
                1.5 * self.lambda_pcc * pcc_loss            # æœ€å¤§åŒ–PCCä¼˜åŒ–
            )
        
        total_loss = generator_total_loss
        
        # æœ€ç»ˆå®‰å…¨æ£€æŸ¥
        if not torch.isfinite(total_loss):
            total_loss = prediction_loss  # å›é€€åˆ°åŸºç¡€æŸå¤±
        
        return {
            'od_flows': generated_od,
            'total_loss': total_loss,
            'prediction_loss': prediction_loss,
            'regression_loss': regression_loss,
            'discriminator_loss': discriminator_loss,
            'generator_adv_loss': generator_adv_loss,
            'generator_total_loss': generator_total_loss,
            'pcc_loss': pcc_loss,  # æ–°å¢PCCæŸå¤±
            'd_real': d_real_mean,
            'd_fake': d_fake_mean,
            'expected_d_generated': expected_d_generated_mean,
            'training_phase': self.training_phase,
            'regression_weight': current_regression_weight
        }
    
    def _forward_eval(self, features, target_od=None):
        """è¯„ä¼°æ¨¡å¼"""
        generated_od = self.generator(features)
        result = {'od_flows': generated_od}
        
        if target_od is not None:
            prediction_loss = self.mse_loss(generated_od, target_od)
            result['prediction_loss'] = prediction_loss
        
        return result
    
    def _forward_predict(self, features):
        """çº¯é¢„æµ‹æ¨¡å¼"""
        with torch.no_grad():
            generated_od = self.generator(features)
            return {'od_flows': generated_od}
    
    def generate(self, features):
        """ç”ŸæˆODæµé‡é¢„æµ‹"""
        with torch.no_grad():
            result = self._forward_predict(features)
            return result['od_flows']

# ========== ä¿æŒä¸åŸä»£ç ä¸€è‡´çš„æ•°æ®é›† ==========
class SimpleODFlowDataset(Dataset):
    """ç®€åŒ–çš„ODæµé‡æ•°æ®é›† - ä¸åŸä»£ç å®Œå…¨ä¿æŒä¸€è‡´"""
    def __init__(self, io_flow_path, graph_path, od_matrix_path, test_ratio=0.2, val_ratio=0.1, seed=42):
        super().__init__()
        
        # åŠ è½½æ•°æ®
        self.io_flow = np.load(io_flow_path)  # (æ—¶é—´æ­¥, ç«™ç‚¹æ•°, ç‰¹å¾æ•°)
        self.graph = np.load(graph_path)      # (ç«™ç‚¹æ•°, ç«™ç‚¹æ•°)
        self.od_matrix = np.load(od_matrix_path)  # (æ—¶é—´æ­¥, ç«™ç‚¹æ•°, ç«™ç‚¹æ•°)
        
        # è½¬æ¢ç»´åº¦é¡ºåºï¼šä» (æ—¶é—´æ­¥, ç«™ç‚¹æ•°, ç‰¹å¾æ•°) åˆ° (ç«™ç‚¹æ•°, æ—¶é—´æ­¥, ç‰¹å¾æ•°)
        if self.io_flow.shape[0] == 28:  # å¦‚æœç¬¬ä¸€ä¸ªç»´åº¦æ˜¯æ—¶é—´æ­¥
            self.io_flow = np.transpose(self.io_flow, (1, 0, 2))
        
        # è½¬æ¢ç»´åº¦é¡ºåºï¼šä» (æ—¶é—´æ­¥, ç«™ç‚¹æ•°, ç«™ç‚¹æ•°) åˆ° (ç«™ç‚¹æ•°, ç«™ç‚¹æ•°, æ—¶é—´æ­¥)
        if self.od_matrix.shape[0] == 28:  # å¦‚æœç¬¬ä¸€ä¸ªç»´åº¦æ˜¯æ—¶é—´æ­¥
            self.od_matrix = np.transpose(self.od_matrix, (1, 2, 0))
        
        # åŠ¨æ€è·å–ç»´åº¦ - æŒ‰ç…§æŒ‡å—è¦æ±‚
        self.num_nodes = self.io_flow.shape[0]
        self.time_steps = self.io_flow.shape[1]
        
        # æ•°æ®ä¸€è‡´æ€§éªŒè¯ - æŒ‰ç…§æŒ‡å—è¦æ±‚
        print(f"æ•°æ®ç»´åº¦: IOæµé‡{self.io_flow.shape}, å›¾{self.graph.shape}, ODçŸ©é˜µ{self.od_matrix.shape}")
        
        # éªŒè¯æ•°æ®ç»´åº¦ä¸€è‡´æ€§
        assert self.io_flow.shape[0] == 28 or self.io_flow.shape[1] == 28, f"IOæµé‡æ•°æ®æ—¶é—´æ­¥æ•°ä¸æ­£ç¡®: {self.io_flow.shape}"
        assert self.io_flow.shape[2] == 2 or self.io_flow.shape[2] == 4, f"IOæµé‡æ•°æ®ç‰¹å¾æ•°ä¸æ­£ç¡®: {self.io_flow.shape} (åº”è¯¥æ˜¯2æˆ–4ä¸ªç‰¹å¾)"
        assert self.graph.shape[0] == self.graph.shape[1], f"å›¾æ•°æ®ä¸æ˜¯æ–¹é˜µ: {self.graph.shape}"
        assert self.graph.shape[0] == self.num_nodes, f"å›¾æ•°æ®ç»´åº¦ä¸èŠ‚ç‚¹æ•°ä¸åŒ¹é…: {self.graph.shape[0]} vs {self.num_nodes}"
        assert self.od_matrix.shape[0] == self.num_nodes and self.od_matrix.shape[1] == self.num_nodes, f"ODçŸ©é˜µç»´åº¦ä¸èŠ‚ç‚¹æ•°ä¸åŒ¹é…: {self.od_matrix.shape} vs ({self.num_nodes}, {self.num_nodes})"
        assert self.od_matrix.shape[2] == self.time_steps, f"ODçŸ©é˜µæ—¶é—´æ­¥æ•°ä¸åŒ¹é…: {self.od_matrix.shape[2]} vs {self.time_steps}"
        
        print(f"âœ… æ•°æ®ä¸€è‡´æ€§éªŒè¯é€šè¿‡: {self.num_nodes}ä¸ªèŠ‚ç‚¹, {self.time_steps}ä¸ªæ—¶é—´æ­¥")
        
        # ç«™ç‚¹å¯¹åˆ—è¡¨ - ä½¿ç”¨åŠ¨æ€èŠ‚ç‚¹æ•°é‡
        self.od_pairs = []
        for i in range(self.num_nodes):
            for j in range(i + 1, self.num_nodes):
                self.od_pairs.append((i, j))
        
        print(f"ç”Ÿæˆ{len(self.od_pairs)}ä¸ªç«™ç‚¹å¯¹ç”¨äºè®­ç»ƒ")
        
        # åŠ è½½ç«™ç‚¹äººå£å¯†åº¦æ•°æ® - ä¼˜å…ˆä½¿ç”¨52èŠ‚ç‚¹ç‰ˆæœ¬
        population_files = [
            "/private/od/data_NYTaxi/grid_population_density_52nodes.json",  # ä¼˜å…ˆä½¿ç”¨52èŠ‚ç‚¹ç‰ˆæœ¬
            "/private/od/data_NYTaxi/grid_population_density.json",  # åŸå§‹å¤‡ç”¨
            "/private/od/data/station_p.json"  # æ—§ç‰ˆæœ¬å¤‡ç”¨
        ]
        
        self.station_data = []
        for pop_file in population_files:
            if os.path.exists(pop_file):
                try:
                    with open(pop_file, 'r', encoding='utf-8') as f:
                        self.station_data = json.load(f)
                    print(f"âœ… åŠ è½½äººå£å¯†åº¦æ•°æ®: {pop_file}, å…±{len(self.station_data)}ä¸ªåŒºåŸŸ")
                    break
                except Exception as e:
                    print(f"âš ï¸ åŠ è½½äººå£å¯†åº¦æ•°æ®å¤±è´¥ {pop_file}: {str(e)}")
                    continue
        
        if not self.station_data:
            print(f"âš ï¸ æ‰€æœ‰äººå£å¯†åº¦æ•°æ®æ–‡ä»¶éƒ½æ— æ³•åŠ è½½ï¼Œä½¿ç”¨ç©ºæ•°æ®")
            self.station_data = []
        else:
            # éªŒè¯äººå£å¯†åº¦æ•°æ®ä¸èŠ‚ç‚¹æ•°é‡çš„ä¸€è‡´æ€§
            if len(self.station_data) != self.num_nodes:
                print(f"âš ï¸ äººå£å¯†åº¦æ•°æ®æ•°é‡({len(self.station_data)})ä¸èŠ‚ç‚¹æ•°é‡({self.num_nodes})ä¸åŒ¹é…")
                if len(self.station_data) > self.num_nodes:
                    print(f"   æˆªå–å‰{self.num_nodes}ä¸ªäººå£å¯†åº¦æ•°æ®")
                    self.station_data = self.station_data[:self.num_nodes]
                else:
                    print(f"   äººå£å¯†åº¦æ•°æ®ä¸è¶³ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼å¡«å……")
            else:
                print(f"âœ… äººå£å¯†åº¦æ•°æ®æ•°é‡ä¸èŠ‚ç‚¹æ•°é‡åŒ¹é…: {len(self.station_data)}ä¸ª")
        
        # æ•°æ®é›†åˆ’åˆ†
        all_indices = list(range(len(self.od_pairs)))
        random.seed(seed)
        random.shuffle(all_indices)
        
        total_samples = len(all_indices)
        train_size = int(total_samples * 0.8)
        val_size = int(total_samples * 0.1)
        
        self.train_indices = all_indices[:train_size]
        self.val_indices = all_indices[train_size:train_size + val_size]
        self.test_indices = all_indices[train_size + val_size:]
        
        print(f"æ•°æ®é›†åˆ’åˆ†å®Œæˆ:")
        print(f"  è®­ç»ƒé›†: {len(self.train_indices)} æ ·æœ¬ ({len(self.train_indices)/total_samples:.1%})")
        print(f"  éªŒè¯é›†: {len(self.val_indices)} æ ·æœ¬ ({len(self.val_indices)/total_samples:.1%})")
        print(f"  æµ‹è¯•é›†: {len(self.test_indices)} æ ·æœ¬ ({len(self.test_indices)/total_samples:.1%})")
        
        self.set_mode('train')
    
    def set_mode(self, mode):
        if mode == 'train':
            self.current_indices = self.train_indices
        elif mode == 'val':
            self.current_indices = self.val_indices
        elif mode == 'test':
            self.current_indices = self.test_indices
        else:
            raise ValueError(f"Unknown mode: {mode}")
    
    def __len__(self):
        return len(self.current_indices)
    
    def __getitem__(self, idx):
        site_pair_idx = self.current_indices[idx]
        site_i, site_j = self.od_pairs[site_pair_idx]
        
        # è·å–ODæµé‡
        od_i_to_j = self.od_matrix[site_i, site_j, :]
        od_j_to_i = self.od_matrix[site_j, site_i, :]
        od_flows = np.stack([od_i_to_j, od_j_to_i], axis=1)
        
        # è·å–IOæµé‡ - æ”¯æŒ2æˆ–4ä¸ªç‰¹å¾
        io_flow_i = self.io_flow[site_i, :, :]  # (æ—¶é—´æ­¥, ç‰¹å¾æ•°)
        io_flow_j = self.io_flow[site_j, :, :]  # (æ—¶é—´æ­¥, ç‰¹å¾æ•°)
        
        # ç®€å•å½’ä¸€åŒ–
        def normalize_data(data):
            data = np.nan_to_num(data, nan=0.0)
            data_min = np.min(data)
            data_max = np.max(data)
            if data_max > data_min:
                return (data - data_min) / (data_max - data_min)
            else:
                return data * 0
        
        io_flow_i = normalize_data(io_flow_i)
        io_flow_j = normalize_data(io_flow_j)
        od_flows = normalize_data(od_flows)
        
        # è·å–è·ç¦»ç‰¹å¾
        distance = self.graph[site_i, site_j]
        distance_normalized = distance / np.max(self.graph) if np.max(self.graph) > 0 else 0
        
        # è·å–ç«™ç‚¹äººå£å¯†åº¦
        if hasattr(self, 'station_data') and len(self.station_data) > 0:
            if site_i < len(self.station_data) and site_j < len(self.station_data):
                pop_density_i = self.station_data[site_i].get('grid_population_density', 0.0)
                pop_density_j = self.station_data[site_j].get('grid_population_density', 0.0)
            else:
                pop_density_i = 0.0
                pop_density_j = 0.0
                
            pop_density = (pop_density_i + pop_density_j) / 2
            max_pop_density = max([station.get('grid_population_density', 1.0) for station in self.station_data])
            if max_pop_density == 0:
                max_pop_density = 1.0
            pop_density_normalized = pop_density / max_pop_density
        else:
            pop_density_normalized = 0.0
        
        # æ„å»ºç‰¹å¾ï¼šIOæµé‡ + è·ç¦»ç‰¹å¾ + äººå£å¯†åº¦ç‰¹å¾
        distance_feature = np.ones((self.time_steps, 1)) * distance_normalized
        pop_density_feature = np.ones((self.time_steps, 1)) * pop_density_normalized
        features = np.concatenate([io_flow_i, io_flow_j, distance_feature, pop_density_feature], axis=1)
        # ç‰¹å¾ç»´åº¦: (æ—¶é—´æ­¥, io_flow_features*2 + 2) = (æ—¶é—´æ­¥, 2*2+2=6) æˆ– (æ—¶é—´æ­¥, 4*2+2=10)
        
        return torch.FloatTensor(features), torch.FloatTensor(od_flows)

# ========== è¯„ä¼°æŒ‡æ ‡è®¡ç®— ==========
def calculate_metrics(model, dataloader, device, desc="Evaluating"):
    model.eval()
    all_predictions = []
    all_targets = []
    total_losses = []
    
    with torch.no_grad():
        progress = tqdm(dataloader, desc=desc, leave=False)
        for features, od_flows in progress:
            features = features.to(device)
            od_flows = od_flows.to(device)
            
            predicted = model.generate(features)
            loss = F.mse_loss(predicted, od_flows)
            total_losses.append(loss.item())
            
            all_predictions.append(predicted.cpu().numpy())
            all_targets.append(od_flows.cpu().numpy())
            
            progress.set_postfix({'MSE': f'{loss.item():.6f}'})
    
    all_predictions = np.concatenate(all_predictions, axis=0)
    all_targets = np.concatenate(all_targets, axis=0)
    
    mse = np.mean((all_predictions - all_targets) ** 2)
    rmse = np.sqrt(mse)
    mae = np.mean(np.abs(all_predictions - all_targets))
    
    pred_flat = all_predictions.flatten()
    target_flat = all_targets.flatten()
    valid_mask = ~(np.isnan(pred_flat) | np.isnan(target_flat))
    if np.sum(valid_mask) > 0:
        pcc = np.corrcoef(pred_flat[valid_mask], target_flat[valid_mask])[0, 1]
        if np.isnan(pcc):
            pcc = 0.0
    else:
        pcc = 0.0
    
    avg_loss = np.mean(total_losses)
    
    return {
        'loss': float(avg_loss),
        'mse': float(mse), 
        'rmse': float(rmse),
        'mae': float(mae),
        'pcc': float(pcc)
    }

# ========== ä¸»è®­ç»ƒå‡½æ•° ==========
def train_true_mcgan_model(args):
    """è®­ç»ƒçœŸæ­£çš„MCGANæ¨¡å‹"""
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    set_seed(args.seed)
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = SimpleODFlowDataset(
        io_flow_path=args.io_flow_path,
        graph_path=args.graph_path,
        od_matrix_path=args.od_matrix_path,
        test_ratio=args.test_ratio,
        val_ratio=args.val_ratio,
        seed=args.seed
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=0)
    dataset.set_mode('val')
    val_loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False, num_workers=0)
    dataset.set_mode('test')
    test_loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False, num_workers=0)
    dataset.set_mode('train')
    
    # åŠ¨æ€è®¡ç®—è¾“å…¥ç‰¹å¾ç»´åº¦
    # ç‰¹å¾æ„æˆ: io_flow_i + io_flow_j + distance + population_density
    # = io_flow_features*2 + 2
    io_flow_features = dataset.io_flow.shape[2]  # 2 æˆ– 4
    input_dim = io_flow_features * 2 + 2  # 6 æˆ– 10
    print(f"âœ… åŠ¨æ€è®¡ç®—è¾“å…¥ç‰¹å¾ç»´åº¦: {input_dim} (IOæµé‡ç‰¹å¾: {io_flow_features})")
    
    # åˆ›å»ºçœŸæ­£çš„MCGANæ¨¡å‹
    model = TrueMCGANODFlowPredictor(
        input_dim=input_dim,
        hidden_dim=args.hidden_dim,
        output_dim=2,
        num_layers=args.num_layers,
        dropout=args.dropout,
        mc_samples=args.mc_samples,
        lambda_regression=args.lambda_regression,
        lambda_adversarial=args.lambda_adversarial
    ).to(device)
    
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    generator_params = sum(p.numel() for p in model.generator.parameters())
    discriminator_params = sum(p.numel() for p in model.discriminator.parameters())
    
    print(f"çœŸæ­£çš„MCGANæ¨¡å‹åˆ›å»ºæˆåŠŸï¼")
    print(f"  æ€»å‚æ•°æ•°é‡: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"  ç”Ÿæˆå™¨å‚æ•°: {generator_params:,}")
    print(f"  åˆ¤åˆ«å™¨å‚æ•°: {discriminator_params:,}")
    print(f"  éšè—ç»´åº¦: {args.hidden_dim}")
    print(f"  ç½‘ç»œå±‚æ•°: {args.num_layers}")
    print(f"  MCé‡‡æ ·æ•°: {args.mc_samples}")
    print(f"  å›å½’æŸå¤±æƒé‡: {args.lambda_regression}")
    print(f"  å¯¹æŠ—æŸå¤±æƒé‡: {args.lambda_adversarial}")
    print(f"  ğŸ¯ æ ¸å¿ƒç‰¹æ€§: ä¿æŒMCGANå›å½’æŸå¤±å’ŒMonte Carloé‡‡æ ·")
    
    # ç¨³å®šçš„ä¼˜åŒ–å™¨
    optimizer_g = torch.optim.AdamW(  # ä½¿ç”¨AdamW
        model.generator.parameters(),
        lr=args.lr,
        betas=(0.9, 0.999),
        weight_decay=args.weight_decay,
        eps=1e-8
    )
    
    optimizer_d = torch.optim.AdamW(
        model.discriminator.parameters(),
        lr=args.lr_d,
        betas=(0.9, 0.999),
        weight_decay=args.weight_decay,
        eps=1e-8
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler_g = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer_g, mode='min', factor=0.8, patience=args.patience, verbose=True, min_lr=1e-6
    )
    
    scheduler_d = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer_d, mode='min', factor=0.8, patience=args.patience, verbose=True, min_lr=1e-6
    )
    
    # è®­ç»ƒå¾ªç¯å˜é‡
    best_val_loss = float('inf')
    best_model_path = os.path.join(args.output_dir, 'best_true_mcgan_od_model.pth')
    epochs_without_improvement = 0
    train_history = []
    
    print(f"\nå¼€å§‹è®­ç»ƒçœŸæ­£çš„MCGAN ODæµé‡é¢„æµ‹æ¨¡å‹...")
    print(f"æ¨¡å‹å°†ä¿å­˜åˆ°: {best_model_path}")
    print(f"æ—©åœç­–ç•¥: éªŒè¯æŸå¤±{args.early_stop_patience}è½®æ— æ”¹å–„æ—¶åœæ­¢è®­ç»ƒ")
    print("="*80)
    
    for epoch in range(args.epochs):
        # è®¾ç½®è®­ç»ƒé˜¶æ®µ
        model.set_training_phase(epoch)
        phase_desc = model.get_phase_description()
        current_regression_weight = model.get_regression_weight()
        
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_losses = []
        train_prediction_losses = []
        train_regression_losses = []
        train_discriminator_losses = []
        train_generator_adv_losses = []
        train_pcc_losses = []  # æ–°å¢PCCæŸå¤±è®°å½•
        train_d_real_scores = []
        train_d_fake_scores = []
        
        train_progress = tqdm(train_loader, desc=f"Epoch {epoch+1:3d}/{args.epochs} [{phase_desc}]")
        for features, od_flows in train_progress:
            features = features.to(device)
            od_flows = od_flows.to(device)
            
            # æ ¹æ®è®­ç»ƒé˜¶æ®µé‡‡ç”¨ä¸åŒçš„è®­ç»ƒç­–ç•¥
            if model.training_phase == 1:
                # ===== é˜¶æ®µ1: åªè®­ç»ƒç”Ÿæˆå™¨ =====
                optimizer_g.zero_grad()
                outputs = model(features, od_flows, mode='train')
                generator_loss = outputs['generator_total_loss']
                
                if torch.isfinite(generator_loss):
                    generator_loss.backward()
                    torch.nn.utils.clip_grad_norm_(model.generator.parameters(), max_norm=1.0)
                    optimizer_g.step()
                    
            elif model.training_phase >= 2:
                # ===== é˜¶æ®µ2&3: åˆ¤åˆ«å™¨ + ç”Ÿæˆå™¨è®­ç»ƒ =====
                
                # ç¬¬ä¸€ä¸ªå‰å‘ä¼ æ’­ï¼šè®­ç»ƒåˆ¤åˆ«å™¨
                outputs_d = model(features, od_flows, mode='train')
                discriminator_loss = outputs_d['discriminator_loss']
                
                if torch.isfinite(discriminator_loss) and discriminator_loss.item() > 1e-6:
                    optimizer_d.zero_grad()
                    discriminator_loss.backward()
                    torch.nn.utils.clip_grad_norm_(model.discriminator.parameters(), max_norm=1.0)
                    optimizer_d.step()
                
                # ç¬¬äºŒä¸ªå‰å‘ä¼ æ’­ï¼šè®­ç»ƒç”Ÿæˆå™¨
                optimizer_g.zero_grad()
                outputs_g = model(features, od_flows, mode='train')
                generator_loss = outputs_g['generator_total_loss']
                
                if torch.isfinite(generator_loss):
                    generator_loss.backward()
                    torch.nn.utils.clip_grad_norm_(model.generator.parameters(), max_norm=1.0)
                    optimizer_g.step()
                
                # ä½¿ç”¨æœ€åä¸€æ¬¡çš„è¾“å‡ºç”¨äºè®°å½•
                outputs = outputs_g
            else:
                # é»˜è®¤æƒ…å†µï¼šå•æ¬¡å‰å‘ä¼ æ’­
                outputs = model(features, od_flows, mode='train')
            
            # è®°å½•æŸå¤±ï¼ˆæ ¹æ®é˜¶æ®µè®°å½•ä¸åŒçš„æŒ‡æ ‡ï¼‰
            if torch.isfinite(outputs['total_loss']):
                train_losses.append(outputs['total_loss'].item())
                train_prediction_losses.append(outputs['prediction_loss'].item())
                train_regression_losses.append(outputs['regression_loss'].item())
                train_generator_adv_losses.append(outputs['generator_adv_loss'].item())
                train_pcc_losses.append(outputs['pcc_loss'].item())  # è®°å½•PCCæŸå¤±
                train_d_real_scores.append(outputs['d_real'].item())
                train_d_fake_scores.append(outputs['d_fake'].item())
                
                # åˆ¤åˆ«å™¨æŸå¤±æ ¹æ®é˜¶æ®µè€Œå®š
                if model.training_phase >= 2:
                    # é˜¶æ®µ2å’Œ3æœ‰åˆ¤åˆ«å™¨è®­ç»ƒ
                    train_discriminator_losses.append(outputs['discriminator_loss'].item())
                else:
                    # é˜¶æ®µ1æ²¡æœ‰åˆ¤åˆ«å™¨è®­ç»ƒï¼Œä½¿ç”¨0å€¼
                    train_discriminator_losses.append(0.0)
            
            # æ›´æ–°è¿›åº¦æ¡ - æ ¹æ®è®­ç»ƒé˜¶æ®µæ˜¾ç¤ºä¸åŒä¿¡æ¯ï¼Œçªå‡ºPCCæŸå¤±
            if model.training_phase == 1:
                train_progress.set_postfix({
                    'Phase': 'åŸºç¡€',
                    'Total': f'{outputs["total_loss"].item():.4f}',
                    'Pred': f'{outputs["prediction_loss"].item():.4f}',
                    'PCC': f'{outputs["pcc_loss"].item():.4f}'  # çªå‡ºPCCæŸå¤±
                })
            elif model.training_phase == 2:
                train_progress.set_postfix({
                    'Phase': 'é¢„è®­ç»ƒ',
                    'Total': f'{outputs["total_loss"].item():.4f}',
                    'PCC': f'{outputs["pcc_loss"].item():.4f}',  # çªå‡ºPCCæŸå¤±
                    'MCGAN_Regr': f'{outputs["regression_loss"].item():.4f}',
                    'D_real': f'{outputs["d_real"].item():.3f}'
                })
            else:
                train_progress.set_postfix({
                    'Phase': 'å®Œæ•´MCGAN',
                    'Total': f'{outputs["total_loss"].item():.4f}',
                    'PCC': f'{outputs["pcc_loss"].item():.4f}',  # çªå‡ºPCCæŸå¤±
                    'MCGAN_Regr': f'{outputs["regression_loss"].item():.4f}',
                    'D_real': f'{outputs["d_real"].item():.3f}'
                })
        
        # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
        avg_train_loss = np.mean(train_losses) if train_losses else float('inf')
        avg_train_prediction = np.mean(train_prediction_losses) if train_prediction_losses else float('inf')
        avg_train_regression = np.mean(train_regression_losses) if train_regression_losses else 0.0
        avg_train_discriminator = np.mean(train_discriminator_losses) if train_discriminator_losses else float('inf')
        avg_train_generator_adv = np.mean(train_generator_adv_losses) if train_generator_adv_losses else float('inf')
        avg_train_pcc = np.mean(train_pcc_losses) if train_pcc_losses else 0.0  # PCCæŸå¤±å¹³å‡å€¼
        avg_d_real_score = np.mean(train_d_real_scores) if train_d_real_scores else 0.5
        avg_d_fake_score = np.mean(train_d_fake_scores) if train_d_fake_scores else 0.5
        
        # éªŒè¯é˜¶æ®µ
        print(f"  ğŸ” è®¡ç®—éªŒè¯é›†æŒ‡æ ‡...")
        val_metrics = calculate_metrics(model, val_loader, device, desc="éªŒè¯é›†è¯„ä¼°")
        
        # å­¦ä¹ ç‡è°ƒæ•´
        scheduler_g.step(val_metrics['loss'])
        scheduler_d.step(avg_train_discriminator)
        current_lr_g = optimizer_g.param_groups[0]['lr']
        current_lr_d = optimizer_d.param_groups[0]['lr']
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
        is_best = val_metrics['loss'] < best_val_loss
        test_metrics = None
        
        if is_best:
            print(f"  ğŸ¯ æ–°æœ€ä½³éªŒè¯æŸå¤±! è¯„ä¼°æµ‹è¯•é›†...")
            test_metrics = calculate_metrics(model, test_loader, device, desc="æµ‹è¯•é›†è¯„ä¼°")
            best_val_loss = val_metrics['loss']
            epochs_without_improvement = 0
        else:
            epochs_without_improvement += 1
            if os.path.exists(best_model_path):
                try:
                    checkpoint = torch.load(best_model_path, map_location=device)
                    test_metrics = checkpoint.get('test_metrics', {})
                except:
                    test_metrics = {}
        
        # æ‰“å°è¯¦ç»†ç»“æœ
        print(f"\nğŸ“Š Epoch {epoch+1:3d}/{args.epochs} çœŸæ­£çš„MCGANè®­ç»ƒå®Œæˆ:")
        print(f"{'='*80}")
        print(f"ğŸ”¹ è®­ç»ƒé˜¶æ®µ: {phase_desc} (é˜¶æ®µ {model.training_phase})")
        print(f"ğŸ”¹ å›å½’æŸå¤±æƒé‡: {current_regression_weight:.3f}")
        print(f"ğŸ”¹ è®­ç»ƒé›† (çœŸMCGAN + PCCä¼˜åŒ–):")
        if model.training_phase == 1:
            print(f"   æ€»æŸå¤±: {avg_train_loss:.6f} | é¢„æµ‹: {avg_train_prediction:.6f} | ğŸ¯PCCæŸå¤±: {avg_train_pcc:.6f}")
            print(f"   é˜¶æ®µ: åŸºç¡€ç”Ÿæˆå™¨ + PCCä¼˜åŒ–")
        elif model.training_phase == 2:
            print(f"   æ€»æŸå¤±: {avg_train_loss:.6f} | é¢„æµ‹: {avg_train_prediction:.6f} | ğŸ¯PCCæŸå¤±: {avg_train_pcc:.6f}")
            print(f"   ğŸ¯MCGANå›å½’: {avg_train_regression:.6f} | åˆ¤åˆ«: {avg_train_discriminator:.6f} | ç”Ÿæˆå¯¹æŠ—: {avg_train_generator_adv:.6f}")
            print(f"   ğŸ¯åˆ¤åˆ«å™¨è¯„åˆ†: D(real)={avg_d_real_score:.3f}, D(fake)={avg_d_fake_score:.3f} | é˜¶æ®µ: åˆ¤åˆ«å™¨é¢„è®­ç»ƒ + PCCä¼˜åŒ–")
        else:
            print(f"   æ€»æŸå¤±: {avg_train_loss:.6f} | é¢„æµ‹: {avg_train_prediction:.6f} | ğŸ¯PCCæŸå¤±: {avg_train_pcc:.6f}")
            print(f"   ğŸ¯MCGANå›å½’: {avg_train_regression:.6f} | åˆ¤åˆ«: {avg_train_discriminator:.6f} | ç”Ÿæˆå¯¹æŠ—: {avg_train_generator_adv:.6f}")
            print(f"   ğŸ¯åˆ¤åˆ«å™¨è¯„åˆ†: D(real)={avg_d_real_score:.3f}, D(fake)={avg_d_fake_score:.3f} | é˜¶æ®µ: å®Œæ•´MCGAN + æœ€å¤§PCCä¼˜åŒ–")
        
        print(f"ğŸ”¹ éªŒè¯é›†:")
        print(f"   æ€»æŸå¤±: {val_metrics['loss']:.6f} | MSE: {val_metrics['mse']:.6f}")
        print(f"   RMSE: {val_metrics['rmse']:.6f} | MAE: {val_metrics['mae']:.6f} | PCC: {val_metrics['pcc']:.6f}")
        
        if test_metrics:
            print(f"ğŸ”¹ æµ‹è¯•é›†:")  
            print(f"   æ€»æŸå¤±: {test_metrics.get('loss', 0):.6f} | MSE: {test_metrics.get('mse', 0):.6f}")
            print(f"   RMSE: {test_metrics.get('rmse', 0):.6f} | MAE: {test_metrics.get('mae', 0):.6f} | PCC: {test_metrics.get('pcc', 0):.6f}")
        else:
            print(f"ğŸ”¹ æµ‹è¯•é›†: æœªè¯„ä¼° (ä»…åœ¨éªŒè¯é›†æ”¹å–„æ—¶è¯„ä¼°)")
        
        print(f"ğŸ”¹ å­¦ä¹ ç‡: G={current_lr_g:.2e}, D={current_lr_d:.2e}")
        
        # ä¿å­˜è®­ç»ƒæ—¥å¿—åˆ°æ–‡ä»¶ - æ·»åŠ ç¼ºå¤±çš„æ—¥å¿—åŠŸèƒ½
        log_file = os.path.join(args.output_dir, "training_log.txt")
        try:
            # å¦‚æœæ˜¯ç¬¬ä¸€è½®ï¼Œåˆ›å»ºæ–°æ–‡ä»¶ï¼›å¦åˆ™è¿½åŠ 
            mode = 'w' if epoch == 0 else 'a'
            with open(log_file, mode, encoding='utf-8') as f:
                if epoch == 0:
                    f.write("çœŸæ­£çš„MCGAN ODæµé‡é¢„æµ‹æ¨¡å‹è®­ç»ƒæ—¥å¿—\n")
                    f.write("=" * 50 + "\n")
                
                f.write(f"Epoch {epoch+1}/{args.epochs}\n")
                f.write(f"   Training Phase: {phase_desc} (Stage {model.training_phase})\n")
                f.write(f"   Training - Total: {avg_train_loss:.6f}, Prediction: {avg_train_prediction:.6f}, MCGAN_Regression: {avg_train_regression:.6f}\n")
                f.write(f"   Validation - Loss: {val_metrics['loss']:.6f}, RMSE: {val_metrics['rmse']:.6f}, MAE: {val_metrics['mae']:.6f}, PCC: {val_metrics['pcc']:.6f}\n")
                
                # ğŸ”§ å…³é”®ä¿®å¤ï¼šç¡®ä¿æµ‹è¯•é›†æŒ‡æ ‡æ€»æ˜¯è¢«è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶
                if test_metrics:
                    f.write(f"   Test - Loss: {test_metrics.get('loss', 0):.6f}, RMSE: {test_metrics.get('rmse', 0):.6f}, MAE: {test_metrics.get('mae', 0):.6f}, PCC: {test_metrics.get('pcc', 0):.6f}\n")
                else:
                    f.write(f"   Test - Not evaluated this epoch (only when validation improves)\n")
                
                if is_best:
                    f.write(f"   New best model saved (Val Loss: {best_val_loss:.6f})\n")
                else:
                    f.write(f"   No improvement ({epochs_without_improvement}/{args.early_stop_patience} epochs without improvement)\n")
                
                f.write(f"   Learning Rate: G={current_lr_g:.2e}, D={current_lr_d:.2e}\n")
                f.write("\n")
                f.flush()
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜è®­ç»ƒæ—¥å¿—å¤±è´¥: {e}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            os.makedirs(os.path.dirname(best_model_path), exist_ok=True)
            torch.save({
                'model_state_dict': model.state_dict(),
                'optimizer_g_state_dict': optimizer_g.state_dict(),
                'optimizer_d_state_dict': optimizer_d.state_dict(),
                'scheduler_g_state_dict': scheduler_g.state_dict(),
                'scheduler_d_state_dict': scheduler_d.state_dict(),
                'epoch': epoch,
                'val_loss': val_metrics['loss'],
                'val_metrics': val_metrics,
                'test_metrics': test_metrics,
                'train_history': train_history,
                'args': args
            }, best_model_path)
            print(f"ğŸ¯ âœ… ä¿å­˜æœ€ä½³çœŸMCGANæ¨¡å‹ (éªŒè¯æŸå¤±: {best_val_loss:.6f})")
        else:
            print(f"â³ éªŒè¯æŸå¤±æœªæ”¹å–„ ({epochs_without_improvement}/{args.early_stop_patience}è½®)")
        
        # æ—©åœæ£€æŸ¥
        if epochs_without_improvement >= args.early_stop_patience:
            print(f"\nğŸ›‘ æ—©åœè§¦å‘! éªŒè¯æŸå¤±å·²{args.early_stop_patience}è½®æœªæ”¹å–„ï¼Œåœæ­¢è®­ç»ƒ")
            print(f"   æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f} (æ¥è‡ªç¬¬{epoch - epochs_without_improvement + 2}è½®)")
            break
        
        print("="*80)
    
    # æœ€ç»ˆæµ‹è¯•é˜¶æ®µ - åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆè¯„ä¼°
    print(f"\n{'='*60}")
    print("ğŸ¯ æœ€ç»ˆæµ‹è¯•é˜¶æ®µ - ä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œè¯„ä¼°")
    print(f"{'='*60}")
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    if os.path.exists(best_model_path):
        checkpoint = torch.load(best_model_path, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        best_epoch = checkpoint['epoch'] + 1
        best_val_metrics = checkpoint.get('val_metrics', {})
        best_test_metrics = checkpoint.get('test_metrics', {})
        print(f"âœ… å·²åŠ è½½æœ€ä½³æ¨¡å‹ (æ¥è‡ªç¬¬{best_epoch}è½®)")
        
        # å±•ç¤ºæœ€ä½³æ¨¡å‹çš„æ€§èƒ½
        print(f"\nğŸ† æœ€ä½³æ¨¡å‹æ€§èƒ½ (ç¬¬{best_epoch}è½®):")
        print(f"ğŸ”¸ éªŒè¯é›†: Loss={best_val_metrics.get('loss', 0):.6f}, RMSE={best_val_metrics.get('rmse', 0):.6f}, MAE={best_val_metrics.get('mae', 0):.6f}, PCC={best_val_metrics.get('pcc', 0):.6f}")
        print(f"ğŸ”¸ æµ‹è¯•é›†: Loss={best_test_metrics.get('loss', 0):.6f}, RMSE={best_test_metrics.get('rmse', 0):.6f}, MAE={best_test_metrics.get('mae', 0):.6f}, PCC={best_test_metrics.get('pcc', 0):.6f}")
        
        # ä½¿ç”¨ä¿å­˜çš„æµ‹è¯•æŒ‡æ ‡ä½œä¸ºæœ€ç»ˆç»“æœ
        final_test_metrics = best_test_metrics
    else:
        print("âš ï¸ æœ€ä½³æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨å½“å‰æ¨¡å‹è¿›è¡Œæœ€ç»ˆæµ‹è¯•")
        final_test_metrics = calculate_metrics(model, test_loader, device, desc="æœ€ç»ˆæµ‹è¯•")
        best_epoch = "å½“å‰"
    
    print(f"\n{'='*60}")
    print("ğŸ‰ çœŸæ­£çš„MCGAN ODæµé‡é¢„æµ‹æ¨¡å‹ - æœ€ç»ˆæµ‹è¯•ç»“æœ")
    print(f"{'='*60}")
    print(f"ğŸ“Š æœ€ç»ˆæµ‹è¯•æŒ‡æ ‡ (åŸºäºç¬¬{best_epoch}è½®æœ€ä½³æ¨¡å‹):")
    print(f"   ğŸ“ˆ å‡æ–¹è¯¯å·® (MSE):     {final_test_metrics.get('mse', 0):.6f}")
    print(f"   ğŸ“ˆ å‡æ–¹æ ¹è¯¯å·® (RMSE):   {final_test_metrics.get('rmse', 0):.6f}")
    print(f"   ğŸ“ˆ å¹³å‡ç»å¯¹è¯¯å·® (MAE):  {final_test_metrics.get('mae', 0):.6f}")
    print(f"   ğŸ“ˆ çš®å°”é€Šç›¸å…³ç³»æ•° (PCC): {final_test_metrics.get('pcc', 0):.6f}")
    print(f"   ğŸ“ˆ æµ‹è¯•æŸå¤±:          {final_test_metrics.get('loss', 0):.6f}")
    print(f"{'='*60}")
    
    # ä¿å­˜è¯¦ç»†ç»“æœåˆ°æ–‡ä»¶
    results_file = os.path.join(args.output_dir, "true_mcgan_od_results.txt")
    with open(results_file, "w", encoding='utf-8') as f:
        f.write("çœŸæ­£çš„MCGAN ODæµé‡é¢„æµ‹æ¨¡å‹æµ‹è¯•ç»“æœ\n")
        f.write("="*50 + "\n")
        f.write("è®ºæ–‡: MCGAN: Enhancing GAN Training with Regression-Based Generator Loss (AAAI 2025)\n")
        f.write("æ¨¡å‹æ ¸å¿ƒç‰¹ç‚¹:\n")
        f.write("  - å›å½’æŸå¤±å‡½æ•°: L_R(Î¸;Ï†) = E[(D_Ï†(x) - E[D_Ï†(Ä)])Â²]\n")
        f.write("  - Monte Carloä¼°è®¡å™¨ä¼°ç®—æœŸæœ›åˆ¤åˆ«å™¨è¾“å‡º\n")
        f.write("  - åˆ¤åˆ«å™¨çœŸæ­£å‚ä¸è®­ç»ƒï¼ˆä¸å›ºå®š0.5ï¼‰\n")
        f.write("  - ç¨³å®šçš„æ•°å€¼è®¡ç®—å’Œåˆ†é˜¶æ®µè®­ç»ƒ\n")
        f.write("\n")
        f.write(f"æ¨¡å‹å‚æ•°:\n")
        f.write(f"  - éšè—ç»´åº¦: {args.hidden_dim}\n")
        f.write(f"  - ç½‘ç»œå±‚æ•°: {args.num_layers}\n")
        f.write(f"  - MCé‡‡æ ·æ•°: {args.mc_samples}\n")
        f.write(f"  - å›å½’æŸå¤±æƒé‡: {args.lambda_regression}\n")
        f.write(f"  - å¯¹æŠ—æŸå¤±æƒé‡: {args.lambda_adversarial}\n")
        f.write(f"  - è®­ç»ƒè½®æ•°: {args.epochs}\n")
        f.write(f"  - æ‰¹æ¬¡å¤§å°: {args.batch_size}\n")
        f.write(f"  - å­¦ä¹ ç‡: G={args.lr}, D={args.lr_d}\n")
        f.write("\n")
        f.write("æµ‹è¯•ç»“æœ:\n")
        f.write(f"  å‡æ–¹è¯¯å·® (MSE):     {final_test_metrics.get('mse', 0):.6f}\n")
        f.write(f"  å‡æ–¹æ ¹è¯¯å·® (RMSE):   {final_test_metrics.get('rmse', 0):.6f}\n")
        f.write(f"  å¹³å‡ç»å¯¹è¯¯å·® (MAE):  {final_test_metrics.get('mae', 0):.6f}\n")
        f.write(f"  çš®å°”é€Šç›¸å…³ç³»æ•° (PCC): {final_test_metrics.get('pcc', 0):.6f}\n")
        f.write(f"  æµ‹è¯•æŸå¤±:          {final_test_metrics.get('loss', 0):.6f}\n")
        f.write(f"  æœ€ä½³éªŒè¯æŸå¤±:       {best_val_loss:.6f}\n")
    
    print(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    print(f"ğŸ“ è®­ç»ƒæ—¥å¿—å·²ä¿å­˜åˆ°: {os.path.join(args.output_dir, 'training_log.txt')}")
    print(f"ğŸ“ æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: {best_model_path}")
    
    return best_model_path

# ========== ä¸»å‡½æ•° ==========
def main():
    parser = argparse.ArgumentParser(description="çœŸæ­£çš„MCGAN ODæµé‡é¢„æµ‹æ¨¡å‹")
    
    # æ•°æ®å‚æ•°
    parser.add_argument("--io_flow_path", type=str, default="/private/od/data_NYTaxi/io_flow_daily.npy")
    parser.add_argument("--graph_path", type=str, default="/private/od/data_NYTaxi/graph.npy")
    parser.add_argument("--od_matrix_path", type=str, default="/private/od/data_NYTaxi/od_matrix_daily.npy")
    
    # çœŸMCGANæ¨¡å‹å‚æ•°
    parser.add_argument("--hidden_dim", type=int, default=64, help="éšè—ç»´åº¦")
    parser.add_argument("--num_layers", type=int, default=2, help="ç½‘ç»œå±‚æ•°")
    parser.add_argument("--dropout", type=float, default=0.1, help="Dropoutæ¦‚ç‡")
    parser.add_argument("--mc_samples", type=int, default=3, help="Monte Carloé‡‡æ ·æ•°é‡")
    parser.add_argument("--lambda_regression", type=float, default=0.8, help="MCGANå›å½’æŸå¤±æƒé‡")
    parser.add_argument("--lambda_adversarial", type=float, default=0.2, help="å¯¹æŠ—æŸå¤±æƒé‡")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--batch_size", type=int, default=32, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--epochs", type=int, default=100, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--lr", type=float, default=0.0002, help="ç”Ÿæˆå™¨å­¦ä¹ ç‡")
    parser.add_argument("--lr_d", type=float, default=0.0002, help="åˆ¤åˆ«å™¨å­¦ä¹ ç‡")
    parser.add_argument("--weight_decay", type=float, default=1e-5, help="æƒé‡è¡°å‡")
    parser.add_argument("--test_ratio", type=float, default=0.1, help="æµ‹è¯•é›†æ¯”ä¾‹")
    parser.add_argument("--val_ratio", type=float, default=0.1, help="éªŒè¯é›†æ¯”ä¾‹")
    parser.add_argument("--seed", type=int, default=82, help="éšæœºç§å­")
    
    # æ—©åœå’Œå­¦ä¹ ç‡è°ƒæ•´å‚æ•°
    parser.add_argument("--early_stop_patience", type=int, default=15, help="æ—©åœè½®æ•°")
    parser.add_argument("--patience", type=int, default=8, help="å­¦ä¹ ç‡è°ƒæ•´è½®æ•°")
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument("--output_dir", type=str, default="/private/od/paper_ny/MCGAN", help="è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = create_dynamic_output_dir(args.output_dir)
    args.output_dir = output_dir
    
    print("="*60)
    print("ğŸ¯ çœŸæ­£çš„MCGAN ODæµé‡é¢„æµ‹æ¨¡å‹")
    print("="*60)
    print("ğŸ“– è®ºæ–‡: MCGAN: Enhancing GAN Training with Regression-Based Generator Loss")
    print("ğŸ“– ä¼šè®®: AAAI 2025")
    print("ğŸ“– ä½œè€…: Baoren Xiao, Hao Ni, Weixin Yang")
    print()
    print("ğŸ”§ çœŸæ­£çš„MCGANç‰¹æ€§:")
    print("  âœ… å›å½’æŸå¤±å‡½æ•° - L_R(Î¸;Ï†) = E[(D_Ï†(x) - E[D_Ï†(Ä)])Â²] ğŸ¯")
    print("  âœ… Monte Carloä¼°è®¡å™¨ - ä¼°ç®—æœŸæœ›åˆ¤åˆ«å™¨è¾“å‡º")
    print("  âœ… åˆ¤åˆ«å™¨çœŸæ­£å‚ä¸è®­ç»ƒ - ä¸å›ºå®š0.5")
    print("  âœ… ç¨³å®šçš„æ•°å€¼è®¡ç®— - LayerNorm + æ¢¯åº¦è£å‰ª")
    print("  âœ… ä¿æŒè®ºæ–‡æ ¸å¿ƒæ€æƒ³ - ä¸ç®€åŒ–å…³é”®ç»„ä»¶")
    print()
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print("="*60)
    
    try:
        best_model_path = train_true_mcgan_model(args)
        print("\nğŸ‰ çœŸæ­£çš„MCGANæ¨¡å‹è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“ æœ€ä½³æ¨¡å‹ä¿å­˜ä½ç½®: {best_model_path}")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)